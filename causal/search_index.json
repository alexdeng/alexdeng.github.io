[["index.html", "Causal Inference and Its Applications in Online Industry Preface", " Causal Inference and Its Applications in Online Industry Alex Deng 2021-03-22 Preface This is a summary of key concepts in causal inference and its application in online industry. Please send feedback to alexdeng_at_live_dot_com. "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Causality concerns about causal relationship between two things. If there is a causal relationship between two things, one thing is responsible for causing the other thing but the reverse is not true. Causality is fundamental to all natural sciences, and accounts for majority of what we mean by knowledge. In classical physics, every law of physics is a form of deterministic causality. That is, the world at the next moment can be exactly predicted by the world at this moment because the predictive nature of cause and effect. The arrow of time can also been seen as an experience of cause and effect as the causal relationship is not reversible. A shattered vase coming together from pieces without intervention is surely against the law of physics. In a deterministic world, establishing causality sounds straightforward. Any law of physics should be able to replicate, as many time as we want. In this view causality is from repeated experience. “Knowledge is based on experience,” as an empiricist like David Hume might say. When there exist significant noises or the measurement of interest is intrinsically stochastic, we have to define causality using the probabilistic statements. Consider two random variables \\(X\\) and \\(Y\\). Roughly speaking, \\(X\\) has a causal effect on \\(Y\\) if changing \\(X\\) through intervention will change the distribution of \\(Y\\). Causal inference is about answering whether there is a causal link between two random variables and how to quantitatively identify the changes in the distribution of \\(Y\\) when we change \\(X\\) from one state to another. An clear understanding of changing through intervention in the above verbal definition of causal effect is fundamental. Unfortunately (multivariate) statistics based on (joint) probability distribution only concerns about association and correlation, but not causation. Brand new frameworks are required to describe causality in a rigorous way. Two most influential frameworks emerged in the second half of the last century are potential outcomes framework, also known as the Neyman-Rubin Causal Model (or simply Rubin Causal Model), and Judea pearl’s causal graphical model. Both frameworks focus on valid statistical inference with an intervention taking place. Regardless of the choices of causal framework, real life causal studies are categorized into two different types: experimental and observational. Experimental study requires investigators of a causal interest to design and conduct an experiment. Therefore controlled intervention is part of the process that generates data. For the purpose of causal inference, randomization is the key controlled intervention employed by experimenters. On the contrary, the data generating process of an observational study either involves no intervention at all, or an intervention that cannot be controlled, such as the effect of a new policy impacting all population. This Part offers a brief overview of causal inference in the language of statistics, introducing only the most fundamental and useful concepts. We introduce both the Rubin Causal Model and the Causal Graphical Model as the foundation for both experimental and observational studies. The core problem we focus on in this Part is the identification strategy of a causal effect. That is, whether we can unbiasedly estimate a causal effect using observed data, and if we can, how. There are further inferences beyond the unbiased point estimation of causal effect, such as the topics of hypothesis testing as well as efficiency and accuracy, e.g. variance and confidence interval. Sometimes unbiasedness need to be sacrificed for accuracy as a direct implication of bias-variance trade-off. We only touch this topics but leave discussions in more detail for later chapters. Nevertheless, unbiased estimation and identification strategy is the foundation and it encapsulates concepts such as selection-bias, confounders and inverse propensity score weighting that we deem essential for all data scientist. These concepts also find their values in many other areas beyond causal inference, including machine learning and AI. Key concepts covered in this part include: Simpson’s paradox. Correlation/Association does not imply causation. Potential outcomes and counterfactual. Randomization, unconfoundedness and naive estimation. Matching and conditional unconfoundedness. Covariate balancing and propensity. Weighted sample and reweighting. Missing data perspective of potential outcomes. Causal Graphical Models/Causal Diagrams. do operator as the meaning of change through intervention. Back-door criterion, Front-door criterion and more general rules. "],["simpson.html", "Chapter 2 Correlation and Simpson’s Paradox", " Chapter 2 Correlation and Simpson’s Paradox Consider the following conceptual graph. This is a simple causal graphical model which will be revisited and explained in detail later in Chapter 5. For now, simply interpret an arrow as a causal relationship. In Figure 2.1, \\(X\\) and \\(Y\\) share a common cause \\(U\\). Structures like this naturally induce a correlation between \\(X\\) and \\(Y\\). Figure 2.1: Graphical model for mutual dependence, a.k.a. a fork. To see that, let \\(U\\sim N(0,\\rho^2)\\) for some \\(\\rho&lt;1\\) and let \\(\\epsilon_1\\) and \\(\\epsilon_2\\) to be independent noises following \\(N(0,1-\\rho^2)\\) and are also independent of \\(U\\). Let \\(X = U + \\epsilon_1\\) and \\(Y = U + \\epsilon_2\\). It can be shown that \\(X\\) and \\(Y\\) have bivariate normal distribution with mean 0, variance 1 and correlation \\(\\rho^2\\). A positive correlation between \\(X\\) and \\(Y\\) means a relatively large \\(X\\) is associated with a relatively large value of \\(Y\\). In fact, \\[\\begin{equation*} \\mathrm{E}(Y|X = x) = \\rho^2\\times x. \\end{equation*}\\] Does it mean increasing \\(X\\) has a causal effect on increasing \\(Y\\)? No! In the ground-truth model, \\(X\\) and \\(Y\\) are both noisy versions of the common underlying signal \\(U\\). By symmetry, the following is also true: \\[\\begin{equation*} \\mathrm{E}(X|Y = y) = \\rho^2\\times y. \\end{equation*}\\] If we consider \\(X\\) causes \\(Y\\), by the same logic \\(Y\\) also causes \\(X\\). This violates the common sense that cause and effect cannot be reversed. \\(X\\) and \\(Y\\) are statistically correlated but it does not warrant a causal relationship. This phenomenon is often quoted as “Correlation does not imply causation.” The above contrived example seems to suggest the distinction between correlation and causation is clear and should be apparent to careful eyes. Quite the opposite! Human nature makes it very hard to distinguish them, as illustrated by the infamous Simpson’s paradox. Simpson’s paradox is often presented as a compelling demonstration of how easy it is to reach paradoxical conclusions when interpreting data too superficially without deeper analyses. It is one of the most known statistics puzzles. On one hand, it only requires elementary math to understand the problem. On the other hand, it has deeper psychological implication and often be interpreted as limitation of statistical methods. To resolve the paradox requires solid understanding about causal inference. Simpson’s paradox is deeply connected to the desire of causal knowledge as part of human nature and the tendency to take correlation as causation. Edward H. Simpson first addressed this phenomenon in a technical paper in 1951, but Karl Pearson et. al. in 1899 and Udny Yule in 1903, had mentioned a similar effect earlier.1 We take the Kidney stone treatment data to illustrate Simpson’s paradox. Other well known data sets used to illustrate Simpson’s paradox include UC Berkeley gender bias data and baseball batting average data. Table 2.1: Kidney Stone Treatments. Treatment A has higher success rate in both small and large stone cases, but lower when both cases combined. Treatment A Treatment B Small Stones 93%(81/87) 87%(234/270) Large Stones 73%(192/263) 69%(55/80) Both 78%(273/350) 83%(289/350) Table 2.1 shows success rate of treating kidney stones using two different treatments, denoted as A and B. The patients were further segmented by their kidney stone size. Success rates were computed for each of the 4 treatment kidney-size combinations, as well as the overall success rate for the two treatments. From the table, treatment B has a higher success rate overall, regardless of the size of the stone. However, segmented by the size of the stone, in both small and large stone cases, treatment A shows higher success rate. How could it be true? Is treatment A better or worse than treatment B? On the surface, Simpson’s paradox can be explained by simple arithmetics. Namely the ratio of sums \\((a+b)/(c+d)\\) could be ordered differently than individual ratios \\(a/b\\) and \\(c/d\\). In the kidney stone example, we see small stone cases generally have higher success rate regardless of treatment. In the data set, treatment A consists of small stone and big stone groups, with 87/(87+263)=23.1% patients have small stones. Treatment B has only 270/(270+80) = 77.1% patients with small stones. When grouped together, treatment B have higher success rate only because it contains more patients that are easier to treat. The arithmetics is simple and the explanation seems to be straightforward. What makes Simpson’s paradox so important and profound? Suppose as a data scientist you are presented with the kidney stone data, with only aggregated data. What conclusion will you make? Most people will conclude that treatment B is better, given that 95% confidence interval of success rate for treatment A overall is estimated to be \\(78\\%\\pm0.1\\%\\), while for treatment B is \\(83\\%\\pm0.08\\%\\)). However, once you are presented with the stone size segmented data, you realize you might just made a huge mistake. How could treatment B be better if treatment A is better in both small stone and large stone cases? You must correct your earlier assessment and then conclude treatment A is better. But wait a minute, what if somebody else gives you a further segmented data that show, say, for both small stone and large stone cases, when segmented by gender, treatment B is better than treatment A for all gender? Isn’t this mathematically possible by the same arithmetics aforementioned? We can ask this kind of questions on and on and reach to a conclusion that no conclusion can be made based on any type of data (pun intended). Now we see the real and unsettling problem. Can we learn anything from data? The key to resolve the paradox is simple yet profound, “correlation does not imply causation!” Why do people feel this is a paradox in the first place? And what do we mean by “learn from data?” What Simpson’s paradox reveals is the hidden truth that as human beings our brains are hardwired to look for causal effect. If the difference between kidney stone success rate is indeed caused by treatment A being more effective than treatment B for both small and large stone cases, then overall treatment A should be more effective than treatment B because a patient can only have small or big stone. In fact, we should be able to further state \\[\\begin{align*} \\text{Overall effect} &amp; = \\text{Effect on small stone patients}\\times P(\\text{patient have small stone}) \\\\ &amp; + \\text{Effect on large stone patients}\\times P(\\text{patient have large stone}). \\end{align*}\\] The rationale in this argument is buried deep down in the definition of causal effect — the notion of change. When we change treatment from A to B, or vice versa, we assume the size of a patient’s kidney stone remains unchanged because it is a precondition of the treatment. Therefore the proportion of patient having small or large stones has nothing to do with our decision of which treatment to use. But did we explicitly claim any causal relationship? No! Before we see the segmented data, all we observe is that there exists a positive correlation between choosing treatment B over A and higher treatment success rate. After the data is further segmented by stone size, the correlation is reversed. Through the arithmetic explanation, we see that the reason why treatment B is better in aggregated data is actually not because the treatment itself, but due to the higher proportion of small stone patients in treatment B. In other words, the link between the choice of treatment and success rate is not causal at all. Treatment B is either just “lucky” to have more easier-to-treat patients with smaller stones, or more likely, there is certain self-selection mechanism that leads more patients with larger stones to choosing treatment A, or more patients with smaller stones to choosing treatment B, or both, despite this dataset showing seemly a balanced design with both treatment A and treatment B each having 350 patients. Correlation does not imply causation. Correlation can happen due to many other unobserved confounder factors. In this case one confounder effect is the size of the stone. There could be more confounders. We just never know. What makes this such a puzzling paradox is the fact that correlation is not what we are looking for, causation is! Researchers collect this data set with the hope to find out which treatment is better. Our goal of searching for causal knowledge tricked everyone into unconsciously considering correlation as causation, hence the paradox!2 Paradox resolved! But an unnerving concern remains: Why are our brains almost hardwired to interpret correlation as causal effect? The following XKCD comic reminds us of how vulnerable we are. Figure 2.2: Correlation(xkcd 552) This is not a book on philosophy, but we hope some degree of philosophical discussions can help: As pointed out by empiricists, causality, and knowledge in general, is learned from experience. When analyzing a dataset — a form of collected experiences, we tend to treat all conclusions as causal. Causal relationship is much more stable and robust than correlation. As demonstrated in Simpson’s paradox, correlation relationship might be due to many other confounders, known or unknown, observed or unobserved. Correlation as knowledge is hard to transfer to a situation in future. Statisticians call this external validity. Interpreting empirical correlation between two events as causation tremendously simplifies our understanding of the world. Occam’s razor in action. Many psychological studies demonstrated the existence of cognitive bias in which people are often overly confident in believing their own discovery. We naturally believe our findings are reflecting a form of truth. This can be seen as a type of systematic error from inductive reasoning. Statistics, not philosophy, should provide us with the rigorous tool to prevent us from falling into the trap of correlation. Unfortunately, correlation, not causation, is the firstborn in the household of statistics. The study of more than one outcome variables belongs to multivariate statistics and many may be more familiar with its modern day incarnations under other names such as predictive analysis/modeling and machine learning or simply AI. Given a joint probability distribution, we can predict one random variable from observations of others, by exploiting correlation structures between them. If two variables \\(X\\) and \\(Y\\) are correlated in a joint distribution, our understanding of \\(X\\) and \\(Y\\) are intertwined together. If we change our estimate of \\(X\\), our estimate of \\(Y\\) also changes. In this sense multivariate statistics and modern predictive modeling are all about correlation under certain joint probability distribution. The definition of causation also relies on change but it is a totally different kind. In correlation, the changes are passively observed. In causation, we change through intervention directly. The difference between passive observing and intervention is critical and begs a proper treatment in statistics. In the former case, the ground-truth underlying joint distribution between random variables of interest is responsible for the change, but not part of the change. In the latter, the joint distribution itself can be changed and disturbed by the act of the intervention. At the very core of causal inference is about how we reason and talk about intervention using appropriate statistical vocabularies. A direct method following the discussion above is to study the impact of intervention on the joint distribution and quantify the causal effect of an intervention on a focal random variable using post-intervention joint distributions. This approach was taken and led by Judea Pearl through a series of works started in 1980s culminated in a new causal graphical model with a new algebra called do calculus.3 An alternative approach is to augment the joint distribution with the idea of potential outcomes. A third approach, with a much longer history is randomization. Randomization is the most straightforward way to learn effect of intervention by really doing intervention. The next few chapters will go through all these different approaches and explain how they are connected to each other. Simpson’s paradox is also called Yule-Simpson effect.↩︎ Simpson’s paradox also links to Savage’s Sure thing principle: “[Let f and g be any two acts], if a person prefers f to g, either knowing that the event B obtained, or knowing that the event not-B obtained, then he should prefer f to g even if he knows nothing about B.” We see that Sure thing principle also has an implicit causal setup.↩︎ Judea Pearl received Turing Award for “fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.”↩︎ "],["randomintro.html", "Chapter 3 Randomized Experiment 3.1 Complete Randomization 3.2 Independent Randomization 3.3 Clustered Randomization 3.4 Analysis of Randomized Experiments as Two Sample Problem", " Chapter 3 Randomized Experiment What could an experimental physicist do if she wants to compare two experiment conditions? Simple. She just need to run two experiments under both conditions and then compare the outcomes. In Table 2.1 there are two different treatments of kidney stones, A and B. Pick any patient in Table 2.1. If a patient took treatment A, we observe the outcome of treating the patient with A. It is impossible for us to rewind the time and force the patient to take treatment B and observe the outcome. Even if we could, what we really want to observe is the outcome of treating this patient with treatment B if he or she chose treatment B under no coercion. The same situation applies to patients took treatment A and we are not be able to know what treatment B would do to them. These hypothetical outcomes, not observed and also impossible to observe, are called the counterfactuals. Counterfactuals are like ghost observations and cannot be directly used in any calculation. They plays a pivotal role in the potential outcome framework. Theories of the potential outcome framework center around the question of whether and how can we infer statistical quantities, e.g. mean, of a counterfactual by only using observed outcomes. Potential outcomes and its related theories are our topics for the next chapter. But long before the advent of potential outcome framework and even the existence of statistics as a discipline, the idea of randomized experiment was already being used. Take the kidney stone treatment example. If we do not allow patients to self-select their treatment. Instead, we design a split of patients into two groups receiving treatment A and B. If the split is done such that the only difference between the two groups is the treatment they use and all other things being equal, then any difference between outcomes of the two groups can only possibly be caused by the difference in the treatments they received. In other words, the comparison of the two groups should be a fair comparison immune to any systematic selection bias. Randomization is the simplest strategy to ensure an all-other-thing-being-equal splitting. It is a form of intervention that echos the definition of causal effect as the result of change through intervention. Inferring causal effect by means of randomization is also called randomized experiment, or controlled experiment. We learn the effect of change through intervention by really doing intervention, similar to scientists running experiments by changing experiment conditions in a lab. The subtle difference is randomization does not allow us to observe the counterfactuals under different conditions directly. Instead, it relies on the magic spell “all other things being equal.” Arguments using the power of “all other things being equal” are extremely simple to almost being obvious. Fisher, whose work in early 1920s popularized the idea of randomized experiment, believed randomization and more generally the idea of permutation to be obvious to anyone and treated them as examples of his “logic of inductive science.” As form of logic, he believed no formal statistics or mathematics is needed to convey the idea to any rational person. Indeed, in logic, Ceteris paribus is the Latin phrase for “all other things being equal” and any conclusion based on using this argument is called a cp law. Different randomization mechanisms can achieve all-other-thing-being-equal. In all randomization mechanism we consider the case of only two groups. Extension to more than two groups is straightforward. For two group cases, we often call one group treatment and the other control. Define assignment indicator \\(Z\\) as \\(Z=0\\) for being assigned to the fist group (control) and \\(Z=1\\) for being assigned to the second group (treatment). Although different randomization mechanisms all achieve the requirement of all-other-thing-being-equal. The choice of randomization mechanism has an implication in the data generating process underneath the collected observations and has an impact on the following statistical analysis. 3.1 Complete Randomization Suppose there are \\(N\\) subjects in total and we want to split them into two groups of sizes \\(M\\) and \\(N-M\\). All we need to do is to sample \\(M\\) subjects out of \\(N\\) without replacement. Each patient have the same probability \\(M/N\\) of being assigned to the first group and probability \\(1-M/N\\) to be assigned to the second group. Complete randomization is closely connected to the idea of permutation. One way of implementing complete randomization would be permute the \\(N\\) subjects into a random order and assign the first \\(M\\) subjects to the first group and the rest to the second. Complete randomization ensures the size of each group to be fixed. However, assignment \\(Z\\) for different subjects are not independent. To see that, \\(Z = 1\\) for one subject reduces the chance of \\(Z=1\\) for another subject because of the fixed size of treatment and control. For \\(i^{\\text{th}}\\) and \\(j^{\\text{th}}\\) subject, \\[\\begin{align*} \\mathrm{Cov}(Z_i, Z_j) &amp; = \\mathrm{E}(Z_i Z_j) - \\mathrm{E}(Z_i)\\mathrm{E}(Z_i) \\\\ &amp; = \\frac{M}{N}\\frac{M-1}{N-1} - \\left( \\frac{M}{N} \\right)^2 &lt; 0. \\end{align*}\\] 3.2 Independent Randomization Complete randomization requires experimenters to know the total size and desired sizes for each groups at the randomization stage. In many applications, subjects were recruited on an ongoing basis. For these cases, randomization can also be done by drawing a group label from a multi-class Bernoulli distribution independently for each subject. Suppose we need to split \\(N\\) subjects into K groups with probabilities \\(p_1, \\dots, p_K\\), \\(\\sum_k p_k = 1\\). For each subject, we draw the group label \\(L\\) from the distribution following \\[ L = k, \\quad \\text{with probability } p_k, \\quad k = 1,\\dots, K. \\] Unlike in complete randomization, assignment indicator \\(Z\\) for different subjects are by design independent. Also, the group sizes \\(N_1,\\dots,N_K\\) are no longer fixed numbers but a random vector from multinomial distribution \\[ (N_1,\\dots,N_K) \\sim \\text{Multinom}(N, p_1,\\dots,p_K). \\] Independent randomization is also called Bernoulli trial as each assignment is an independent Bernoulli random variable. 3.3 Clustered Randomization Often, treatments are assigned at the level of each subject. This is not always the case nor is it always desired. For example, when doing an experiment to study effect of certain education method, randomizing each individual student is not possible and randomizing each classroom is more appropriate. Randomizing clusters of subjects still satisfies all-other-things-being-equal. Either complete randomization or independent randomization can be used to randomize clusters. Clustered randomization is commonly used when social spillover or interference effect exists. In the education method example above, since students are interacting with each other and even might form study groups, it is possible that the effect of a new education method on a student will also be affected by assignments of other students he or she closely interacts with. In case like this, if we can cluster students such that inter-cluster level interference is negligible, then a clustered randomization should be used.4 Clustered randomization highlights the distinction of two different units: randomization unit and analysis unit. Analysis unit is the unit at which we want to infer causal effect. Randomization unit is the unit at which randomization is performed. Randomization unit has to be defined equal or less granular than the analysis unit. This means randomization can be applied on clusters of analysis units but within each analysis unit randomization of sub-analysis unit level does not make sense. To see this, if a sub-analysis unit is used for randomization, then each analysis unit could experience both treatment and control and can no longer belong to only one treatment group. 3.4 Analysis of Randomized Experiments as Two Sample Problem Although randomization experiment provides a straightforward method to infer causal effect and the all-other-thing-being-equal argument is obvious to most minds, the analysis of randomized experiments remains a statistical problem. Rigorous analysis of randomized experiment requires using a causal model such as potential outcomes framework. Potential outcome framework will be our main topic in the Chapter 4, where we will show randomization satisfies a condition called “ignorability” or “unconfoundedness” under which naive comparison of different treatment groups leads to an unbiased estimation of causal effect — mathematically justifying the all-other-thing-being-equal logic. Here we briefly go through a widely used practice simply treating the analysis of randomized experiments as a special application of the two sample problem. The two sample problem concerns with two independent samples drawn from two distributions, and a typical interest is to compare the means of the two. Let \\(X, X_1,\\dots,X_N\\) and \\(Y, Y_1,\\dots,Y_M\\) are i.i.d. random variables. That is, we observed \\(N\\) i.i.d. observations having the same distribution as \\(X\\) and \\(M\\) i.i.d. observations as \\(Y\\). We are interested in the difference of the two means \\[\\begin{equation*} \\delta = \\mathrm{E}(Y) - \\mathrm{E}(X). \\end{equation*}\\] \\(\\delta\\) can be unbiased estimated by the estimator \\[\\begin{equation*} \\Delta = \\overline{Y} - \\overline{X}. \\end{equation*}\\] When \\(N\\) and \\(M\\) are small, the distribution of \\(\\Delta\\) is hard to track down. If we further assume \\(X\\) and \\(Y\\) are both normally distributed, then we know \\(\\Delta\\) is also normally distributed and we should focus on estimating its variance. Since \\(X_i\\) and \\(Y_i\\) are independent, \\[\\begin{align*} \\mathrm{Var}_\\text{TS}(\\Delta) &amp; = \\mathrm{Var}(\\overline{Y})+ \\mathrm{Var}(\\overline{X}) \\\\ &amp; = \\frac{\\mathrm{Var}(Y)}{M} + \\frac{\\mathrm{Var}(X)}{N}, \\end{align*}\\] where the subscript \\(TS\\) stands for two sample and \\(\\mathrm{Var}(Y)\\) can be unbiasedly estimated using sample variance formula \\[ \\widehat{\\sigma}_Y^2 = \\frac{\\sum_i (Y_i - \\overline{Y})^2}{M-1} \\] given i.i.d. \\(Y_i\\) and similarly \\(\\widehat{\\sigma}_X^2\\) for \\(\\mathrm{Var}(X)\\). \\(M-1\\) here is merely for finite sample unbiasedness and is often replaced by \\(M\\) unless \\(M\\) is very small. Let’s denote the estimated \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) by \\(\\widehat{\\sigma}_\\text{TS}^2\\), \\(\\frac{\\Delta - \\delta}{\\widehat{\\sigma}_\\text{TS}}\\) follows a standard normal distribution if we ignore the fact that \\(\\widehat{\\sigma}_\\text{TS}\\) is an estimator and not same as the true standard deviation of \\(\\Delta\\). Indeed, when \\(M\\) and \\(N\\) are large, by the law of large number and the Slutsky’s theorem, \\(\\frac{\\Delta - \\delta}{\\widehat{\\sigma}_\\text{TS}}\\) converges in distribution to the standard normal distribution. When \\(M\\) and \\(N\\) are small, Gosset, under the name Student, showed that \\(\\frac{\\Delta - \\delta}{\\widehat{\\sigma}_\\text{TS}}\\) follows a t-distribution which has a heavier tail than standard normal distribution.5 With moderate to large number of samples, even without \\(X\\) and \\(Y\\) being normally distributed, \\(\\Delta\\) and its standardized version \\(\\frac{\\Delta - \\delta}{\\widehat{\\sigma}_\\text{TS}}\\) will be approximately normal thanks to the central limit theorem (and Slutsky’s theorem for the normalized \\(\\Delta\\)). Also in this case the t-distribution, with a degree of freedom over 30, is practically equivalent to a normal distribution. The moderate to large sample case is also referred to as z-statistic, and z-test for the corresponding hypothesis test, as compared to t-test based on t-statistic. In practice this distinction is often (rightfully) ignored. The normal assumption on both \\(X\\) and \\(Y\\) for the small sample case is too strong to meet in most applications. Efforts often have to be made to transform observations into a normal-like distribution. This is not necessary for large sample case as long as the central limit theorem kicks in. This is why z-statistic and z-test are much more useful in practice. However because t-statistic automatically turns into a z-statistic when degree of freedom is large. People often keep using the name t-statistic and t-test. For moderate to large sample case, a \\(1-\\alpha\\) two sided confidence interval is \\[ \\Delta \\pm z_{\\alpha/2} \\widehat{\\sigma}_\\text{TS}\\,, \\] where \\(z_{\\alpha/2}\\) is the \\(1-\\alpha/2\\) quantile of the standard normal distribution. Correspondingly, a two-sided z-test for the null hypothesis \\(H_0: \\delta = 0\\) is to reject \\(H_0\\) when \\[ \\frac{|\\Delta|}{\\widehat{\\sigma}_\\text{TS}} \\ge z_{\\alpha/2}\\,. \\] For a randomized experiment with binary treatment, by the all-other-thing-being-equal argument, the causal effect of the treatment becomes the difference between the two groups. Comparing the treatment group and the control group seems to naturally fit into a two sample problem. \\(\\Delta\\) becomes an unbiased estimator for the causal effect for the mean — the average treatment effect. However, there are a few notable differences. Unlike the standard two sample problem, where the data-generating-process is such that we sample i.i.d. observations from treatment and control groups separately and independently. Data-generating process in a randomized experiments is more complicated and relies on two things: the randomization mechanism and also the causal estimand. For complete randomization, the data-generating process involves two steps, first step is to sample \\(N+M\\) units for the experiment with their counterfactual pair \\((Y(1),Y(0))\\), and the second step is the permutation such that \\(N\\) units are assigned to control and \\(M\\) to the treatment. The observation is \\(Y(1)\\) if assigned to treatment and \\(Y(0)\\) if control. The first step is optional as some researchers such as Neyman prefer to assume \\(N+M\\) units are given and only focus on the average treatment effect on this set of \\(N+M\\) units (This is called sample average treatment effect SATE, comparing to population average treatment effect PATE). In this case observations are not independent within each group or across groups due to sampling without replacement. The randomness in a complete randomized experiments are only due to the assignment randomization. Considering population average treatment effect adds the first step of sampling \\(M+N\\) units into the picture but it draws from a single distribution of counterfactual pairs rather than from two distributions independently as in the independent two sample problem. For independent randomization and for population average treatment effect, the data-generating-process is first independently toss a coin as the treatment assignment process and then independently draw from treatment distribution or control distribution based on the treatment assignment. This does provide i.i.d. observations for both treatment group and control groups, and also guarantees independence between the two groups. In the classic Bernoulli trial introduced above, we fix the total sample size \\(M+N\\) and \\(M\\) (and \\(N\\)) is not a fixed number but rather from a Binomial distribution. If instead of population average treatment effect, the sample average treatment effect is desired, the data-generating-process is again different. \\(M+N\\) counterfactual pairs are assumed to be given. For each of them, we independently assign treatment or control to decide \\(Y(1)\\) or \\(Y(0)\\) to observe. Despite the differences, two sample t-test or z-test are often used in practice for both complete randomization design and independent randomization design. Is this a huge mistake that practitioners have been making? Fortunately, the conclusion is this practice is totally fine and we will now explain. We first look at the complete randomization design. Recall it differs from independent sampling because treatment assignments of units are negatively correlated with each other. Typically, if we have sum of negatively correlated random variables, we expect the variance of the sum to be smaller than the independent case because \\[\\begin{align*} \\mathrm{Var}(\\sum_i Y_i) &amp; = \\sum_i \\mathrm{Var}(Y_i) + \\sum_i \\sum_{j \\neq i}\\mathrm{Cov}(Y_i, Y_j) \\\\ &amp; &lt; \\sum \\mathrm{Var}(Y_i) \\end{align*}\\] if \\(\\mathrm{Cov}(Y_i, Y_j) &lt; 0\\). This suggests that the variance estimation \\({\\widehat{\\sigma}_X^2}/N\\) and \\({\\widehat{\\sigma}_Y^2/M}\\) are over-estimating the true variance of \\(\\mathrm{Var}(\\overline{X})\\) and \\(\\mathrm{Var}(\\overline{X})\\). However, because of the same negative correlation, \\[ \\mathrm{Var}(\\Delta) = \\mathrm{Var}(\\overline{Y}-\\overline{X}) &gt; \\mathrm{Var}(\\overline{Y})+\\mathrm{Var}(\\overline{X}), \\] leading to under-estimation of \\(\\mathrm{Var}(\\Delta)\\) when ignoring the dependency between \\(\\overline{Y}\\) and \\(\\overline{X}\\). Put these two ``errors’’ together, when we pretend two samples are i.i.d. from its own distribution and apply two sample t-test or z-test with \\[ \\mathrm{Var}_\\text{TS}(\\Delta) = \\mathrm{Var}(\\overline{Y})+ \\mathrm{Var}(\\overline{X}), \\] we are over-estimating the variance of both component on the RHS, but also missing a positive covariance between \\(\\overline{Y}\\) and \\(\\overline{X}\\), leading to under-estimation of the LHS. It is unclear \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) is over-estimating or under-estimating the true variance because the two opposing corrections without exact derivation. Imbens and Rubin (2015) analyzed the exact variance formula for completed randomized experiments, for both sample average treatment effect (SATE) and population average treatment effect (PATE). The results are comforting. For PATE, they showed that \\(\\mathrm{Var}_\\text{TS}(\\Delta) = \\frac{\\mathrm{Var}(Y)}{M} + \\frac{\\mathrm{Var}(X)}{N}\\) turns out to be the correct variance of \\(\\Delta\\). In other words, the two errors in different directions strike a perfect balance and the sum of the two has no error — the two sample problem, under a totally different data-generating-process, produces the correct variance. For SATE, the true variance is always smaller than the two sample variance \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\). Imbens and Rubin (2015) showed the correction term relies on the variance of individual treatment effect for the \\(M+N\\) units. When there is no individual treatment effect variation, the correction term is \\(0\\) so \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) again is exact. In general there is no estimator for the correction term available because individual treatment effects are not observable. \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) remains an close upper bound. Now we look at the independent randomization. Independent randomization for PATE is closer to the data-generating-process of two sample t-test and z-test, except that sample sizes \\(M\\) and \\(N\\) are not fixed but random. In the analysis of two sample problem when sample sizes are fixed, for \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) we used the fact \\[ \\mathrm{Var}(\\overline{Y}) = \\frac{\\mathrm{Var}(Y)}{M}. \\] When \\(M\\) is random, the above is not correct. We offer two justifications that we can pretend sample sizes are fixed in practice. The first justification is using large sample theory and the second justification is conditional test. Applying the conditional variance formula leads to \\[\\begin{align*} \\mathrm{Var}(\\overline{Y}) &amp; =\\mathrm{E}\\left( \\mathrm{Var}(\\overline{Y} | M) \\right) + \\mathrm{Var} \\left( \\mathrm{E} (\\overline{Y} | M) \\right ) \\\\ &amp; = \\mathrm{E}\\left( \\mathrm{Var}(Y)/M \\right). \\end{align*}\\] The last equality is because \\(\\mathrm{E} (\\overline{Y} | M) = \\mathrm{E}(Y)\\) is a constant with zero variance. Moving the constant \\(\\mathrm{Var}(Y)\\) out of the last expectation, \\[\\begin{equation} \\mathrm{Var}(\\overline{Y}) = \\mathrm{Var}(Y)\\times \\mathrm{E}(\\frac{1}{M}). \\tag{3.1} \\end{equation}\\] We see that the correct variance of \\(\\overline{Y}\\) depends on \\(\\mathrm{E}(\\frac{1}{M})\\) (and \\(\\mathrm{E}(\\frac{1}{N})\\) for \\(\\overline{X}\\)). (Technically the expectation here is a conditional expectation given \\(M&gt;0\\) and \\(N&gt;0\\) because the comparison does not make sense if only one group gets observations.) For independent randomization, \\(M\\) and \\(N\\) follows binomial distribution. By central limit theorem \\(M\\) is \\(O(\\sqrt{M})\\) away from its expectation \\(\\mathrm{E}(M)\\), and for \\(N\\) similarly. As \\(M+N\\) increases and when \\(M\\) and \\(N\\) are both moderately large, \\(\\frac{1}{M}\\), \\(\\mathrm{E}(\\frac{1}{M})\\) and \\(\\frac{1}{\\mathrm{E}(M)}\\) are all close enough to be practically equivalent and the same goes for \\(N\\). This is why for independent randomized experiment, in almost all applications we can treat the sample sizes as if they are fixed numbers. A proof of this requires using central limit theorem with the delta method and can be found in Section 11.2. The result holds not only for binomial distribution, but more general for any \\((M,N)\\) abiding the central limit theorem. Here we use simulation to illustrate the point. Figure shows the distribution of the ratio of \\(1/M\\) to \\(\\frac{1}{\\mathrm{E}(M)}\\) when \\(M\\) is from a binomial(K,0.5) distribution with increasing \\(K = M+N\\). The vertical line is the ratio of \\(\\mathrm{E}(\\frac{1}{M})\\) to \\(\\frac{1}{\\mathrm{E}(M)}\\). We see with \\(K = 1,000\\), the ratio of \\(\\mathrm{E}(\\frac{1}{M})\\) to \\(\\frac{1}{\\mathrm{E}(M)}\\) is already very close to 1. Moreover, \\(1/M\\) is almost always within 10% relative error for \\(\\frac{1}{\\mathrm{E}(M)}\\). A 10% relative error for variance only translate to 5% error for the standard error, and for the test statistics. This error is even smaller with larger sample sizes. Figure 3.1: Ratios of \\(1/M\\) and \\(\\frac{1}{\\mathrm{E}(M)}\\) as \\(N+M\\) increases from \\(100\\) to \\(100,000\\). vertical line is the ratio of \\(\\mathrm{E}(\\frac{1}{M})\\) to \\(\\frac{1}{\\mathrm{E}(M)}\\). The conditional test perspective offers another justification regardless of sample size. Note that the sample size \\(M\\) and \\(N\\) are irrelevant to our interest of comparing two distributions. \\(M\\) and \\(N\\) are called nuisance parameters. We can partition the data-generating-process into two components, one that involves the parameter of interest, and one that only involves nuisance parameters. One can argue that only the first part contains information for the inference of the parameter of interest. And the second part only adds noises that contains no useful information. By this argument, for the purpose of two sample comparison, we can condition on \\(M\\) and \\(N\\) by taking them out of the data-generating-process, meaning that we change the natural data-generating-process into a conditional version where we fix not only \\(N+M\\) but also \\(N\\) and \\(M\\). After fixing \\(N\\) and \\(M\\), the conditional data-generating-process reduces to the the two sample comparison and we can treat both \\(X_i\\) and \\(Y_i\\) are i.i.d.6 The last case to justify is independent randomization for SATE. After conditioning on sample sizes, the conditional data-generating-process is the same as complete randomization for SATE. And we have already justified using two sample t-test or z-test for this case. The analysis of randomized experiments are much sophisticated and richer than two sample problem described in this section. Other randomization mechanism such as cluster randomization and stratified randomization poses different data-generating-processes where \\(\\mathrm{Var}_\\text{TS}(\\Delta)\\) could deviate from the true variance of \\(\\Delta\\) a lot. Also, our causal estimand of interest might not be as simple as the difference in the mean. Moreover, there exists better experiment design and more accurate estimator (smaller variance) for average treatment effect. Randomized experiments have huge application in the Internet era for online industry, where it got a new name called A/B testing (or A/B/n testing for more than one treatment). We will see randomized experiments again in later chapters on randomized experiments with a focus on online application. Interference violate SUTVA assumption. See Section 4.4.↩︎ Technically it is a Welch’s t. Student’s original t-statistic assumes common variance of \\(X\\) and \\(Y\\) and used a pooled variance estimator.↩︎ In Bayesian statistics, we always condition on observations and infer the conditional distribution of the parameter of interest given observations. There is no such issue as nuisance parameter. In Frequentist statistics, conditional test often improves statistical power because it removes noises from nuisance parameters.↩︎ "],["rcm.html", "Chapter 4 Potential Outcomes Framework 4.1 Naive Estimation 4.2 Randomization and Unconfoundedness 4.3 Propensity Score 4.4 SUTVA 4.5 Missing Data and Weighted Samples 4.6 Missing Data Mechanisms and Ignorability 4.7 Importance Sampling 4.8 Inverse Propensity Score Weighting (IPW) 4.9 Doubly Robust Estimation 4.10 Bias-Variance Trade off and Covariates Overlap 4.11 Other Propensity Score Modeling Methods", " Chapter 4 Potential Outcomes Framework Consider a binary \\(Z= 0, 1\\) for control and treatment and we are interested in knowing the effect of \\(Z\\) on an outcome variable \\(Y\\). The potential outcome framework, also called Rubin-Causal-Model (RCM), augments the joint distribution of \\((Z,Y)\\) by two random variables \\((Y(1),Y(0))\\) — the potential outcome pair of \\(Y\\) when \\(Z\\) is \\(1\\) and \\(0\\) respectively.7 Since \\(Y\\) is the observed outcome and by definition we have \\[\\begin{equation} Y = \\left \\{ \\begin{array}{ll} Y(1) &amp; \\quad \\text{if } Z = 1, \\\\ Y(0) &amp; \\quad \\text{if } Z = 0 \\end{array} \\tag{4.1} \\right. \\end{equation}\\] When \\(Z=1\\), \\(Y(0)\\) is not observed and is the counterfactual, and when \\(Z=0\\), \\(Y(1)\\) is the counterfactual. Some authors prefer to use \\(Y^{\\textrm{obs}}\\) in place of plain \\(Y\\) to emphasize it is the observable and the above equation can be made more compact as \\[\\begin{equation*} Y^{\\textrm{obs}} = Z\\cdot Y(1) + (1-Z)\\cdot Y(0). \\end{equation*}\\] At the first look the introduction of counterfactual seems useless because it represents something that we don’t and can’t observe. They will not help us directly to estimate causal effect because we won’t be able to use any formula made of \\(((Y(1),Y(0))\\) if we don’t have observations to plug into the formula. Nevertheless, the introduction of potential outcome allows statisticians to conduct rigorous causal inference under the familiar joint probability distribution. Under the potential outcomes framework, the problem of identifying a causal effect becomes the challenge of inferring quantities about unobserved counterfactual using only observed. As the first step, now we can define causal effect rigorously. In the binary \\(Z\\) case, we call state \\(1\\) treatment and \\(0\\) control, and let \\[\\begin{equation} \\tau = Y(1) - Y(0), \\tag{4.2} \\end{equation}\\] to be the treatment effect of changing \\(X\\) from control to treatment, or simply the treatment effect of \\(Z\\). It should be noticed that potential outcome pair is a pair of random variables and treatment effect \\(\\tau\\) is also random. Usually \\(Y\\) represents certain measurement for an individual subject or unit, such as a person. When we randomly sample a unit from a population, the observation, denoted by \\(Y\\), is random. The treatment effect \\(\\tau\\) of \\(Z\\) on \\(Y\\) is also random. We define the (population) average treatment effect (ATE) to be \\[\\begin{equation} \\mathrm{E^*}(\\tau) = \\mathrm{E^*}\\left(Y(1) - Y(0)\\right) \\tag{4.3} \\end{equation}\\] We emphasize again Equation (4.3) cannot be directly used because we do not observe \\(Y(1)\\) and \\(Y(0)\\) simultaneously. We only observe \\(Y(1)\\) for treatment and \\(Y(0)\\) for control. In (4.3) we used the superscript \\(*\\) as a reminder that the expectation is taken under the augmented joint distribution \\(P^*\\), not the original joint distribution \\(P\\). ATE is the most common causal estimand used in practice, as it stands for the expected effect for a random units in a population. When given a sample of \\(N\\) units \\(Y_i, i=1,\\dots,N\\), we define the sample average treatment effect (SATE) to be \\[\\begin{equation} \\sum_i\\left(Y_i(1) - Y_i(0)\\right) /N \\tag{4.4}. \\end{equation}\\] SATE is the population average treatment effect (PATE) when the population is fixed to be the given sample. SATE is still a popular causal estimand in literature largely influenced by statistics pioneers like Fisher and Neyman in the complete randomized experiment setting. In this chapter we use ATE almost exclusively for population average treatment effect (PATE) (4.3). 4.1 Naive Estimation A naive attempt to estimate The ATE \\(\\mathrm{E^*}(\\tau)\\) is to use the following \\[\\begin{equation} \\widehat{\\tau}_\\text{naive} := \\overline{Y_T} - \\overline{Y_C} \\tag{4.5}, \\end{equation}\\] where \\(\\overline{Y_T}\\) is the average of treatment observations and \\(\\overline{Y_C}\\) for control. We call (4.5) the naive estimation. \\(\\overline{Y_T}\\) is an unbiased estimate of \\(\\mathrm{E}(Y|Z = 1)\\) and \\(\\overline{Y_C}\\) is an unbiased estimate of \\(\\mathrm{E}(Y|Z = 0)\\). Therefore the naive estimation (4.5) is an unbiased estimator of \\[\\begin{equation} \\mathrm{E}(Y|Z = 1) - \\mathrm{E}(Y|Z = 0) \\tag{4.6}. \\end{equation}\\] (4.6) is called the association effect of \\(Z\\) on \\(Y\\). Unlike the causal effect (4.3), association can be defined using the observed joint distribution \\(P\\). In general, association (4.6) and causation (4.3) are different, because \\(\\mathrm{E^*}(Y(z))\\) and \\(\\mathrm{E}(Y|Z = z)\\) are not the same. This is just another version of correlation does not imply causation. Associations can be caused by many confounders other than the cause of interest \\(Z\\). Figure 4.1 demonstrates this using causal graphical models. We will wait until section 5 to introduce causal graphical model. But the idea is very clear in Figure 4.1. There is a potential causal link from \\(Z\\) to \\(Y\\). But there might exist confounder \\(U\\) which can impact \\(Z\\) and \\(Y\\) simultaneously. Figure 4.1: Confounder. Confounders are the main hurdle of causal inference and why causal inference without randomized experiments are difficult. It is now a common belief that smoking can cause lung cancer. The link between smoking and lung cancer had been suspected for a long time based on many observational studies. But none of them are as definitive as a randomized experiment because it is just impossible to randomize people and force them to smoke or not smoke. Fisher, who popularized randomized experiments in statistics and well understood correlation does not imply causation, publicly spoke out against a 1950 study showing a positive association between smoking tobacco and lung cancer (R. Fisher 1958). One of his arguments, is that there may be a genetic predisposition to smoke and that genetic predisposition is presumably also linked to lung cancer. In other words, there might exist a gene, a confounder that both increase a person’s tendency of smoking and the likelihood of developing lung cancer. 4.2 Randomization and Unconfoundedness Despite not generally applicable, the naive estimation is the basis of more sophisticated method. Equipped with the potential outcome framework, we not only can clearly see the difference between association and causation, but are also able to further develop methods that can help us isolate confounding effects to recover causal effect. The distinction of \\(P\\) and \\(P^*\\), hence \\(\\mathrm{E^*}\\) and \\(\\mathrm{E}\\) is precisely what we need to conquer. Fortunately, under certain conditions, we can rewrite a causal quantity defined with counterfactuals in the augemented world, e.g. involving \\(P^*\\) and \\(\\mathrm{E^*}\\), into a form that only involves \\(P\\)(and/or \\(\\mathrm{E}\\)). The latter can in turn can be estimated with observations since they were drawn from \\(P\\). In these cases, we say the causal quantity of interest can be identified and we have an identification strategy. We already know randomization dispels all confounders and naive estimation works in randomized experiments. We first generalize the idea of randomization in the following definition of unconfoundedness, also called ignorability.8 Definition 4.1 (Unconfoundedness/Ignorability) The treatment indicator \\(Z\\) is said to be unconfounded or ignorable if in the potential outcome augmented distribution \\(P^*\\), \\[\\begin{equation} (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z \\tag{4.7}. \\end{equation}\\] First, randomization implies unconfoundedness. Recall independence means \\(Z\\) is irrelevant for predicting \\((Y(1),Y(0))\\). For all three randomization mechanisms mentioned earlier, treatment assignment \\(Z\\) is produced using an exogenous procedure (complete randomization, independent randomization, cluster randomization) totally unrelated to anything else. Knowing the value of \\(Z\\) lends us no extra knowledge toward predicting \\((Y(1),Y(0))\\) since the distribution of potential outcomes in different treatment groups are still the same. Unconfoundedness (4.7) is the mathematical incarnation of all-other-thing-being-equal! Secondly, association implies causation without confounding. To see that, \\[\\begin{equation*} \\mathrm{E}(Y|Z = 1) = \\mathrm{E^*}(Y(1)|Z = 1) = \\mathrm{E^*}(Y(1)), \\end{equation*}\\] where the first equality is from the definition (4.1) and the second is from \\(Z\\) independent of \\(Y(1)\\). Similarly, \\[\\begin{equation*} \\mathrm{E}(Y|Z = 0) = \\mathrm{E^*}(Y(0)). \\end{equation*}\\] Put them together, \\[\\begin{equation*} \\mathrm{E^*}(\\tau) = \\mathrm{E^*}\\left(Y(1) - Y(0)\\right) = \\mathrm{E}(Y|Z = 1) - \\mathrm{E}(Y|Z = 1). \\end{equation*}\\] We’ve derived the equivalence between causation (4.3) and association (4.6). Since naive estimation (4.5) unbiasedly estimate association, it also is unbiased for ATE, under the condition of unconfoundedness. Let us take a second look at the simple proof as it contains the essence of potential outcome framework. The ATE (4.3), defined with counterfactual, can be rewritten as the difference of two conditional expectations involving only observables. This key transition hinges on the additional assumption being made regarding the independence between \\(Z\\) and \\((Y(1),Y(0))\\). As we will shortly see, \\(Z\\) and \\((Y(1),Y(0))\\) being independent is not the only condition that allows such transition from potential outcome distribution \\(P^*\\) to observation distribution \\(P\\). 4.2.1 Conditional Unconfoundedness, Matching and Covariates Balancing Unconfoundedness seems too restrictive to be applicable to anything beyond randomized experiments. In most observational studies, confounders do exist. Such cases require us to isolate confounding effects. This can be achieved by conditioning on confounders, leading to the concept of conditional unconfoundedness. Definition 4.2 (Covariates) A random variable \\(X\\) is a covariate if the treatment \\(Z\\) is known to have no effect on \\(X\\). Covariate is often very easy to find as they generally represent side (exogenous) or baseline or pre-treatment information. For this reason they are also called attributes or pre-treatment variables. In the kidney stone example, patients’ stone size prior to being treated is a pre-treatment variable, and therefore cannot be affected by the choice of treatments. Discrete covariates are also commonly referred to as segments or strata. Definition 4.3 (Conditional Unconfoundedness/Strong Ignorability) The treatment indicator \\(Z\\) is said to be conditionally unconfounded or strongly ignorable given a set of covariates \\(X\\) if in the potential outcome augmented distribution \\(P^*\\), \\[\\begin{equation} (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | X \\tag{4.8}. \\end{equation}\\] As the name suggests, Definition 4.3 is a conditional version of Definition 4.1. Conditional unconfoundedness means all confounders have already been identified and accounted for by the set of covariates \\(X\\).9 When covariates \\(X\\) are all discrete, within any given stratum \\(X = x\\), there is no confounding effect remains and causal effect can be identified by naive estimation. The overall ATE can be identified by aggregating over all strata. The procedure of forming strata and using naive estimation within each strata is called matching. The following derivation assuming discrete \\(Z\\) and \\(X\\) provides a rigorous proof10. Under the assumption of conditional unconfoundedness, \\[\\begin{align} P^*(Y(1) = y) &amp; = \\sum_x P^*(Y(1)=y |X=x)P^*(X=x) \\notag \\\\ &amp; = \\sum_x P^*(Y(1)=y |X=x, Z=1)P^*(X=x) \\notag \\\\ &amp; = \\sum_x P^*(Y = y |X=x, Z=1)P^*(X=x) \\notag \\\\ &amp; = \\sum_x P(Y = y |X=x, Z=1)P(X=x). \\tag{4.9} \\end{align}\\] The second equality is from (4.8). The last equality is from the fact that \\(P\\) and \\(P^*\\) are the same for all observables \\(X\\), \\(Z\\) and \\(Y\\). Similarly \\[\\begin{equation} P^*(Y(0) = y) = \\sum_x P(Y = y |X=x, Z=0)P(X=x). \\tag{4.10} \\end{equation}\\] For ATE \\(\\mathrm{E^*}\\left(Y(1) - Y(0)\\right)\\), from (4.9) and (4.10) \\[\\begin{align} &amp;\\mathrm{E^*}\\left(Y(1) - Y(0)\\right) \\notag \\\\ &amp; = \\sum_x \\left(\\mathrm{E}(Y = y |X=x, Z=1) - \\mathrm{E}(Y = y |X=x, Z=0)\\right)\\times P(X=x). \\tag{4.11} \\end{align}\\] Let \\(\\overline{Y_T(x)}\\) and \\(\\overline{Y_C(x)}\\) be the sample average of observed \\(Y\\) in treatment and control groups for a stratum \\(X = x\\) and \\(\\widehat{\\mu}_\\text{naive}(x) = \\overline{Y_T(x)} - \\overline{Y_C(x)}\\) be the naive estimation on the stratum. Also let \\(\\widehat{p}(x)\\) be the proportion of stratum \\(X = x\\) in the total observations. An unbiased estimate for ATE in the form of (4.11) is \\[\\begin{equation} \\widehat{\\mu}_\\text{match} : = \\sum_x \\widehat{\\mu}_\\text{naive}(x) \\times \\widehat{p}(x). \\tag{4.12} \\end{equation}\\] We call Equation (4.12) the (exact) matching estimate. It is a sum of naive estimations weighted by the observed proportion of each stratum \\(X=x\\). Intuitively we are matching subjects having the same covariates \\(X\\). In the kidney stone example, we already identified pre-treatment stone size as a possible confounder. Matching on kidney stone means comparing two treatment on each stone size group separately and then aggregate up to get an overall estimate for all patients. This will resolve the Simpson’s paradox as shown in Table 2.1. The obvious challenges regarding matching estimator (4.12) is that naive estimation for each stratum requires a reasonable number of units in each stratum. In other words, how many units can we match together for each different values of \\(X\\). If the matching produce sparse strata, i.e. those with small number of units either from treat or control group. The naive estimation for these strata would have high variance, making the overall ATE estimation high variance too. This kind of sparseness can easily happen if one covariate have too many levels or be continuous, or if we put too many discrete covariates in \\(X\\) so the total number of combinations is large. To overcome sparseness after matching, one approach is to notice that it is not necessary to match perfectly on every individual levels of \\(X\\). The rationale behind matching is that when confounders exist, we want to isolate confounding effects so that any correlation (association) between \\(Z\\) and \\(Y\\) cannot be attributed by correlation of \\(Z\\) and any confounder. Fixing the value of confounder by exact conditioning certainly can make sure \\(Z\\) is independent of all confounders (any random variable is independent of a fixed value). But this is sufficient but not necessary. All we need to make sure is \\(Z\\) is independent of confounders \\(X\\). When \\(Z\\) is independent of \\(X\\), the distribution \\(X\\) and conditional distribution of \\(X\\) given \\(Z\\) are the same. Equivalently speaking, the distribution of \\(X\\) for control group \\(Z=0\\) and the distribution for treatment group \\(Z=1\\) are the same. In this sense there should be no difference in the distribution of any confounders in \\(X\\) between the two groups and the two groups should be balanced for covariates \\(X\\). This suggests it is sufficient to form strata such that distribution of covariates \\(X\\) is balanced between treatment and control and conditioning on the strata. Definition 4.4 (Balancing Score) A balancing score \\(b(X)\\) is a function of covariates \\(X\\) such that \\[\\begin{equation} X\\perp \\! \\! \\! \\! \\perp Z | b(X) \\tag{4.13}. \\end{equation}\\] A trivial balancing score is \\(b(X)\\) = X. Theorem 4.1 If covariates \\(X\\) ensure conditional unconfoundedness (4.8), e.g. \\[\\begin{equation*} (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | X, \\end{equation*}\\] and \\(b(X)\\) is a balancing score. Then \\[\\begin{equation*} (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | b(X). \\end{equation*}\\] In other words, \\(b(X)\\) also ensure conditional unconfoundedness. Proof. Since \\(b(X)\\) is a function of \\(X\\), \\[ (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | X \\implies (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | X , b(X). \\] From the definition of balancing score, \\[ X \\perp \\! \\! \\! \\! \\perp Z | b(X). \\] Combine the two together with the contraction rule of conditional independence in Appendix 12.1.1 entails, \\[ (Y(1),Y(0)) \\perp \\! \\! \\! \\! \\perp Z , X | b(X), \\] which implies \\[ (Y(1),Y(0)) \\perp \\! \\! \\! \\! \\perp Z | b(X) \\] from the rule of decomposition. Being a function of \\(X\\), \\(b(X)\\) will be coarser than \\(X\\). When \\(X\\) are all discrete, \\(b(X)\\) is also discrete and will have less number of levels than \\(X\\). Theorem 4.1 says \\(b(X)\\) is all we need to condition on for naive estimation to work. For the purpose of matching, we only need to match on a balancing score \\(b(X)\\). Note \\(b(X)\\) might not be univariate. (e.g. The trivial case \\(b(X) = X\\) when \\(X\\) is multivariate.) When \\(X\\) are discrete, we derive a special balancing score called propensity score. In the next section we show how to extend propensity score to general continuous case and prove the optimal property of propensity score as the coarsest balancing score. Let \\(N_T(x), N_C(x), N(x)\\) be the sample sizes of treatment, control, and total size for the stratum \\(X = x\\). Let \\(N\\) be the total sample size. Then \\(\\widehat{p}(x) = N(x)/N\\). Define the propensity score \\(e(x)\\) to be \\(N_T(x)/N(x)\\). \\(e(x)\\) is the proportion of samples within the stratum \\(X = x\\) that are in the treatment group. Let \\(N(e)\\), \\(N_T(e)\\) and \\(N_C(e)\\) be sample sizes when stratified by the propensity score, i.e. \\[\\begin{equation*} N_T(e) = \\sum_{e(x) = e} N_T(x),\\quad N_C(e) = \\sum_{e(x) = e} N_C(x), \\\\ N(e) = \\sum_{e(x) = e} N(x). \\end{equation*}\\] We can reorganize the matching estimator as the following. Let \\((X_i, Z_i, Y_i), i=1,\\dots,N\\) be the observations. \\[\\begin{align} \\sum_x &amp; \\widehat{\\mu}_\\text{naive}(x) \\times \\frac{N(x)}{N} = \\frac{1}{N} \\sum_x \\left (\\frac{\\sum_{X_i = x, Z_i = 1} Y_i}{N_T(x)/N(x)} - \\frac{\\sum_{X_i = x, Z_i = 0} Y_i}{N_C(x)/N(x)}\\right) \\notag \\\\ &amp; = \\frac{1}{N}\\left ( \\sum_{Z_i = 1} \\frac{Y_i}{e(X_i)} - \\sum_{Z_i = 0} \\frac{Y_i}{1-e(X_i)} \\right). \\tag{4.14} \\end{align}\\] We now regroup samples by their values of \\(e(X_i)\\), and noting \\(N_T(e)/N(e) = e\\), \\(N_C(e)/N(e) = 1-e\\), (4.14) equals \\[\\begin{align} &amp; \\frac{1}{N}\\sum_e\\left ( \\sum_{e(X_i) = e, Z_i = 1} \\frac{Y_i}{e} - \\sum_{e(X_i) = e, Z_i = 0} \\frac{Y_i}{1-e} \\right)\\notag\\\\ &amp; = \\frac{1}{N}\\sum_e\\left ( \\sum_{e(X_i) = e, Z_i = 1} \\frac{Y_i}{N_T(e)/N(e)} - \\sum_{e(X_i) = e, Z_i = 0} \\frac{Y_i}{N_C(e)/N(e)} \\right)\\notag \\\\ &amp; = \\sum_e\\left ( \\left ( \\frac{\\sum_{e(X_i) = e, Z_i = 1} Y_i}{N_T(e)} - \\frac{\\sum_{e(X_i) = e, Z_i = 0} Y_i}{N_C(e)} \\right) \\times \\frac{N(e)}{N} \\right) \\notag \\\\ &amp; = \\sum_e \\left (\\widehat{\\mu}_\\text{naive}(e) \\times \\frac{N(e)}{N} \\right ).\\tag{4.15} \\end{align}\\] (4.15) is the exact matching estimator when matched by \\(e(X)\\) instead of \\(X\\) directly. (4.14) has a form of weighted average of \\(Y_i\\). Make a note of this form as we will return to this very soon. There is an intuitive connection between propensity score stratification and randomized experiment. We have seen unconfoundedness allows us to treat the observed data as if it were collected from a randomized experiment and use the naive estimation to unbiasedly estimate ATE. Conditional unconfoundedness is weaker for that unconfoundedness holds only for every stratum \\(X = x\\). Within each stratum, we can pretend the data to be collected from a randomized experiment using independent randomization, with probability \\(e(X)\\) to be assigned to treatment. In this connection, conditional unconfoundedness allows us to pretend the data set \\((Y_i, X_i, Z_i)\\) was collected using a conditional randomized experiment, in which for each given \\(X\\), a unit has a probability \\(e(X)\\) to be assigned to treatment. It is easy to see that we can reduce the complexity of these conditional randomized experiments by combining strata with the same \\(e(X)\\) together as one randomized experiment because the assignment probabilities are the same. This reduction is also the optimal in the sense we cannot combine two randomized experiment with two different assignment probabilities together. (4.15) is a direct consequence from the conditional randomized experiment view. 4.3 Propensity Score Under the conditional unconfoundedness assumption (4.8), define propensity score to be a function of covariates \\(X\\) as \\[\\begin{equation} e(X) := P(Z = 1 | X) \\tag{4.16}. \\end{equation}\\] That is, \\(e(X)\\) is the probability a unit will be associated with \\(Z=1\\) (be treated) conditioned on the given \\(X\\). It is also the expected proportion of units with \\(Z=1\\) among units with the same value of \\(X\\). The following theorem says propensity score is the best balancing score. Theorem 4.2 (Optimality of Propensity Score) \\(e(X)\\) is a balancing score. That is, \\[\\begin{equation*} X \\perp \\! \\! \\! \\! \\perp Z | e(X). \\end{equation*}\\] Moreover, \\(b(X)\\) is a balancing score if and only if \\(e(X)\\) is a function of \\(b(X)\\). Theorem 4.2 says not only is the propensity score \\(e(X)\\) a balancing score, it is the coarsest one possible. Also this definition works for both discrete or continuous covariates. Proof. We only need to prove the second part. The first part follows directly from the second part by realizing \\(e(X)\\) is \\(\\mathit{id}(e(X))\\) for the identify function \\(\\mathit{id}\\). For the if part, if \\(e(X) = f(b(X))\\), we need to show the distribution of \\(Z\\) given \\(X\\) and \\(e(X)\\) is the same as the distribution of \\(Z\\) given only \\(e(X)\\). Since \\(Z\\) is binary, we only need to show \\[\\begin{equation} P(Z = 1 | X, e(X)) = P(Z=1|e(X)). \\tag{4.17} \\end{equation}\\] For the LHS \\(P(Z=1|X, e(X)) = P(Z = 1 | X) = e(X)\\) because \\(e(X)\\) is redundant given \\(X\\). For the RHS, \\[\\begin{align*} P(Z=1|e(X)) = \\mathrm{E}\\left( \\mathbf{1}_{Z=1} | e(X) \\right) = \\mathrm{E}\\left( \\mathrm{E}\\left( \\mathbf{1}_{Z=1} | X, e(X) \\right) |e(X) \\right) \\\\ = \\mathrm{E}\\left( e(X) | e(X)\\right) = e(X). \\end{align*}\\] LHS = RHS, hence \\(e(X)\\) is a balancing score. For the only if part. If \\(e(X)\\) is not a function of \\(b(X)\\), then there must exists two \\(x\\) and \\(x&#39;\\) such that \\(b(x) = b(x&#39;)\\) but \\(e(x)\\neq e(x&#39;)\\). We proved the RHS of (4.17) is \\(e(X)\\). So RHS is not the same for \\(X = x\\) and \\(X = x&#39;\\). The LHS of (4.17) is \\(P(Z = 1 |X)\\) and by definition of balancing score, \\(P(Z = 1 |X) = P(Z = 1 |b(X))\\). Therefore LHS for \\(X = x\\) and \\(X = x&#39;\\) must be the same if \\(b(x) = b(x&#39;)\\). We have reached a contradiction. Hence (4.17) must be false. Propensity score plays a central role in matching and covariate balancing. Under the conditional unconfoundedness assumption, matching on the propensity score is equally good at removing confounding effect as matching on all confounders. It reduces the dimension of matching from arbitrary high dimension to only one dimension. Being the coarsest balancing score, propensity score provides the largest sample size for each matched stratum. When \\(X\\) is continuous, \\(e(X)\\) could still be continuous. Exact matching on even a univariate continuous scale is sparse. There are two options in general. First option is to forgo exact matching. Approximate matching can be performed using a distance metric such as Mahalanobis distance to match each unit with its nearest neighbors. For a univariate matching score, simple discretization is also used. In both cases the unbiasedness property of the matching estimator will be disturbed and it is a price to pay for more robust estimator. More on bias-variance trade off in Section 4.10. Fortunately, propensity score can also be used beyond exact matching. We can treat propensity score, or more exactly the inverse of propensity score, as a reweighting of samples. This perspective is closely connected to importance sampling in Monte Carlo simulation, and missing data perspective of the potential outcome framework. See Section 4.8. In the next section, we have to talk about the single most important assumption underneath the potential outcome framework — the Stable Unit Treatment Value Assumption, or SUTVA. 4.4 SUTVA Potential outcome framework naturally extends to non-binary treatment \\(Z\\) and even continuous \\(Z\\). In this case we define \\(Y_z\\) as the potential outcome of \\(Y\\) if \\(Z = z\\). Equation (4.1) becomes \\[ Z = z \\Rightarrow Y_z = Y. \\] Keen readers may have sensed the potential outcome framework as we presented seems to be oversimplified. Indeed it is not complete unless we bring up the following key assumption called stable unit treatment value assumption. Definition 4.5 (SUTVA) The potential outcomes for any unit(observation) do not vary with the treatments assigned to other units. In a nutshell, SUTVA means there is no interference between different observations. Let \\(X\\) be binary for illustration and there are two observations \\((Y_1, Z_1)\\) and \\((Y_2,Z_2)\\). If the potential outcomes of the first observation depends not only on \\(Z_1\\) but also on \\(Z_2\\), then there are 4 total scenarios! We would need 4 potential outcomes for \\(Y_1\\), and similarly for \\(Y_2\\). Often the total number of observations can be much larger, leading to a huge number of treatment assignment scenarios — all combinations of the assignment vector \\(\\mathbf{Z}\\). Without SUTVA, the dimensionality of potential outcomes for each unit can be easily unmanageable even for a handful of observations. Moreover, it also increases when number of units increase. Both the model and math required to analyze it easily gets complicated and unwieldy. For this reason, SUTVA is the implicit assumption underneath most results developed under the potential outcome framework. When there is no mention of SUTVA, we should still treat SUTVA as an assumption when using potential outcome framework, unless interference or violation of SUTVA is explicitly called out. SUTVA should not be confused with independent observations assumption. For a simple example, consider a binary treatment with constant treatment effect. e.g. \\(Y(1) = Y(0) + \\mu\\). SUTVA is satisfied here even when \\(Y(0)\\) of different units can be dependent and therefore observation \\(Y_i\\)s are not independent of each other. In practice, independence is still a common and reasonable assumption to make, or at least we assume independence at some level that might be different from the original analysis unit. For example in cluster randomized experiment, clusters are typically assumed to be independent of each other, even though the clustering might introduce dependencies between analysis units. 4.5 Missing Data and Weighted Samples In the previous sections we’ve seen how the potential outcome framework work leads us to the matching estimator (4.12). We relies a bit on rigorous mathematical derivations and didn’t spend much time explaining the natural intuition. Here is a recap of what we did We introduced potential outcomes by augmented the joint distribution of observed by unobserved counterfactuals. We defined causal effect such as ATE under the augmented joint distribution \\(P^*\\). We identified conditional unconfoundedness as the condition under which we can express ATE using only the joint distribution \\(P\\) for the observed, leading to matching estimator. We show exact matching can be relaxed to matching on a balancing score. Propensity score is the coarsest balancing score. Starting from this section we take another look at potential outcomes using a slightly different, more intuitive yet mathematically equivalent view. Instead of focusing on the distinction between the augmented joint distribution \\(P^*\\) and original distribution \\(P\\), we just treat \\(P^*\\) as the only joint distribution governs all observations. However, we treat the unobserved counterfactuals as missing data! To be specific, we assume there is an oracle that observed all potential outcomes. But after she observed everything, she will remove all counterfactuals and keep only the potential outcome associated with the observed assignment. What’s the difference between the two views? Mathematically they are equivalent, just two different ways of telling the story of the same data-generating-process. In the augmented joint distribution view, we need to keep two distributions in check. We use \\(P^*\\) to define causal estimands, and \\(P\\) to form estimates. And we need to consciously keep track of whether we can re-express any estimands defined in \\(P^*\\) using \\(P\\). In the missing data perspective, there is only one distribution — \\(P^*\\) to keep in mind. The difficulty lies in the fact some observations are missing and we need to do inference about \\(P^*\\) using observed data only. In the authors’ subjective opinion, there are a few advantages of the missing data perspective: The idea of missing data is easier to apprehend and convey without using heavy math notations. Concepts like missing completely at random speaks for itself. Missing data is an important problem occurring in many fields. There are many theories and results existed. Missing data naturally connects the concepts of randomness (missing at random), sample reweighting, and missing data imputation together. 4.6 Missing Data Mechanisms and Ignorability Let \\(Z\\) be the indicator of missingness for an observation \\(Y\\). \\(Z=1\\) if \\(Y\\) is observed and \\(Z=0\\) if missing. Definition 4.6 (Missing Completely at Random) A data missing mechanism is Missing Completely at Random (MCAR) if the event of a value missing occurs entirely at random. In this case the missingness of any data is independent of everything else. In particular, \\(Y \\perp \\! \\! \\! \\! \\perp Z\\). Definition 4.7 (Missing at Random) A data missing mechanism is Missing at Random (MAR) if the missing data mechanism is entirely at random, or conditionally at random after conditioning on a set of completely observed variables \\(X\\). In particular, \\(Y \\perp \\! \\! \\! \\! \\perp Z | X\\). There is a clear connection between MCAR and unconfoundedness (4.7), and MAR and conditional unconfoundedness (4.8). The difference is there is no potential outcome pair \\((Y(1),Y(0))\\), just one \\(Y\\). More specifically, when counterfactuals are MAR, both \\(Y(1)\\) and \\(Y(0)\\) are MAR. Therefore, \\[ Y(1) \\perp \\! \\! \\! \\! \\perp Z | X, \\quad \\text{and} \\quad Y(0) \\perp \\! \\! \\! \\! \\perp Z | X, \\] which is weaker than the conditional unconfoundedness condition \\[ (Y(1),Y(0)) \\perp \\! \\! \\! \\! \\perp Z | X, \\] but is equally powerful for majority of cases. Exercise: Show \\(A \\perp \\! \\! \\! \\! \\perp X\\) and \\(B \\perp \\! \\! \\! \\! \\perp X\\) does not imply \\(A,B \\perp \\! \\! \\! \\! \\perp X\\). The name ignorability for unconfoundedness and strong ignorability for conditional unconfoundedness are better understood using missing data perspective. In both cases it refers to the notion that the fact there are missing data can be ignored as if they are complete data. This is obvious if missing data is MCAR because the observed data distribution is the same as the complete data distribution. For MAR, the missing data is ignorable within each stratum. But the probability of missing may vary from stratum to stratum. This means for MAR, within each stratum, the distribution of observed data represents the distribution of the complete data. But when all strata combined together, the proportion or weight of each stratum for observed data is different from that of the complete data, because some strata have more data missing while some have less. In other words the weights of different strata are skewed due to different impact of data missing. If we can correct the observed weights into their original weights in the complete data, then we will be able to correct the distribution of observed data so that it faithfully represents the complete data distribution. How should each stratum be reweighted? If the missing rate for a stratum is higher than average, then it is more impacted by the missing event, and hence under-represented in the observed data. If missing rate is lower than average, this stratum will be over-represented. This is the key observation leads to the inverse propensity score weighting (IPW). The idea of reweighting comes from importance sampling in Monte Carlo simulation. It is an essential concept and a must-have tool in every data scientist’s toolbox. We will talk about it first before returning to inverse propensity score weighting in Section 4.8. Here is a short summary of connections among randomized experiments, unconfoundedness and missing data we have observed so far: Randomized Experiment \\(\\implies\\) unconfoundedness/ignorability \\(\\iff\\) \\(Y(1)\\) and \\(Y(0)\\) are MCAR Conditional Randomized Experiment \\(\\implies\\) conditional unconfoundedness/strong ignorability \\(\\iff\\) \\(Y(1)\\) and \\(Y(0)\\) are MAR If \\(Y(1)\\) is MAR, the event of \\(Y(1)\\) missing follows an independent Bernoulli distribution with the probability equal to the propensity score \\(e(X) = P(Z = 1 |X)\\). Similarly, \\(Y(0)\\) follows the same distribution with probability \\(1-e(X)\\) of missing. Under MCAR the fact that there are data missing can be ignored. For MAR, the impact of missing data is measured by the propensity score. 4.7 Importance Sampling Definition 4.8 For any distribution \\(P\\), if a sample \\(X_i, i = 1, \\dots, n\\), is drawn \\(i.i.d.\\) from the distribution. The \\(n\\) samples form a discrete distribution \\(P_n\\) with \\(n\\) point of masses. We call \\(P_n\\) the empirical distribution. The idea is that the empirical distribution \\(P_n\\) provide an unbiased representation of \\(P\\) and it converges to \\(P\\) as \\(n\\) increases. A rigorous understanding of the convergence here requires theories like Glivenko–Cantelli theorem and Donsker’s theorem; see endnotes. Heuristically speaking, the histogram of \\(X_i, i=1,\\dots,n\\) must converge to the density function of \\(P\\). This heuristics suggest that for estimating any quantity defined by the theoretical distribution \\(P\\), a natural estimator is to replace \\(P\\) with its empirical version \\(P_n\\). This estimator should be consistent, i.e., it converges to the statistical quantity of interest and asymptotically unbiased if \\(P_n\\) correctly represents \\(P\\). It is even better if the estimator after proper standardization could also be asymptotically normal. A set of very useful results from theoretical asymptotic statistics showed our naive wishes are mostly true under mild regularity conditions. For causal inference, effect on mean and quantiles are of major interest. With i.i.d. sample from \\(P\\), estimating mean is straightforward. Theorem 4.3 ((Naive) Simulation) If \\(X_i, i = 1, \\dots, N\\), is drawn \\(i.i.d.\\) from distribution \\(P\\). Then for any function \\(f\\) with finite expectation \\(\\mathrm{E_P}\\left(f(X)\\right)\\), the simple average \\[ \\mathrm{E_{P_n}}\\left (f(X)\\right) = \\overline{f(X)} = \\frac{\\sum_i f(X_i)}{N} \\] is an unbiased estimator. Moreover, \\[ \\mathrm{E_{P_n}}\\left (f(X)\\right) \\to E_P\\left(f(X)\\right) \\quad a.s. \\] Theorem 4.3 is a direct application of strong law of large numbers. It is the foundation of Monte Carlo Simulation/Integration. For any statistical quantify of interest that can be expressed as an expectation of a, the empirical expectation, i.e. the average \\(\\overline{f(X)}\\), can be used as an unbiased estimator. We can also employ the central limit theorem to see \\(\\overline{f(X)}\\) will be asymptotically normal provided \\(f(X)\\) has finite variance. Let \\(F\\) be the distribution function of \\(P\\) and \\(F_n\\) be the empirical version. p-quantile is defined as \\(F^{-1}(p)\\) and sample p-quantile can be defined as \\(F_n^{-1}(p)\\). Theorem 4.4 If \\(F\\) is differentiable at \\(F^{-1}(p)\\) with positive density \\(f(F^{-1}(p))\\), then \\[\\begin{equation} \\sqrt{n}\\left( F_n^{-1}(p) - F^{-1}(p) \\right) \\xrightarrow{d} Normal\\left(0, \\frac{p(1-p)}{f(F^{-1}(p))^2}\\right) \\tag{4.18} \\end{equation}\\] Theorem 4.4 says for continuous random variable with density function \\(f\\), the empirical quantile estimator is asymptotically unbiased with normality. For discrete random variable, the empirical quantile estimator is still asymptotically unbiased, but not asymptotically normal because empirical quantile is always discrete. However it should still be close to a normal distribution and we can use continuity correction to make it even closer. A unified view of both mean and quantile is to treat both as solutions of their own corresponding optimization problems. Theorem 4.5 Median of a random variable \\(X\\) with distribution \\(P\\) is the solution of the following optimization problem: \\[ \\min_{\\theta} \\mathrm{E_P}\\left( |X - \\theta| \\right). \\] More generally, the \\(p\\)-quantile is the solution of \\[ \\min_{\\theta} \\mathrm{E_P}\\left( \\psi(X)\\right), \\] where \\[\\begin{equation*} \\psi(x) = \\left \\{ \\begin{array}{ll} (1-p)|x| &amp; \\quad \\text{if } x &lt; 0, \\\\ 0 &amp; \\quad \\text{if } x = 0,\\\\ p|x| &amp; \\quad \\text{if } x &gt; 0 \\end{array} \\right. \\end{equation*}\\] As a comparison, mean of a random variable \\(X\\) with distribution \\(P\\) is the solution of the following optimization problem: \\[ \\min_{\\theta} \\mathrm{E_P}\\left( (X - \\theta)^2 \\right). \\] Proof of Theorem 4.5 is left as an exercise. Note that the derivative of \\(\\psi(x)\\) is \\(-(1-p)\\) if \\(x&lt;0\\), \\(p\\) if \\(x&gt;0\\) and the subderivative for \\(x=0\\) is \\([-(1-p), p]\\). Quantile and mean share the same empirical form as the solution of minimizing the expectation of a parameterized function \\(\\psi_\\theta(x)\\) of a random variable. By simply replacing the theoretical distribution \\(P\\) by its empirical version \\(P_n\\), solving the empirical version of the same minimization problems leads to the empirical mean and empirical quantile. By the heuristics of \\(P_n\\) converging to \\(P\\), we expect the solutions of the empirical problem also converge to solutions of the theoretical problem. This is the topic of M-estimator (and closely related Z-estimator). M-estimators are asymptotically normal under mild assumptions. See Section 11.3. We saw empirical distribution can be a really powerful aid. But to get an empirical distribution, so far we have been assuming we know how to draw \\(i.i.d.\\) samples from \\(P\\).11 In some cases drawing from a distribution \\(P\\) might be very hard or even infeasible. In a nutshell, importance sampling allows us to represent a distribution empirically using samples drawn from another. Theorem 4.6 (Change of Measure/Radon–Nikodym) If probability measure \\(P\\) is absolutely continuous with respect to another probability measure \\(Q\\), then there exists a random variable \\(LR\\) such that \\[\\begin{equation} \\mathrm{E_P} \\left(f(X)\\right) = \\mathrm{E_Q} \\left( f(X) \\times LR \\right).\\tag{4.19} \\end{equation}\\] The random variable \\(LR\\) is called the Radon-Nikodym derivative in measure theory. It is unique up to a null set of \\(Q\\) (almost surely unique w.r.t. to \\(Q\\)). The following special non-measure-theoretic form is sufficient for practice. If both \\(P\\) and \\(Q\\) have density function denoted by \\(dP = p(x)\\) and \\(dQ = q(x)\\), and \\(q(x) = 0\\) always implies \\(p(x) = 0\\), then \\[ LR = \\frac{dP(X)}{dQ(X)} = \\frac{p(X)}{q(X)} \\quad a.s. \\] i.e., the Radon-Nykodym derivative is the likelihood ratio. Theorem 4.6 is called change of measure because is transform an expectation under \\(P\\) using an expectation under \\(Q\\) by reweighting random variable \\(X\\) from \\(Q\\) by the likelihood ratio. The idea is obvious but a rigorous proof requires measure theory and is beyond our scope nor is it essential for data scientists. See the endnotes of this chapter for further readings. Change of measure leads to importance sampling. Corollary 4.1 (Importance Sampling) If \\(X_i, i = 1, \\dots, N\\), is drawn \\(i.i.d.\\) from distribution \\(Q\\) and \\[ LR_i = L(X_i) = \\frac{dP(X_i)}{dQ(X_i)} \\] be the likelihood ratio of \\(P\\) to \\(Q\\). Then \\[\\begin{equation} \\widehat{\\mu}_\\text{IS}:= \\overline{f(X)\\times LR} = \\frac{\\sum_i f(X_i) \\times LR_i}{N} \\tag{4.20} \\end{equation}\\] is an unbiased estimator for \\(\\mathrm{E_P}\\left(f(X)\\right)\\). If only given the likelihood ratio up to a constant, i.e. \\[ w_i \\propto LR_i = \\frac{dP(X_i)}{dQ(X_i)}. \\] An asymptotically unbiased self-normalized estimator for \\(\\mathrm{E_P}\\left(f(X)\\right)\\) is \\[\\begin{equation} \\widehat{\\mu}_\\text{IS-SN} := \\frac{\\sum_i f(X_i) \\times w_i}{\\sum_i w_i} \\tag{4.21} \\end{equation}\\] The self-normalized estimator \\(\\widehat{\\mu}_\\text{IS-SN}\\) (4.21) is a ratio estimator and is not finite sample unbiased. Nevertheless, it is asymptotically unbiased and converges to \\(\\mathrm{E}_P\\left(f(X)\\right)\\) almost surely. Both (4.20) and (4.21) are asymptotically normal with additional mild (finite seond moment) conditions. Given i.i.d. weighted samples drawn from \\(Q\\), the variance for the importance sampling estimator \\(\\widehat{\\mu}_\\text{IS}\\) (4.20) is \\[\\begin{equation*} \\frac{1}{N}\\left(\\mathrm{E_Q}\\left( f(X)^2LR^2\\right) - \\mu^2\\right), \\end{equation*}\\] where \\(\\mu = \\mathrm{E}_P\\left(f(X)\\right)\\). It can be estimated by \\(\\widehat{\\sigma}^2/N\\), where \\[\\begin{equation} \\widehat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N f(X_i)^2LR_i^2 - \\overline{f(X)\\times LR}^2. \\tag{4.22} \\end{equation}\\] The variance of the self-normalized estimator \\(\\widehat{\\mu}_\\text{IS-SN}\\) (4.21) is trickier. Using the delta method, the asymptotic variance of \\(\\sqrt{N}(\\widehat{\\mu}_\\text{IS-SN} - \\mu)\\) is \\[ \\mathrm{E_Q} \\frac{ \\left( (f(X)W - \\mu W)^2 \\right) }{\\mathrm{E_Q}(W)^2} \\] and can be estimated by \\[\\begin{equation} \\widehat{\\sigma}^2_\\text{SN} = \\frac{\\frac{1}{N} \\sum_i w_i^2(f(X_i)-\\widehat{\\mu}_\\text{IS-SN})^2 }{\\left(\\frac{1}{N}\\sum_i w_i\\right)^2}. \\tag{4.23} \\end{equation}\\] An approximate variance for \\(\\widehat{\\mu}_\\text{IS-SN}\\) (4.21) is just \\(\\widehat{\\sigma}^2_\\text{SN}/N\\). When using importance sampling (IS), each \\(i.i.d.\\) observation comes together with a weight \\(W_i\\) equals the likelihood ratio of the target distribution \\(P\\) to the operating distribution \\(Q\\). We call them weighted samples. When weights are integers, this is conceptually equivalent to replicating each observation \\(X_i\\) for \\(W_i\\) copies. Note that this conceptual equivalence is only for unbiased representation of the target distribution \\(Q\\), not including independence since replicas are not independent. The non-integer weights can also be interpreted as replicates as long as you are comfortable with the idea of non-integer number of replicates. Exercise: how to define sample quantile using weighted samples? We use Figure 4.2 to illustrate the effect of likelihood ratio reweighting. We first simulate 10,000 i.i.d. samples from Poisson(5) distribution. Top left in Figure 4.2 shows the empirical distribution \\(P_n\\) in its histogram. If we wish to turn this empirical distribution into a \\(Q_n\\) that mimics Poisson(3), we need to give each sample \\(X_i\\) a weight equal to the probability of observing \\(X_i\\) under Poisson(3) to that under Poisson(5). This is shown in the two lower plots, where probability mass function of Poisson(5) and Poisson(3) are overlaid and the likelihood ratio is shown. The top right is the weighted histogram. We see its shape is very close to the probability mass function of a Poisson(3) distribution, marked by the black points. Figure 4.2: Illustration of Importance Sampling to transform a Poisson(5) sample into a weighted sample representing Poisson(3). Top left: Histogram of 10,000 i.i.d. Poisson(5) samples. Top right: Histogram of the 10,000 samples weighted by likelihood ratio of Poisson(3) to Poisson(5) compared to the Poisson(3) density (black points). Lower left: overlay of Poisson(5) and Poisson(3) histograms. Lower right: Likelihood ratio of Poisson(3) to Poisson(5). Importance sampling and the concept of weighted samples are examples of simple yet powerful ideas. They are widely applied in different theoretical and applied disciplines and essential for every data scientist. We will see how inverse propensity score weighting could be considered as a special case of importance sampling. 4.8 Inverse Propensity Score Weighting (IPW) Recall the missing data perspective postulate the complete data distribution \\(P^*\\) where \\(Y(1)\\) and \\(Y(0)\\) are all observed no matter \\(Z = 1\\) or \\(Z = 0\\). Let \\(P\\) be the distribution of the observed. Under MAR, we know the observed distribution \\(P\\) is a skewed version of the complete data distribution \\(P^*\\) and we also know the mechanism of how it is skewed. IPW is to transform \\(P\\) back to \\(P^*\\), so that we can estimate \\[ \\mathrm{E^*}\\left( Y(1) \\right)\\ \\text{and}\\ \\mathrm{E^*}\\left( Y(0) \\right). \\] Let’s focus on \\(Y(1)\\) first as \\(Y(0)\\) is very similar. From the importance sampling perspective, our observations are samples from \\(P\\) and our target distribution is \\(P^*\\). Let \\(p\\) and \\(p^*\\) be their densities. The weights should be equal to the likelihood ratio between the two \\(\\frac{dP^*}{dP} = \\frac{p^*}{p}\\). How do we compute the likelihood ratio of \\(P^*\\) to \\(P\\)? We need to use MAR. MAR means the data generating process for \\(Y\\) given \\(X\\) is independent of the missing process, so we can decompose the density \\(p\\) as \\[ p(x,y) \\propto p^*(x,y,z=1) = p^*(x) \\times p^*(y|x)\\times P^*(z = 1|X=x) = p^*(x,y)\\times e(x), \\] where \\(e(x)\\) is the propensity score \\(P^*(z=1|x)\\). Reorganize this equation leads to \\[ \\frac{p^*(x,y)}{p(x,y)} \\propto \\frac{1}{e(x)}. \\] We derived the likelihood ratio to transform \\(P\\) to \\(P^*\\) is proportional to the inverse of propensity score! If you are curious to know the normalzation constant, it is \\[ \\frac{p^*(x,y)}{p(x,y)} = \\frac{1}{e(x)} \\times P^*(Z = 1) \\] A unbiased estimator for the normalization constant \\(P^*(Z=1)\\) is simply \\(N_T/N\\). The importance sampling estimator for \\(\\mathrm{E^*}\\left( Y(1)\\right)\\) is \\[\\begin{equation} \\widehat{\\mu}_{IPW}(Y(1)) = \\frac{\\sum_{Z=1}Y_i \\times \\frac{N_T/N}{e(X_i)}}{N_T} = \\frac{\\sum_{Z=1}{Y_i/e(X_i)}}{N} \\tag{4.24} \\end{equation}\\] The self-normalized importance sampling estimator is \\[\\begin{equation} \\widehat{\\mu}_{IPW-SN}(Y(1)) = \\frac{\\sum_{Z=1}Y_i/e(X_i)}{\\sum_{Z=1} 1/e(X_i)} \\tag{4.25} \\end{equation}\\] IPW estimator for \\(\\mathrm{E^*}\\left( Y(0)\\right)\\) is similar with the only notable difference being the likelihood ratio is the inverse of \\(1-e(X_i)\\), i.e., the propensity of the control observation being missed. The IPW is (4.26) and the self-normalized IPW estimator is (4.27) \\[\\begin{align} \\widehat{\\mu}_{IPW}(Y(0)) &amp; = \\frac{\\sum_{Z=0} Y_i \\times \\frac{N_C/N}{1-e(X_i)}}{N_C} = \\frac{\\sum_{Z=0}{Y_i/(1-e(X_i))}}{N} \\quad \\tag{4.26} \\\\ \\widehat{\\mu}_{IPW-SN}(Y(0))&amp; = \\frac{\\sum_{Z=0} Y_i/(1-e(X_i))}{\\sum_{Z=0} 1/(1-e(X_i))} \\tag{4.27} \\end{align}\\] Unbiased estimator for the ATE can be derived using the difference of unbiased estimator for \\(\\mathrm{E^*}(Y(1))\\) and \\(\\mathrm{E^*}(Y(0))\\). For example, the self-normalized IPW estimator for ATE is \\[\\begin{equation} \\Delta_\\text{IPW-SN} = \\frac{\\sum_{Z=1}Y_i/e(X_i)}{\\sum_{Z=1} 1/e(X_i)} - \\frac{\\sum_{Z=0} Y_i/(1-e(X_i))}{\\sum_{Z=0} 1/(1-e(X_i))} \\tag{4.28}. \\end{equation}\\] The notion of weighted sample and importance sampling can easily allow us to derive causal estimands like average treatment effect of treated (ATT). Let \\(P_T\\) be the distribution of treated units, i.e. \\(P_T(X,Y(1),Y(0)) = P^*(X,Y(1),Y(0)|Z = 1)\\). For \\(Y(1)\\), there is no missing data so IPW is not needed and sample average of treatment observations serves as an unbiased estimate. For \\(Y(0)\\), the observed distribution is \\(P_C(X,Y(0)) = P^*(X, Y(0)|Z = 0)\\) and we want to estimate \\(\\mathrm{E_{P_C}}\\left( Y(0)\\right) = \\mathrm{E^*}\\left( Y(0) |Z=1\\right)\\) using observations from \\(P_C\\). The likelihood ratio of \\(P_T\\) to \\(P_C\\) is \\[ \\frac{dP_T}{dP_C} \\propto \\frac{p^*(x)p^*(y|x)p^*(z=1|x)}{p^*(x)p^*(y|x)p^*(z=0|x)} = \\frac{p^*(z=1|x)}{p^*(z=0|x)} = \\frac{e(x)}{1-e(x)}. \\] We can easily transform from \\(P_C\\) to \\(P_T\\) by using (unnormalized) importance weight \\(\\frac{e(x)}{1-e(x)}\\). The rest is left as an exercise. Exercise: Derive normalization constant for \\(\\frac{e(X)}{1-e(X)}\\) w.r.t. the distribution \\(P_T\\). Derive IPW and self-normalized IPW estimator of \\(\\mathrm{E_{P_T}}\\left( Y(0)\\right)\\). 4.9 Doubly Robust Estimation In Section 4.8 we have shown reweighting observed data using the inverse propensity score as weights transforms the observed data into a weighted sample representing the complete data distribution. The purpose of this reweighting/transformation is so that we can infer statistical quantities, e.g. mean and percentiles, of the complete data distribution. In the basic case when the missing data is missing completely at random, the observed data distribution and the complete data distribution are the same — corresponds to a single propensity score for all observed data. In randomized experiments, the designed proportion of each group is the common propensity score. Reweighting is a powerful tool, but it still relies on missing at random condition, given a set of completely observable covariates \\(X\\), and a good estimation of the propensity score \\(e(X) = P(Z = 1|X)\\). An even more naive idea to handle missing data is to impute the missing values, using covariates \\(X\\) as predictors. For potential outcomes, we want to impute \\(Y(1)\\) for control units and \\(Y(0)\\) for treatment units. Since we do observe \\(Y(1)\\) and \\(X\\) for the treatment group, we can surely fit a regression model of \\(Y(1)\\) on \\(X\\). For control group, we still observe \\(X\\). Can we apply the same model we just learned and apply it to the control group to impute missing \\(Y(1)\\)? The answer is yes under MAR. Recall under MAR condition, \\(Y(1)\\) is independent of \\(Z\\) given \\(X\\). So knowing \\(Z\\) will not help us better predicting \\(Y(1)\\) using \\(X\\). This suggests that there is no point of applying different regression models of \\(Y(1)\\) on \\(X\\) for the cases of \\(Z=1\\) and \\(Z=0\\)! By the same argument, we can regress \\(Y(0)\\) on \\(X\\) using control data, and apply the same model to impute missing \\(Y(0)\\) values for the treatment group. Table 4.1 illustrates this idea. Table 4.1: Illustration of missing data imputation using regression function \\(f_1\\) for predicting \\(Y(1)\\) and \\(f_0\\) for \\(Y(0)\\). Missing data are marked as NA. i \\(Z\\) \\(Y(1)\\) \\(\\widehat{Y(1)}\\) \\(Y(0)\\) \\(\\widehat{Y(0)}\\) \\(X\\) Inverse Propensity 1 1 \\(Y_1(1)\\) \\(f_1(X_1)\\) NA \\(f_0(X_1)\\) \\(X_1\\) \\(1/e(X_1)\\) 2 1 \\(Y_2(1)\\) \\(f_1(X_2)\\) NA \\(f_0(X_2)\\) \\(X_2\\) \\(1/e(X_2)\\) 3 1 \\(Y_3(1)\\) \\(f_1(X_3)\\) NA \\(f_0(X_3)\\) \\(X_3\\) \\(1/e(X_3)\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(N-2\\) 0 NA \\(f_1(X_{N-2})\\) \\(Y_{N-2}(0)\\) \\(f_0(X_{N-2})\\) \\(X_{N-2}\\) \\(1/(1-e(X_{N-2}))\\) \\(N-1\\) 0 NA \\(f_1(X_{N-1})\\) \\(Y_{N-1}(0)\\) \\(f_0(X_{N-1})\\) \\(X_{N-1}\\) \\(1/(1-e(X_{N-1}))\\) \\(N\\) 0 NA \\(f_1(X_{N})\\) \\(Y_{N}(0)\\) \\(f_0(X_N)\\) \\(X_N\\) \\(1/(1-e(X_{N}))\\) The regression is unbiased if it recover the theoretical regression conditional expectation for all \\(X\\). If we have unbiased prediction for both \\(Y(1)\\) and \\(Y(0)\\), i.e. \\[ f_0(X) = \\mathrm{E}(Y(0)|X), \\ \\text{and} \\ f_1(X) = \\mathrm{E}(Y(1)|X), \\quad a.s. , \\] then by taking expectation over covariates \\(X\\), \\[ \\mathrm{E}(Y(0)) = \\mathrm{E}(f_0(X)) \\ \\text{and} \\ \\mathrm{E}(Y(1)) = \\mathrm{E}(f_1(X)). \\] We can just use \\(\\overline{f_1(X)}\\) and \\(\\overline{f_0(X)}\\) to unbiasedly estimate \\(\\mathrm{E}(Y(1))\\) and \\(\\mathrm{E}(Y(0))\\), and hence \\[ \\Delta_\\text{Reg} := \\overline{f_1(X)} - \\overline{f_0(X)} \\] is an unbiased estimator for the average treatment effect.12 We call \\(\\Delta_\\text{Reg}\\) the regression estimator. The regression model can be learn by fitting two regression models using treatment and control group data separately. There are no constraints on what kind of regression model to use. It can be linear or nonlinear, parametric or nonparametric, even ensemble methods. Consider the linear regression models \\[ Y_i(1) \\sim \\beta_0(1) + \\beta(1)\\times X \\] and \\[ Y_i(0) \\sim \\beta_0(0)+ \\beta(0)\\times X. \\] They can be combined together as one linear regression model with \\[ Y_i(Z_i) \\sim \\beta_0 + \\alpha_0 Z_i+ (\\beta + \\alpha Z)\\times X, \\] which is a linear model with interaction between treatment assignment \\(Z\\) and covariates \\(X\\). Assuming any regression model to be unbiased is a strong assumption. The true regression models are rarely linear and two complicated nonlinear model with high degree of freedom can lead to severe over-fit. To make the unbiasedness of \\(Y\\) model even more difficult, we can only fit \\(f_1\\) using treatment data and we want the model to work for both treatment and control data! This is like training a model in one data set and hope this model works the same on a test data set which is known to be different from the training data. Therefore in practice it is a bad idea to rely on unbiasedness of the \\(Y\\) model and just apply regression model to estimate causal effect on an observational data. On the other hand, IPW requires us to have a good estimate for propensities \\(e(X) = P(Z=1|X)\\), and does not rely on predicting performance of \\(Y\\) using \\(X\\). Can we combine these two orthogonal modeling approaches, where one tries to predict \\(Z\\) and the other tries to predict \\(Y\\)? Turns out, by combining two, we can get the better of both. The result estimator will be (asymptotically) unbiased if either the \\(Y\\) model, or the propensity \\(e\\) model is unbiased. Moreover, doubly robust estimator can also be more efficient, reducing variance compared to IPW estimator when \\(e\\) model is unbiased. There are different ways to construct a doubly robust estimator (and there are different estimators with the DR property). Here we introduce one from a bias correction perspective. Suppose we have fit \\(f_1\\) and \\(f_0\\) as in Table 4.1. If both models are unbiased, we can just use \\(\\Delta_\\text{Reg}\\). If they might be not, we want to correct the bias of \\(\\overline{f_1(X)}\\) and \\(\\overline{f_0(X)}\\). Let us consider \\(\\overline{f_1(X)}\\) first. We do observe \\(Y_i(1)\\) for those in the treatment group, and can get the residuals \\(\\epsilon_i = Y_i(1)-f_1(X)\\). \\(\\overline{\\epsilon}\\) is the empirical estimator for the bias on the treatment group. Recall that we want \\(\\overline{f_1(X)}\\) to be unbiased not only for treatment group by for both treatment and control. To do that we need to transform the empirical distribution of those residuals from the treatment group into one that represents both groups. We’ve already used IPW to transform \\(Y_i(1)\\) and we can use the same on residual \\(\\epsilon_i(1) = Y_i(1)-f_1(X)\\)! This leads to the bias corrected regression estimator of \\(\\mathrm{E}(Y(1))\\): \\[ \\overline{f_1(X_i)} + \\frac{\\sum \\epsilon_i(1) /e(X_i)}{\\sum 1/e(X_i) }. \\] Similarly for \\(\\mathrm{E}(Y(0))\\) we have \\[ \\overline{f_0(X_i)} + \\frac{\\sum \\epsilon_i(0) /(1-e(X_i))}{\\sum 1/(1-e(X_i))}. \\] The DR estimator for average treatment effect is \\[\\begin{equation} \\Delta_\\text{DR} := \\left(\\overline{f_1(X_i)} - \\overline{f_0(X_i)} \\right)+ \\left(\\frac{\\sum \\epsilon_i(1) /e(X_i)}{\\sum 1/e(X_i) } - \\frac{\\sum \\epsilon_i(0) /(1-e(X_i))}{\\sum 1/(1-e(X_i))}\\right). \\tag{4.29} \\end{equation}\\] To see (4.29) is doubly robust. When \\(Y\\) model is unbiased, residual \\(\\epsilon_i(1)\\) and \\(\\epsilon_i(0)\\) have \\(0\\) mean for each \\(X_i\\) so the two correction terms both have mean \\(0\\) conditioned on \\(X_i,i=1,\\dots,N\\) (and hence conditioned on \\(e(X_i)\\)). As a result the unconditional mean is also \\(0\\). So the correction terms both have mean \\(0\\) and \\(\\Delta_\\text{DR}\\) has the same mean of \\(\\Delta_\\text{Reg}\\) and are both unbiased with unbiased \\(Y\\) model. Now consider the case where only the propensity model \\(e\\) model is unbiased. In this case by the property of IPW, \\(\\frac{\\sum \\epsilon_i(1) /e(X_i)}{\\sum 1/e(X_i)}\\) converges to \\(\\mathrm{E}(Y(1)-f_1(X))\\) and similarly for \\(\\mathrm{E}(Y(0)-f_0(X))\\). Therefore, \\[ \\mathrm{E}(\\Delta_\\text{DR}) = \\mathrm{E}(f_1(X)) - \\mathrm{E}(f_0(X)) - \\mathrm{E}(Y(1)-f_1(X)) - \\mathrm{E}(Y(0)-f_0(X)) = \\mathrm{E}\\left(Y(1)-Y(0)\\right). \\] There are more ways to combine IPW with regression. When training the \\(Y\\) models, because we want both models to perform well on the complete data (treatment and control), we can train models on the IPW samples such that the reweighted raining data align better with the complete data. For linear regression, the model can be fit using weighted least square. Since the sum of residuals are always 0 in least square. Doing so means the correction terms \\(\\frac{\\sum \\epsilon_i(1) /e(X_i)}{\\sum 1/e(X_i) }\\) and \\(\\frac{\\sum \\epsilon_i(0) /(1-e(X_i))}{\\sum 1/(1-e(X_i))}\\) are both 0 so the correction term disappear. The weighted least square estimator is doubly robust without any further correction. This is not the case for general regression models. When the \\(e\\) model is reasonably close to the truth, training on weighted samples can significantly improve the model performance if treatment and control have very different distributions of \\(X\\). DR estimators not only improves consistency and asymptotic unbiasedness, they are also shown to be more efficient when either \\(Y\\) model or \\(e\\) model is correct. Robins and Rotnitzky (1995) showed DR estimate (4.29) has the smallest asymptotic variance among all practically useful asymptotically unbiased estimator. (The technical term is Regular and Asymptotic Linear (RAL), also see Tsiatis (2006).) This is a very strong result. Unfortunately this optimality property only holds when either \\(Y\\) model or \\(e\\) model is correct and results in Kang and Schafer (2007) showed DR estimator can have poor performance and even worse than IPW when both \\(Y\\) and \\(e\\) models are misspecified. For randomized experiments, \\(e\\) model is known to the experimenter as part of the design. Therefore, DR estimator (4.29) is guaranteed to attain the best efficiency. Let \\(n_1 = \\sum_i Z_i\\) and \\(n_0 = \\sum_i (1-Z_i)\\), (4.30) becomes \\[\\begin{equation} \\overline{Y_T} - \\overline{Y_C} - \\sum_i (Z_i - \\overline{Z})(f_1(X_i)/n_1 + f_0(X_0)/n_0) \\tag{4.30} \\end{equation}\\] (4.30) has been studied in Yang and Tsiatis (2001) where the authors showed simple linear regression model works surprisingly well in practice even when the true regression model is nonlinear. This technique of using a misspecified model to improve efficiency is called efficiency augmentation. Exercise: When \\(e(X) = p\\) is fixed, show (4.30) and (4.29) are the same. 4.10 Bias-Variance Trade off and Covariates Overlap The exact matching estimator (4.12) depends on the assumption that for every covariate \\(X\\), we can find samples from both treatment and control group (or every variant groups for cases of more than one treatment). Exact matching is impossible covariate is continuous or dimensionality of covariates is high. Propensity score reduces matching dimension down to 1, alleviating the high dimensionality issue. It also offers inverse propensity score weighting as an alternative to exact matching, solving the continuous matching problem. We can think of IPW as fuzzy or soft matching. However, propensity score and IPW has its own issue and practical application or propensity score rarely only use IPW. Propensity model can be mis-specified and the score might be biased from \\(P(Z=1|X)\\) making it unreliable to be used as weight. Because the weights are the inverse of propensity. When \\(e(X)\\) is close to 0, \\(1/e(X)\\) will be large. Similarly, when \\(e(X)\\) is close to 1, \\(1/(1-e(X))\\) will be large. Large weights lead to high variance of the IPW estimator (4.28). For the first issue, instead of using inverse propensity score as weights directly, we can treat propensity score as a way of reducing dimensionality of high dimensional covariates so that we only need to match on the one dimensional propensity score. The rationale is matching on propensity score only rely on it being a balancing score. If the model is correct, then propensity score is a balancing score (and the optimal one). Even if the model might be misspecified, the propensity score might still be a valid balancing score if there exists a one to one mapping from the mis-specified score to the true propensity score. In other words, matching on the mis-specified score maps to matching on the true propensity score implicitly. In this sense using propensity scores for matching could be more robust than using it for weighting. The second issue is more fundamental. If the propensity model is not far from the truth, a propensity score close to either 0 or 1 indicates lack of overlapping between the two covariate distributions. Figure 4.3: Two Distributions have large lack-of-overlapping areas with propensity function (of being from the distribution on the right). area outside the two vertical lines have propensity scores close to 0 or 1, indicating poor matching and high variance. Figure 4.3 shows an example of two distribution that are apart. Outside of the central region between 4.5 and 7.5, propensity scores are less than 0.05 or larger than 0.95. A 0.05 propensity score means a 5:95 sample size ratio between the two distributions, and 0.95 propensity means 95:5. Either case shows a drastically imbalanced sample ratio. The problem of highly imbalanced sample ratio? High variance! For two sample t-test, since the variance of the \\(\\Delta\\) is proportional to \\[ \\frac{\\sigma_T^2}{N_T}+\\frac{\\sigma_C^2}{N_C} \\] where \\(\\sigma_T\\) and \\(\\sigma_C\\) are standard deviation of the treatment and control distribution. For illustration purpose, let’s assume the two standard deviations are the same. Then the variance of the \\(\\Delta\\) is proportional to the sum of inverse \\[ \\frac{1}{N_T}+\\frac{1}{N_C}. \\] For a fixed total size \\(N = N_T+N_C\\), Figure 4.4 plots the sum of inverse as a function of the proportion of \\(N_T\\). We see that it is smallest when \\(N_T = N_C\\) and increase to infinity when \\(N_T/N\\) is either close to 0 or 1. Figure 4.4: \\(1/N_T+1/N_C\\) as function of \\(N_T/N\\) when \\(N=N_T+N_C\\) is fixed. The connection to matching is obvious. Since the purpose of exact matching or propensity matching is such that for each matching stratum, the observed data are as if they were from a conditional randomized experiments where treatment and control are randomized according to \\(e(X):(1-e(X))\\) ratio. We estimate the conditional average treatment effect (CATE) for each stratum and then aggregate to the ATE. The variance of ATE depends on variance of each CATEs. If one of it has high variance, the ATE will also be impacted. If some strata have \\(e(X)\\) close to 0 or 1, the variance of the corresponding CATE will be much larger than those stratum where \\(e(X)\\) is close to 1/2. Variance of ATE will be large and dominated by a few very large variances contributed by those small number of strata. In the extreme case of \\(e(X)\\) equals 0 or 1, the matching fails completely. There exist either treatment group observations, or only control group observations, not both. This is equivalent to running a 100% vs. 0% randomized experiment and it is obvious in this total none-overlapping case that we cannot estimate the causal effect from only treatment or control observations. Equivalently we can reach to the same conclusion that \\(e(X)\\) close to 0 or 1 leads to high variance of the IPW estimator (4.28) using the variance formula (4.23). There is no silver bullet to overcome the lack-of-overlapping/none-overlapping problem. Practically solutions can be categorized into two orthogonal directions. Remove observations with propensity scores close to 0 or 1. Modify the propensity score model by removing some covariates from \\(X\\). Change the weights explicitly. For the first approach, throwing away observations means we give up on estimating the average treatment effect for the whole population. For those regions where the propensity scores under the current model is close to 0 or 1, as we explained above, estimating CATE for these strata requires a lot of observations and is very inefficient because either treatment or control will only be a very small proportion of the sample. By leaving these strata out of the picture, we leave ourselves with an estimator that only estimates the average treatment effect on the well-overlapped region — a biased estimator if our goal is average treatment effect for whole population. But this biased estimator has much smaller variance. In other words, we are trading off variance with bias. Moreover, the bias here is explicitly known as not accounting effect from those lack-of-overlapping areas. For the second approach, why can we modify the propensity score model by removing some covariates? The theory of propensity score and IPW does not mandate that there is only one propensity score model. In fact, there can be infinitely many. All we need is that the covariates \\(X\\) satisfies the conditional unconfoundedness condition \\[ (Y(1),Y(0))\\perp \\! \\! \\! \\! \\perp Z | X . \\] Imagine there exists a smallest set of covariates \\(X^*\\) satisfying the above condition. If we remove any one covariate from \\(X^*\\), the condition will fail. But it is possible that we can still add more covariates into the set \\(X^*\\) and still maintains the conditional independence condition. The effect of adding more covariates? Potential more prediction power for \\(Z\\) given \\(X\\)! However, unlike traditional supervised learning problem where more prediction power is better, in propensity modeling, more prediction power leads to more propensity scores close to 0 or 1 — a nightmare for variance consideration. On the contrary, randomized experiments with a balanced treatment allocation (50% vs. 50%) corresponds to a propensity score being exactly 0.5 and there is no prediction power of any covariate set \\(X\\) to the treatment assignment \\(Z\\) since assignment is purely random. But randomized design using balanced assignment provides the best case scenario for estimating average treatment effect. Therefore, the purpose of propensity modeling is not for predicting treatment assignment \\(Z\\). Its real intention is for providing the correct weight for IPW estimation or simply dimension reduction for matching. Pursuing prediction accuracy when modeling propensity score does not help and to the contrary it only leads to high variance. This is a fundamental difference between propensity modeling and predictive modeling. As an illustration, in the kidney stone example in Table 2.1 of Chapter 2, suppose the size of the stone is the only confounder needed to meet the conditional unconfoundedness requirement. Suppose treatment B is only offered in limited locations. In this case, knowing location information of a patient can greatly improves prediction power for which treatment a patient will choose. For patient outside of these limited locations, she can only pick treatment A, and we can 100% accuracy if our goal is to predict \\(Z\\). When conditioning on both kidney stone size and location, for these locations where treatment B is not available, there is of course no treatment B observations. In other words, keeping location as covariate to condition on leads to none-overlapping strata. But if kidney stone size alone meets the conditional unconfoundedness condition, we only need to condition on kidney stone size. Without location in the propensity score model, we no longer get extreme scores like 0 or 1. The central issue here is how do we know whether a set of covariates \\(X\\) satisfies conditional unconfoundedness. Unfortunately there is no test. In practice, it is usually safer to put more covariates at first^[To the surprises of many, it is not true that it is safe to pick as many covariates as possible. Sometimes conditioning on certain covariates may create confoundness! We then check whether we get extreme propensity scores and as a result need to throw away some covariates that have high prediction power for assignment but at the same time deemed to be low in confounding effect. Of course the judgment of whether a covariate is likely to have confounding effect or not is subjective and with each covariate removed we are running under greater risk of missing confounding effect so the propensity scores are biased. In this sense, removing covariates should be justified as trading confidence in unbiasedness for reducing variance. But at least these subjective choices can be clearly documented so that the assumptions are part of the results. For the third approach, Li, Morgan, and Zaslavsky (2018) proposed to use weight proportional to \\(1-e(X)\\) for treatment and \\(e(X)\\) for control. Comparing to the weights in IPW where treatment weight is proportional to \\(1/e(X)\\) and control weight proportional to \\(1/(1-e(X))\\), these new weights will never explode even for \\(e(X)\\) close to 0 and 1 (these will get very small weights). Intuitively, this has the similar effect as if we remove those non-overlapping region of \\(X\\). Li, Morgan, and Zaslavsky (2018) call this overlap weight and showed that this weighting method unbiasedly estimate the average treatment effect under a reweighted population distribution of \\(X\\) by putting an weight of \\(e(X)(1-e(X)\\). That is, by using the overlap weight, we explicitly avoid the task of estimate the ATE under the true population distribution where non-overlapping issue exists; instead, we give small weights to non-overlapping areas and larger weights to overlapping areas to mitigate the issue of exploding weight. This can be seen as a soft version of the first approach where we give 0 weight for observations in non-overlapping areas. 4.11 Other Propensity Score Modeling Methods Propensity scores \\(P(Z=1|X)\\) have to be learned from the data unless the data generation process is known as in randomzed experiments. Traditional approach takes the data pairs \\((Z,X)\\) and fit a probabilistic model such as a logistic regression model to predict the propensity scores. These parametric models are subject to model mis-specification. This is why in practice directly using IPW with logistic regression fitted propensity score often give unreliable results and people will either use matching on propensity score and use propensity score merely as dimension reduction tool instead of weights, or use doubly robust estimation. Imai and Ratkovic (2014) realized that the MLE of a logistic regression model for propensity modeling is equivalent to minimize the imbalance of the sigmoid function’s partial derivative with respect to the parameter (sigmoid function is a function of covariate \\(X\\) and model parameter) between treatment and control. Since covariate \\(X\\) is not affected by treatment, ideally any function of covariate \\(X\\) should be balanced. Why just optimize for this partial derivative of a sigmoid function? In light of this, Imai and Ratkovic (2014) proposed to fit any parametric propensity score model using method of moments for a set of balancing equations. This covariance balancing propensity score (CBPS) method generalized logistic regression and other parametric models by making the optimization goal of the model fitting explicit in forms of balancing arbitrary functions of \\(X\\). This idea of direct optimization of covariate balance make the problem of propensity score modeling into a constrained covariate blancing optimization problem, which can also be extended to nonparametric model. For instance, Hainmueller (2012) proposed entropy balancing to choose individual weights directly without a parametric model. Also see Athey, Imbens, and Wager (2018). The idea of potential outcome is first introduced by J. Neyman in his 1925 master thesis on application of randomized experiments in agriculture where he talked about “potential yield.”↩︎ The name “ignorability” stems from the missing data view. It means the missing data mechanism can be ignored and we can treat them to be missing completely at random. See Section 4.5.↩︎ For this reason some authors prefer the name complete unconfoundedness to emphasize no confounders are left outside.↩︎ This proof is borrowed from Pearl (2009) with notation changes↩︎ Actually independence is not required but variances of Monte Carlo estimators will be harder to estimate without independence.↩︎ Here and after \\(\\mathrm{E}\\) is taken under the complete data probability measure \\(P^*\\) unless otherwise explicitly specified.↩︎ "],["cgm.html", "Chapter 5 Causal Graphical Model 5.1 Structural Equation Model, Causal Diagram and d-separation 5.2 the do operator 5.3 The Back-door Criterion 5.4 Causal Mechanism and the Front-door Criterion 5.5 General Identification Strategy 5.6 RCM vs. CGM", " Chapter 5 Causal Graphical Model Need more work to turn notes into readable chapter. Here is the outline of this section. Law of physics is a set of formulas that allow us to model the causes and effects in a deterministic world. Structural equation model is the natural analogy that describes the data-generating-process of a set of random variables using a set of equations. These models are often Markovian, i.e. each variable only affects its decedents and there is no cycles or loopy effect. Equivalently, any Markovian model can be factorized into a set of conditional probability models that corresponds to each structural equation. Many key information in a structural equation model can be visually encoded into a directed acyclic graph/Bayesian network, including many conditional and unconditional dependence relationships among these random variables. A useful tool called d-separation allows us to easily identify conditional and unconditional independence from the graph. The effect of changing a variable by intervention is called a do operation. do operation changes the factorized Markovian model into a truncated factorization. Knowing the truncated factorization allows us to study the effect of the do operation. The analysis of do operation is called do calculus. The goal of do calculus is to turn an expression with do operation into a do-free expression involving only observable variables so we can come up with an estimator. Front-door and Back-door criteria represents two different identification strategies and the morals of these two criteria can be extended to build up identification strategies for more complex graphs. 5.1 Structural Equation Model, Causal Diagram and d-separation A directed graph \\(G\\) represents a set of vertices with arrowed connections. Mathematically it is a set of vertices \\(V\\) and an edge set \\(E\\) of ordered pairs of vertices where the ordering represents the direction of the arrow. Each vertex or node will correspond to a random variable. If the ordered pair \\((X, Y)\\in E\\) then there is an arrow pointing from \\(X\\) to \\(Y\\), and we say \\(X\\) is a parent of \\(Y\\) and \\(Y\\) is a child of \\(X\\). A directed path between two vertices is a path with arrows all pointing to the same direction. \\(Z\\) is said to be a descendant of \\(X\\) if there is a directed path leading from \\(X\\) to \\(Z\\). A directed path that starts and ends with the same node is called a cycle. A directed graph is acyclic if it has no cycles. A directed graph \\(G\\) without cycle is called a directed acyclic graph or DAG. For this chapter we forbid any cycles (loopy effect) and leave the analysis of feedback loops outside of our scope. From now on we only focus on DAG. A DAG \\(G\\) with vertices \\(V=(V_1,\\dots,V_k)\\) is often associated with a Markovian model defined in the following. Definition 5.1 (Markovian Model/Bayesian Belief Network) If \\(P\\) is distribution on \\(V\\) with probability function \\(f\\), we say \\(P\\) is Markov to \\(G\\), or that \\(G\\) represents \\(P\\), if \\[\\begin{equation} f(v) = \\prod_i f(v_i|\\pi_i) \\tag{5.1} \\end{equation}\\] where \\(\\pi_i\\) is are parents of \\(V_i\\). Markovian models are also called Bayesian Networks or Belief Networks. When a distribution \\(P\\) is Markov to a DAG \\(G\\) ,the graph encodes part of the conditional independence structures. This means we can read conditional independences in the graph structure and these conditional independences must be true for the distribution. (We will show how to do that soon.) Unfortunately graph has limited expressiveness so it is not true that all conditional independences in a distribution \\(P\\) can be represented in a graph, since some might depend on the special structure of \\(f(v_i|p_i)\\). Technically speaking, this makes the graph \\(G\\) an I-Map of \\(P\\) (D. Barber 2012), meaning that the conditional independences induced by the graph is a subset of the set of all conditional independences for \\(P\\). We will shortly see that by adding more edges into a graph we will only reduce the set of conditional independences. So for any distribution \\(P\\) we would like to find the minimal graph that is an I-Map of \\(P\\). Such graph is called a minimal I-Map — a graph that contains the most complete conditional independence relationships of \\(P\\). The type of conditional independences that can be expressed by the graph is the most important because it only requires the distribution to be factored as in Definition 5.1 without extra assumptions on the specific factors \\(f(v_i|\\pi_i)\\) look like. In this sense the independence relationships represented by the graph is more general and robust. The complete study of DAG or even the loopy graph is the subject of graphical models. Interested readers with a taste for machine learning can refer to D. Barber (2012) or Murphy (2012) for a more detailed treatment of the subject. Wasserman (2003), chap. 17 also provides a concise introduction. Lauritzen (1996) is more mathematics-heavy. The factorization property (5.1) is equivalent to the following Directed Local Markov Property (Lauritzen 1996). Theorem 5.1 (Directed Local Markov Property) A distribution \\(P\\) is Markov to a DAG \\(G\\) if and only if for any variable \\(V\\) \\[\\begin{equation} V \\perp \\! \\! \\! \\! \\perp nd(V) | \\pi_V \\tag{5.2} \\end{equation}\\] where \\(\\pi_V\\) is parents of \\(V\\) and \\(nd(V)\\) is all other variables that are not \\(V\\)’s descendants. A full proof can be found in (Lauritzen 1996). But (5.2) is intuitively easy to understand. It confirms \\(V\\) can only influence its descendants and also its dependency to its ancestors are only through its direct parents \\(\\pi_V\\). So given \\(\\pi_V\\), \\(V\\) is independent of its non-descendants. Note that it is trivial that (5.2) is equivalent to \\[ V \\perp \\! \\! \\! \\! \\perp nd(V)\\backslash \\pi_V | \\pi_V \\] since \\(\\pi_V\\) is already given. We will give a proof of the only if part using d-separation in the end of this section. Next we will see how we can read conditional independences from a graph. No mater how complicated the DAG is, there are only 3 core patterns we need to know: The chain, the fork and the collider. The simplest form is the chain (Figure 5.1), also known as “Mediation.” Figure 5.1: The chain pattern. In a chain as in Figure 5.1, all information in A for predicting C has been included in B. We have the following conditional independence: \\[ A \\perp C | B. \\] The second form is the fork (Figure 5.2). also known as “Mutual Dependence.” Figure 5.2: The fork pattern. We have already seen Figure 5.2 earlier in an explanation of Simpson’s paradox where \\(A\\) is a confounder. In a fork, conditioning on the common dependence/confounder \\(A\\), \\(B\\) and \\(C\\) are conditionally independent. But they are not marginally independent. In other words, \\(B \\perp C | A\\) but \\(B \\perp C\\) is generally not true. This is an example of Conditional Independence \\(\\nRightarrow\\) Marginal Independence! The last pattern is collider (Figure 5.3), also known as “Mutual Causation.” In Figure 5.3, \\(A\\) and \\(B\\) are marginally independent and \\(C\\)’s distribution is affected by values of \\(A\\) and \\(B\\) together. Figure 5.3: The collider pattern. This pattern is the most interesting as it provides a result that is a bit counter intuitive: \\(A\\) and \\(B\\) are not independent conditioned on \\(C\\) although without observing \\(C\\) they are independent. This is an example of Marginal Independence \\(\\nRightarrow\\) Conditional Independence! The interpretation of why \\(A\\) and \\(B\\) are conditionally dependent is often referred to as he “explain away” effect. We use an interesting example from Jordan (2004) to make the explanation more palatable. Your friend appears to be late for a meeting with you. There are two explanations: she was abducted by aliens or you forgot to set your watch ahead one hour for daylight savings time. Figure 5.4 shows this in a collider pattern. Figure 5.4: The alien abduction example (Jordan 2004). In a world where Alien abduction is possible although unlikely. Let’s say that the probability of your friend showing up late due to alien abduction is extremely small. However, if you also realized that you have forgotten to set your watch to adjust for daylight saving time, you will be almost certain that this is the reason why your friend showing up “late” and the probability of your friend being abducted by alien is further reduced. The fact that your forgot to set your watch explained away the other potential cause of an alien abduction. On the contrary, if you check your watch and realize that you have already set your watch, and for illustration purpose let’s assume there is no other possible cause of your friend being late except alien abduction, then you are certain that your friend has been abducted by alien, no matter how small the chance it originally was. : Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth. — Sherlock Holmes In this example, conditioned on the fact that your friend showed up late (\\(C\\)), information of whether you have set your watch (\\(A\\)) changes your belief of whether your friend has been taken by aliens (\\(B\\)). This means the conditional distribution of \\(B\\) given \\(A\\) and \\(C\\) is different from the distribution of \\(B\\) given \\(C\\) alone — \\(A\\) and \\(B\\) is not independent given \\(C\\)! These three patterns are all we need to identify conditional and unconditional dependence in a Causal Diagram, as shown in the next Theorem. Theorem 5.2 (d-separation) A set \\(S\\) of vertices is said to block a path \\(p\\) if either of the following two condition is satisfied: \\(p\\) contains at least one arrow-emitting node (middle node in a chain or a fork pattern) that is in \\(S\\). \\(p\\) contains at least one collision node (middle node in a collider pattern) that is outside \\(S\\) and has no descendant in \\(S\\). If \\(S\\) blocks all paths from \\(X\\) to \\(Y\\), it is said to “d-separate \\(X\\) and \\(Y\\).” Two set \\(A\\) and \\(B\\) are said to be d-separated by \\(S\\) if any pair \\(X\\) and \\(Y\\) from the two sets are d-separated by \\(S\\). If so, then we have the following directed global Markov property: \\[\\begin{equation} A \\perp \\! \\! \\! \\! \\perp B | S \\tag{5.3} \\end{equation}\\] \\(S\\) can be an empty set and the independence becomes unconditional. The first condition of d-separation basically says information can flow through a chain or a fork and the only way to block it is to condition on the arrow-emitting node. The second condition is somewhat the opposite, it says for a collider pattern information is naturally blocked unless the collision node or any of its descendant is observed and creates the “explained away” effect previously introduced. For make two sets of variable \\(X\\) and \\(Y\\) independent we need to condition on a blocking set \\(S\\) that blocks all paths from \\(X\\) to \\(Y\\). d-separation is hence very natural and intuitive once we master the three basic pattern and understand the special case of the collider pattern. It can be shown (Lauritzen 1996) that the global Markov property (5.3) and the local Markov property (5.2) are all equivalent to the factorization property (5.1). Here we show Theorem 5.2 implies (5.2) to illustrate the usage of d-separation. Let \\(\\widetilde{V}\\) be \\(nd(V) \\backslash \\pi_V\\). Any path connecting \\(\\widetilde{V}\\) and \\(V\\) must be either through a back-door path (starting with an arrow pointing to \\(V\\)), or a front-door path (starting with an arrow emitting from \\(V\\)). All back-door paths are blocked by \\(\\pi_V\\) already. For front-door paths, since \\(\\widetilde{V}\\) has no descendants of \\(V\\), there is no directed path from \\(V\\) to \\(\\widetilde{V}\\). Because front-door paths start with arrow emitting from \\(X\\), any path that is not directed connecting \\(V\\) to \\(\\widetilde{V}\\) must have a collider vertex which is a descendant of \\(V\\). This vertex and all its descendants are also descendants of \\(V\\) hence cannot be part of \\(\\widetilde{V}\\). So all front-door paths are also d-separated. We have shown all paths connecting \\(V\\) and \\(\\widetilde{V}\\) are d-separated by \\(\\pi_V\\). By Theorem 5.2, this proves \\(V \\perp \\! \\! \\! \\! \\perp nd(V)\\backslash \\pi_V | \\pi_V\\) which is equivalent to (5.2). 5.2 the do operator The central difference of the causal graphical model (CGM) and the potential framework is how the concept of change with intervention is modeled. In potential framework, we augment the probability distribution by hypothesizing a counterfactual pair that represents all potential outcomes when different interventions were to be applied. This way we conceptually circumvent the issue of understanding how forcing a change through intervention will affect the joint distribution \\(P\\) itself. In CGM, because the factorization of a distribution \\(P\\) encodes how random variables locally affects each other, we can directly analyze the probability distribution generated by the intervention. We call this generated distribution the post-intervention distribution. The intervention operation is called the do operator. If we know how to handle the do operator and can calculate \\(P(Y|do(X=x))\\) for all \\(x\\), then we can answer any causal question about the change of \\(X\\) to \\(Y\\). For example, ATE of changing \\(X\\) from \\(X_0\\) to \\(X_1\\) is just \\[ E(Y|do(X=x_1)) - E(Y|do(X=x_0)). \\] The calculations involving the do operator is called do calculus. The goal of do calculus is to transform an expression with do operator into a do-free expression which only involves observable variables. This is analogous to potential outcomes framework where we need to transform expressions involving counterfactuals into a counterfactual-free expression. To handle the do operator, we need to understand the post-intervention distribution. The following theorem characterizes the post-intervention distribution \\(P(V|do(X=x))\\), i.e. the new probability distribution governs \\(V\\) after we manually fix \\(X\\) to be \\(x\\). Theorem 5.3 (Truncated factorization) For any distribution \\(P\\) (with probability function \\(f\\)) Markov to a DAG \\(G\\), the post-intervention distribution \\(P^*\\) generated by an intervention \\(do(X=x)\\) has its probability function \\(f^*\\) given by the truncated factorization \\[\\begin{equation} f^*(v_1,\\dots,v_k) =\\prod_{i|V_i\\notin X} f(v_i|\\pi_i)|_{X=x}, \\tag{5.4} \\end{equation}\\] where \\(f(v_i|\\pi_i)|_{X=x}\\) is the original conditional probability of \\(v_i\\) given its parents \\(\\pi\\) with any parents in \\(X\\) is fixed according to the intervention \\(X=x\\). Graphically, the effect of \\(do(X=x)\\) is to remove all arrow from \\(X\\)’s parents to \\(X\\) in \\(G\\) and then condition on \\(X=x\\). (5.4) is equivalent to the following \\[\\begin{equation} P(v_1,\\dots,v_k|do(X=x)) = \\frac{P(v_1,\\dots,v_k)}{P(X=x|\\pi_X)}, \\tag{5.5} \\end{equation}\\] where the denominator \\(P(X=x|\\pi_X)\\) is the removed arrow. With Theorem 5.3, it seems the problem of causal inference in a DAG has been solved because we have derived the post-intervention distribution only using the factors \\(f(v_i|\\pi_i)\\) in the original distribution. If we know these factors, we can perform any do calculus with ease. Conceptually the answer is yes. But practically Theorem 5.3 alone can rarely be useful as a formula for \\(P(Y|do(X=x))\\). To use the truncated factorization, we need to observe all random variables in the DAG or at least the sub graph that contains all ancestors and descendants of \\(X\\) and \\(Y\\)13. But in reality we are always limited by the amount of measurements we can gather and there are almost always nodes in the DAG that are not observed. Many vertices and edges are added to the graph to account for unknown dependency links and to register the existence of certain unknown confounder. It is often true that for the purpose of studying causal relationship for two nodes \\(X\\) and \\(Y\\), many details in the graph are irrelevant and can be simplified or ignored. For this reason, a simple identification strategy requiring only a small set of observations is desired. In the potential outcome framework, we have seen that the key is to condition on a set of covariates \\(X\\) that meets the conditional unconfoundedness condition (Definition 4.3). In CGM, with a DAG and the understanding of the post-intervention distribution, a similar idea is to find a set \\(S\\) such that \\(P(Y|do(X=x),S=s) = P(Y|X=x, S=s)\\). This is called the back-door criterion. A different strategy called the front-door criterion identifies a set \\(S\\) such that \\(P(Y|do(X=x),S=s) = P(Y|do(S=s))\\) where \\(P(Y|do(S=s))\\) can be easily identified. The back-door and front-door criterion are special cases or more general methods based on rules of do operations. We introduce back-door and front-door criterion in the next two sections. These two criterion have conceptual importance and can be directly applied for causal effect identification in many simple graphs. They also help building up intuitions to facilitate our discussion of more general strategies. 5.3 The Back-door Criterion We first introduce the Back-door criterion which is closely related to conditional unconfoundedness in the potential outcome framework. Equation (5.5) shows the only changes a do operation make on \\(P\\) beyond conditioning \\(X\\) on a fixed value is to remove all arrows from \\(X\\)’s parents \\(\\pi_X\\) to \\(X\\). Denote the post-intervention distribution by \\(P(\\cdot|do(X=x))\\). There are two simple facts we can directly derive from (5.5): \\(P(\\pi_X=t|do(X=x)) = P(\\pi_X=t)\\); \\(P(\\cdot|do(X=x),\\pi_X = t) = P(\\cdot|X=x, \\pi_X=t)\\), where \\(P(\\cdot|do(X=x),\\pi_X = t)\\) is the conditional distribution of the post-intervention distribution given \\(\\pi_X=t\\): \\[P(\\cdot|do(X=x),\\pi_X = t) = \\frac{P(\\cdot,\\pi_X=t|do(X=x))}{P(\\pi_X=t|do(X=x))}.\\] The first result trivially implies the do operation has no effect on its parents. The second says if we further condition on \\(\\pi_X\\), then the conditional distribution \\(P(\\cdot|do(X=x), \\pi_X=t)\\) is the same as \\(P(\\cdot|X=x, \\pi_X=t)\\) because the existence of arrows between \\(\\pi_X\\) and \\(X\\) has no effect when we are already given the values of \\(\\pi_X\\) and \\(X\\). If we are interested in \\(P(Y|do(X=x)\\), then \\[\\begin{align*} P(Y|do(X=x)) &amp; = \\sum_{t} P(Y|do(X=x),\\pi_X = t)\\times P(\\pi_X=t|do(X=x)). \\end{align*}\\] Together with \\(P(Y|do(X=x),\\pi_X=t) = P(Y|X=x,\\pi_X=t)\\) and \\(P(\\pi_X=t|do(X=x)) = P(\\pi_X=t)\\), we have proven the following: Theorem 5.4 (Adjustment by Parents) \\[\\begin{equation} P(Y|do(X=x)) = \\sum_t P(Y|X=x,\\pi_X = t) \\times P(\\pi_X=t) \\end{equation}\\] Theorem 5.4 successfully turns a do expression into a do-free expression by conditioning/adjustment on the set \\(\\pi_X\\). Paths connecting \\(X\\) to \\(Y\\) starting with an arrow pointing to \\(X\\) from one of its parents are called back-door paths. Back-door paths are all blocked by \\(\\pi_X\\) because the path through \\(\\pi_X\\) can be either a chain or a fork and not a collider. Turns out, together with d-separation, this result can be easily generalized from using all parents of \\(X\\) as the adjustment set into using any sets that blocks all back-door paths without involving any descendants of \\(X\\). This is the Back-door Criterion. Theorem 5.5 (Back-door Criterion) A set \\(S\\) in a DAG \\(G\\) is admissible or sufficient for adjustment if two conditions hold: No vertex in \\(S\\) is a descendant of \\(X\\) \\(S\\) blocks all back-door paths (all paths end with an arrow pointing to \\(X\\)) from \\(X\\) to \\(Y\\). For any admissible set \\(S\\), \\[\\begin{equation} P(Y|do(X=x)) = \\sum_s P(Y|X=x, S=s)P(S=s) \\tag{5.6} \\end{equation}\\] provides an identification strategy for \\(P(Y|do(X=x))\\). In particular, when \\(S\\) is the empty set, \\(P(Y|do(X=x)) = P(Y|X=x)\\). Proof. By Theorem 5.4 we can always adjust by parents of \\(X\\). We can always further conditioning on \\(S\\). This leads to \\[\\begin{align} &amp; P(Y|do(X=x)) \\notag \\\\ = &amp;\\sum_t P(Y|X=x, \\pi_X=t)P(\\pi_X=t) \\notag \\\\ = &amp;\\sum_t\\left[\\left(\\sum_s P(Y|X=x, \\pi_X=t, S=s) P(S=s|X=x, \\pi_X=t)\\right) P(\\pi_X=t) \\right]. \\tag{5.7} \\end{align}\\] We make the following two claims (proofs will follow): \\(X\\) and \\(S\\) are d-separated by \\(\\pi_X\\), \\(\\pi_X\\) and \\(Y\\) are d-separated by \\(S\\) and \\(X\\). So by Theorem 5.2, \\[ P(S=s|X=x,\\pi_X=t) = P(S=s|\\pi_X=t), \\] \\[ P(Y|X=x,\\pi_X=t, S=s) = P(Y|X=x, S=s) \\] Plugging the two above into (5.7) and switching the order of two summations leads to \\[\\begin{align*} &amp; P(Y|do(X=x)) = \\sum_t\\left[(\\sum_s P(Y|X=x, S=s) P(S=s|\\pi_X=t)) P(\\pi_X=t)\\right] \\\\ = &amp; \\sum_s P(Y|X=x, S=s) \\sum_t \\left[P(S=s|\\pi_X=t)P(\\pi_X=t)\\right] \\\\ = &amp; \\sum_s P(Y|X=x, S=s) P(S=s). \\end{align*}\\] The proof is complete if we can prove the two claims above. The first is a direct result of the first Back-door condition that \\(S\\) does not contain any descendants of \\(X\\) and the Markov Property (5.2) saying \\(X\\) is conditionally independent of all its non-descendants given its parents. For the second claim, if \\(Y\\) and a vertex \\(R\\) in \\(\\pi_X\\) are d-connected by a path \\(p\\). Then the path \\[ X \\longleftarrow R \\leftarrow \\overset{p}{---} \\rightarrow Y \\] must also d-connect \\(X\\) with \\(Y\\) and it is a back-door path. This contradicts the second condition of \\(S\\) blocking all back-door paths and complete the proof. We use Graph 5.5 to illustrate how to use the back-door criterion. The double arrow dashed line means we don’t assume any details of the path between two vertices other than they are connected. There are two admissible sets \\(S\\) possible in this case. First choice is to block on \\(C\\) by noticing all back-door paths must go through \\(C\\) to reach \\(Y\\) and the fact that \\(C\\) has a emitting arrow to \\(Y\\) so the path through \\(C\\) to \\(Y\\) can only be a chain or a fork. Another choice is to block on both \\(A\\) and \\(B\\). \\(B\\) blocks all back-door paths through \\(B\\) because it is not a collider. \\(A\\) blocks all back-door paths through \\(A\\) because \\(A\\) cannot be a collider for paths like \\(X \\longleftarrow A \\leftarrow --- \\rightarrow C \\longrightarrow Y\\). There are no other possible back-door paths so they are all blocked. Figure 5.5: A causal diagram illustrating the back-door criterion. The two double arrow dashed line means we don’t assume any details of the path between two vertices other than they are connected. Here all back-door paths from \\(X\\) to \\(Y\\) can be blocked by either conditioning on \\(C\\) or both \\(A\\) and \\(B\\). When using the back-door criterion, beyond the obvious need for observing \\(X\\) and \\(Y\\), we only need to further observe \\(C\\) or \\(A\\) and \\(B\\) to identify the causal effect of \\(X\\) on \\(Y\\). Next we prove a simple result which says do operation on a vertex only have effects on its descendants. In other words, the arrows in the DAG dictates the direction of possible causal effect. Theorem 5.6 (The Arrow of Causal Effect) If there is no directed path from \\(X\\) to \\(Y\\). Then \\[ P(Y|do(X=x)) = P(Y). \\] That is, \\(X\\) has no causal effect on \\(Y\\). So for \\(X\\) to have causal effect on \\(Y\\), \\(X\\) must be \\(Y\\)’s ancestor. Proof. By the directed local Markov Property (5.2) and the fact that \\(Y\\in nd(X)\\), \\[ X \\perp \\!\\!\\!\\! \\perp Y | \\pi_X. \\] So \\(P(Y|X=x, \\pi_X=t) = P(Y|\\pi_X=t)\\) and \\[\\begin{align*} &amp; P(Y|do(X=x)) = \\sum_t P(Y|X=x, \\pi_X=t)P(\\pi_X=t) \\\\ = &amp; \\sum_t P(Y|\\pi_X=t)P(\\pi_X=t) = P(Y). \\end{align*}\\] It should be clear that the back-door adjustment by conditioning on a set of variables \\(S\\) such that \\[ Y \\perp \\!\\!\\!\\! \\perp X | S \\] is very similar to the conditional unconfoundedness assumption in the potential outcome framework which posits \\[ (Y(1),Y(0)) \\perp \\!\\!\\!\\! \\perp X | S. \\] The requirement of \\(S\\) not containing any descendant of \\(X\\) together with Theorem 5.6 guarantees \\(X\\) cannot have causal effect on any variable in \\(S\\) so \\(S\\) are indeed covariates. 5.4 Causal Mechanism and the Front-door Criterion Back-door criterion does not directly work for the following basic confounder pattern when confounder \\(U\\) is not observed. Figure 5.6: Back-door path not blocked when \\(U\\) is not observed. Dashed vertex and edges indicates unobservable confounding effect. In fact, if the diagram 5.6 is all we know about the relationship of \\(X\\) and \\(Y\\) and we know there exists some confounder \\(U\\) that is not observable, there is fundamentally no way for us to be able to differentiate the true causal effect from the confounding effect. This pattern, also called the bow pattern (Pearl 1995), is the basis of Fisher’s defense of smoking not necessarily causing lung cancer even though a 1950 study showed a positive association between smoking tobacco and lung cancer. He hypothesized a confounder (of a genetic trait) that is linked to lung cancer but also make one more likely to enjoy smoking. The causal link between tobacco usage and lung cancer was finally recognized by the medical community only after people discovered the tobacco residue, also known as tar, when in lung, coats the cilia causing them to stop working and eventually die and allows toxic particles in tobacco smoke to enter the alveoli directly. These toxic particles including carcinogens such as benzene, acrylamide and acrylonitrile that can be commonly found in tar. In other words, people have discovered the mechanism of how tobacco smoking can cause lung cancer, which is through the forming of toxic tar in the lung. It is even believed that 90 percent of lung cases could be attributed to smoking.14 The above deduction using mechanism can be abstracted into the following diagram. Here \\(X\\) take an effect on \\(Y\\) exclusively through \\(M\\). Figure 5.7: \\(X\\) take an effect on \\(Y\\) exclusively through \\(M\\) and all back-door paths from \\(M\\) to \\(Y\\) are blocked by \\(X\\). The identification strategy can be described in two steps. Recall \\(U\\) is not observed in Figure 5.7. But there is no back-door path from \\(X\\) to \\(M\\) and the only back-door path from \\(M\\) to \\(Y\\) can be blocked by \\(X\\). In other words, both \\(P(M|do(X=x))\\) and \\(P(Y|do(M=m))\\) can be identified using back-door criterion. The second step relies on the argument that \\(X\\) can impact \\(Y\\) only through \\(M\\), therefore intuitively \\[\\begin{equation} P(Y|do(X=x)) = \\sum_m P(M=m|do(X=x))P(Y|do(M=m)). \\tag{5.8} \\end{equation}\\] With (5.8), an do-free expression of \\(P(Y|do(X=x))\\) can be obtained by replacing \\(P(M=m|do(X=x))\\) and \\(P(Y|do(M=m))\\) with their corresponding do-free expressions. We call (5.8) do propagation. Theorem 5.7 (Front-door Criterion) A set of nodes \\(M\\) is said to satisfy the front-door criterion for an ordered pair \\((X,Y)\\) if: \\(M\\) intercepts all directed paths from \\(X\\) to \\(Y\\); all back-door paths from \\(M\\) to \\(Y\\) are blocked by \\(X\\). there is no back-door paths from \\(X\\) to \\(M\\); When such a set \\(M\\) exists, \\[ P(Y|do(X=x)) =\\sum_m P(M=m|X=x)\\left(\\sum_x&#39; P(Y|X=x&#39;,M=m) P(X=x&#39;)\\right) \\] is a valid identification strategy. Condition 2 and 3 ensures the two sub-mechanisms \\(M \\rightarrow Y\\) and \\(X\\rightarrow M\\) are identifiable by back-door criterion. It remains to prove (5.8). The following Lemma shows condition 1 and 2 together justify do propagation (5.8). Lemma 5.1 (do propagation) For a DAG \\(G\\) there is a directed path from \\(X\\) to \\(M\\) and then \\(Y\\). A sufficient condition for \\[ P(Y|do(X=x)) = \\sum_m P(M=m|do(X=x))P(Y|do(M=m)) \\] is that \\(M\\) intercepts all directed paths from \\(X\\) to \\(Y\\); all back-door paths from \\(M\\) to \\(Y\\) are blocked by \\(X\\) Proof. Because post-intervention distribution is still a probability distribution which satisfies the law of total probability: \\[ P(Y|do(X=x)) = \\sum_m P(M=m|do(X=x))P(Y|do(X=x),M=m). \\] Lemma 5.1 is proven once we show the following: \\[ P(Y|do(X=x),M=m) = P(Y|do(X=x),do(M=m)) = P(Y|do(M=m)). \\] The first equality follows from applying the back-door criterion on an empty set to identify the causal effect of \\(M\\) on \\(Y\\) under the post-intervention distribution \\(P(\\cdot|do(X=x)\\). In the \\(do(X)\\) distribution, all back-door arrows of \\(X\\) is already removed, and because of condition 2, there is no back-door path remains from \\(M\\) to \\(Y\\). So without any adjustment, we can replace do operator \\(do(M=m)\\) by simple observation conditioning. The second equality is a result of Theorem 5.6 and condition 1. In the post \\(do(M=m)\\) distribution, \\(Y\\) is no longer a descendant of \\(X\\) because \\(M\\) intercepts all directed path from \\(X\\) to \\(Y\\) and these paths are broken with all back-door edges of \\(M\\) removed. In the do-calculus above we use the back-door criterion Theorem 5.5 to show the equivalence of the operation \\(do(M=m)\\) and conditioning on \\(M=m\\). We further use Theorem 5.6 to remove the do operation \\(do(X=x)\\). These are two of Pearl’s three rules of do calculus(Pearl 1995) and the other one is related to d-separation. See Section 5.5. Front-door criterion, and more generally, the idea of identifying causal effect using combinations of do propagation together with a chain of identifiable do expressions, are very inspiring. It allows us to search for identification strategy in very complicated graphs using divide and conquer. If we can find a chain \\(M_0=X,M_1,\\dots,M_K=Y\\) with every \\(P(M_k|do(M_{k-1}=m))\\) identifiable and do propagation can be recursively applied to transform \\(P(Y|do(X=x))\\) into only using sub-mechanisms \\(P(M_k|do(M_{k-1}=m))\\), then \\(P(Y|do(X=x))\\) can be identified. Some authors call Condition 1 in Theorem 5.7 the exhaustion condition and the other two conditions isolation condition (Morgan and Winship 2014). Although the two isolation conditions imply identification of the two sub-mechanisms \\(P(Y|do(M=m))\\) and \\(P(M|do(X=x))\\), it is not generally true that the the exhaustion condition alone guarantees the do propogation equation (5.8). This is why we need one additional condition beyond the exhaustion condition in Lemma 5.1. Figure 5.8 presents a example with only the exhaustion condition. But there is a back-door path from \\(M\\) to \\(Y\\) that is not blocked by \\(X\\). As a result, it is no longer true that \\(P(Y|do(X=x),do(M=m)) = P(Y|do(X=x), M=m)\\). On the other hand, \\(P(Y|do(X=x),do(M=m)) = P(Y|do(M=m))\\). Hence \\(P(Y|do(M=m))\\neq P(Y|do(X=x), M=m)\\). Figure 5.8: \\(M\\) satisfies the exhaustion condition but the do propagation (5.8) is not true. It is worthwhile to note that \\(M\\) in the front-door criterion can be more than one node. In Figure 5.9, effect of \\(X\\) on \\(M\\) and \\(N\\) both can be identified and all back-door path from \\(M\\) or \\(N\\) to \\(Y\\) are through \\(X\\). If \\(M\\) is observed but not \\(N\\), then the front-door mechanism breaks and we can only learn the partial effect of \\(X\\) on \\(Y\\) through \\(M\\). Partial effect can sometimes be interesting in its own right. Figure 5.9: A causal diagram illustrating the front-door criterion. Here the causal mechanism is \\(X\\) affecting \\(M\\) and \\(N\\) and then to \\(Y\\). All back-door path from \\(M\\) to \\(Y\\) and \\(N\\) to \\(Y\\) are blocked by \\(X\\). 5.5 General Identification Strategy A do expression has a form \\(P(\\cdot|\\text{actions and observations})\\) where actions are \\(do\\) operations and observations are just conditioning. Pearl (1995) introduced do calculus with three fundamental rules. These rules defines necessary conditions for three basic transformations on a do expression. Any identification strategy for a causal effect \\(P(Y|do(X=x)\\) must follow from a series of these three operations together with basic probability equalities that transforms \\(P(Y|do(X=x)\\) into a do free expression. The three basic operations are: Insertion/deletion of observations Action-observation exchange Insertion/deletion of actions We have already seen theorems justifying these operations in previous sections. For example, d-separation 5.2 allows us to delete observation from a conditional probability expression via conditional independence. Back-door criterion 5.5 allows us to replace a do operation with observation. The Arrow of Causal Effect Theorem 5.6 dictates action can be removed for any causal effect on a non-descendant. These three theorems are all special cases of Pearl’s three rules, and interestingly, contains the essence of the three rules. We introduce Pearl’s three rules and explain why they are simple extensions of the three theorems. Let \\(X\\), \\(Y\\), \\(Z\\) be any disjoint set of vertices in a causal diagram \\(G\\). Denote by \\(G_{\\overline{X}}\\) the graph obtained by removing all back-door arrows from parents of \\(X\\) pointing to \\(X\\), and denote by \\(G_{\\underline{X}}\\) the graph obtained by removing all front-door arrows of \\(X\\) that are emitting from \\(X\\). \\(G_{\\overline{X}\\underline{Z}}\\) is the graph obtained by removing all back-door arrows of \\(X\\) and front-door arrows of \\(Z\\). For any DAG \\(G\\), \\((X\\perp \\!\\!\\!\\!\\perp Y |Z)_G\\) is interpreted as \\(X\\) and \\(Y\\) are d-separated given \\(Z\\) in \\(G\\). Theorem 5.8 (3 Rules of do calculus) Rule 1 (Insertion/deletion of observations): \\[ P(Y|do(X=x),Z=z,W=w) = P(Y|do(X=x),W=w) \\quad \\text{if} \\ (Y\\perp \\!\\!\\!\\! \\perp Z|X,W)_{G_{\\overline{X}}} \\] Rule 2 (Action-observation exchange): \\[ P(Y|do(X=x),do(Z=z),W=w) = P(Y|do(X=x),Z=z,W=w) \\quad \\text{if} \\ (Y\\perp \\!\\!\\!\\! \\perp Z|X,W)_{G_{\\overline{X}\\underline{Z}}} \\] Rule 3 (Insertion/deletion of actions): \\[ P(Y|do(X=x),do(Z=z),W=w) = P(Y|do(X=x),W=w) \\quad \\text{if} \\ (Y\\perp \\!\\!\\!\\! \\perp Z|X,W)_{G_{\\overline{X}\\overline{Z(W)}}} \\] where \\(Z(W)\\) is the set of \\(Z\\) vertices that are not ancestors of \\(W\\) in \\(G_{\\overline{X}}\\). Rule 1 is basically d-separation in the post \\(do(X=x)\\) distribution. Rule 2 is back-door criterion applied to the post \\(do(X=x)\\) distribution. To see that, post \\(do(X=x)\\) distribution is Markov to \\(G_{\\overline{X}}\\) plus observation \\(X=x\\). Compare to \\(G_{\\overline{X}}\\), \\(G_{\\overline{X}\\underline{Z}}\\) further removes front-door edges of \\(Z\\). \\(Y\\) and \\(Z\\) d-separated in \\(G_{\\overline{X}\\underline{Z}}\\) by \\(X\\) and \\(W\\) means all back-door path from \\(Z\\) to \\(X\\) are blocked by \\(X\\) and \\(W\\). By the back-door criterion, we can exchange \\(do(Z=z)\\) with observation \\(Z=z\\) in the post \\(do(X=x)\\) distribution with observations of \\(X=x\\) and \\(W=w\\). Rule 3 is generalization of the arrow of causal effect 5.6. Again we start with the post \\(do(X=x\\) distribution represented by \\(G_{\\overline{X}}\\). First let’s assume \\(W\\) is empty set. \\(Y\\) and \\(Z\\) d-separated in \\(G_{\\overline{X}\\overline{Z}}\\) by \\(X\\) means there is no front-door path from \\(Z\\) to \\(Y\\) after conditioning by \\(X\\). This means \\(Y\\) cannot be a descendant of \\(Z\\) after conditioning by \\(X\\), otherwise there must be a directed path from \\(Z\\) to \\(Y\\) that is not blocked by \\(X\\) which has to be also a front-door path. Theorem 5.6 says there is no additional effect of \\(do(Z=z)\\) when \\(do(X=x)\\) is already there. Now if we add additional observations \\(W\\), will the previous argument still be valid? Figure 5.10 explains the danger of conditioning on any descendant of \\(Z\\). In this simple graph of \\(Y \\rightarrow Z \\rightarrow W\\), it is clear that \\(P(Y|do(Z=z),W=w) = P(Y)\\). However, \\(P(Y|W=w)\\neq P(Y)\\). The reason is that observation \\(W\\) will impact \\(Y\\) (back-propagation of information), but \\(do(Z=z)\\) will block this effect. On the other hand, for any \\(W\\) that is not descendant of \\(Z\\), then any path between \\(Z\\) to \\(W\\) must have a collider pattern in the middle, or \\(W\\) is an ancestor of \\(Z\\) and \\(Z\\) itself must be a collider. Therefore \\(Y\\) and \\(W\\) becomes d-separated and \\(P(Y|W=w)=P(Y) = P(Y|do(Z=z),W=w)\\). The discussion above explains why we need \\(Z(W)\\) in Rule 3. The arrow of causal effect holds when conditioning on \\(W\\) that are not descendants of \\(Z\\). It will break if any \\(W\\) conditioned is a descendant of \\(Z\\). \\(Z(W)\\) is a subset of \\(Z\\) so the difference of \\(G_{\\overline{X}\\overline{Z(W)}}\\) and \\(G_{\\overline{X}\\overline{Z}}\\) is the former could have certain back-door edges of \\(Z\\) from ancestors of \\(W\\) not removed. In other word, \\(W\\) might activates some back-door paths from \\(Z\\) to \\(Y\\) that should otherwise be blocked. Figure 5.10: Illustration of why in Rule 3 we need to be careful about conditioning on descendants of \\(Z\\). \\(P(Y|do(Z=z),W=w) eq P(Y|W=w)\\) in this case even though \\(Y\\) is not a descendant of \\(X\\). Now let’s talk about general identification strategy for \\(P(Y|do(X=x))\\). If there is no back-door path from \\(X\\) to \\(Y\\), we can just apply Rule 2 to change the above into a conditional probability. If not, we can pick any set \\(S\\) such that \\(P(S|do(X=x))\\) is identifiable and use law of total probability \\[ P(Y|do(X=x)) = \\sum_s P(Y|do(X=x),S=s)P(S=s|do(X=x)). \\] We have an identification strategy if we can change \\(P(Y|do(X=x),S=s)\\) into something identifiable. We then have three choices, Back-door adjustment to exchange action-observation using Rule 2: \\(P(Y|do(X=x),S=s) = P(Y|X=x,S=s)\\) Front-door mechanism with Rule 2 and Rule 3 to facilitate the following transition \\(P(Y|do(X=x),S=s) = P(Y|do(X=x),do(S=s)) = P(Y|do(S=s)\\). Expand \\(P(Y|do(X=x),S=s)\\) further using the law of total probability. In all options, we breaks \\(P(Y|do(X=x))\\) up into smaller problems of identification which should be easier. We repeat this procedure recursively. As an example, let’s look at the graph in Figure 5.11. We immediately identify \\(P(M|do(X=x))\\) identifiable using front-door criterion with \\(N\\) being the only mechanism. We can let \\(M\\) to be our \\(S\\) when using law of total probability. We then focus on \\(P(Y|do(X=x),M=m)\\). It is also obvious that \\(M\\) blocks all back-door path from \\(X\\) to \\(Y\\), which means \\(P(Y|do(X=x),M=m) = P(Y|X=x,M=m)\\) by Rule 2. Putting front-door identification of \\(P(M|do(X=x))\\) and \\(P(Y|X=x,M=m)\\) together solves the problem. Figure 5.11: A case where front-door criterion and back-door criterion are combined together. Note that Figure 5.11 is very similar to the back-door criterion, except that \\(M\\) blocks all back-door path from \\(X\\) to \\(Y\\) but \\(M\\) is also a descendant of \\(X\\) itself. Since back-door criterion is closely related to conditional unconfoundedness in potential outcomes framework where adjustment can only be taken on covariates unaffected by the change. Here \\(X\\) do have effect on \\(M\\) and yet adjustment by \\(M\\) is still useful. We see that do calculus can provide identification strategy that the potential outcome framework cannot, with extra assumption of a Markovian model. In Figure 5.12, both \\(P(M|do(X=x)\\) and \\(P(Y|do(M=m))\\) can be identified using front-door criterion. However, because of the back-door path \\(M\\leftarrow W \\rightarrow Y\\), we cannot apply do propagation (5.8) to combine the two sub-mechanisms into an identification strategy for \\(P(Y|do(X=x))\\). Also, because of the unblockable back-door path \\(X \\leftarrow U \\rightarrow Y\\), there exists no simple back-door adjustment. Nonetheless, we can first condition on \\(W\\). \\(P(W|do(X=x)) = P(W)\\) by rule 3 since \\(W\\) is ancestor of \\(X\\). For \\(P(Y|do(X=x),W=w)\\), we can identify sub-mechanisms \\(P(Y|do(M=m),W=w)\\) and \\(P(M|do(X=x),W=w)\\) using rule 2 because \\(X\\) and \\(W\\) blocks all back-door paths from \\(M\\) to \\(Y\\) and \\(W\\) blocks all from \\(X\\) to \\(M\\). We claim the do propagation is now valid since the only back-door path from \\(M\\) to \\(Y\\) not through \\(X\\) is now blocked by \\(W\\). To be specific, \\(P(Y|do(X=x),M=m,W=w) = P(Y|do(X=x),do(M=m),W=w)\\) by rule 2. And then \\(P(Y|do(X=x),do(M=m),W=w) = P(Y|do(M=m),W=w\\) by rule 3. Figure 5.12: Front-door criterion applies only with conditioning. To close our discussion on CGM and identification strategy, an interesting result shows that the key to the identification problem is to make sure there exists no bow pattern from \\(X\\) to all its local children. Theorem 5.9 (Tian and Pearl) A sufficient condition for identifying the causal effect \\(P(y|do(x))\\) is that every path between \\(X\\) and any of its children traces at least one arrow emanating from a measured variable. Test for existence of any identification strategy in a CGM is a solved problem in the literature. A necessary and sufficient condition and algorithm for identification is discovered in Shpitser and Pearl (2012) together with computer algorithm to find the strategy when exists. 5.6 RCM vs. CGM To do: Discuss pros and cons In fact only ancestors of \\(Y\\) is needed, see Theorem 5.6.↩︎ https://en.wikipedia.org/wiki/Tar_(tobacco_residue)↩︎ "],["regression-based-methods.html", "Chapter 6 Regression-based Methods", " Chapter 6 Regression-based Methods IV Regression discontinuity design Diff-in-diff and in general synthetic control methods "],["abintro.html", "Chapter 7 A/B Testing: Beyond Randomized Experiments 7.1 Special Aspects of A/B Tests 7.2 Instrumentation and Telemetry 7.3 Common Pitfalls", " Chapter 7 A/B Testing: Beyond Randomized Experiments In the last few chapters we focused on essential ideas in causal inference to identify causal effect from randomized or observational data. Traditionally, causal inference is a secret weapon wielded mainly by academic researchers in fields like social science, economics, psychology, biostatistics and pharmaceutical sciences. In these applications, a dataset with thousands of subjects and more than 10 attributes per subject is considered fairly large. From the beginning of this century, with the expanding of a connected world through world-wide web and mobile devices, cheaper data connection, storage, and computation, and the resurgence of artificial intelligence, “Big Data” has been redefined by the online industry. In 2018, a Forbes article15 estimated the size of data created each day is in the order of Exabytes (one Exabytes is 1 million Terabytes) and it is growing exponentially fast with the growth of the Internet of Things (IoT). In this transformation to the big data era, causal inference also evolved and adapted to the online industry and became an important component in distilling data into knowledge. Online randomization experiments, often called A/B tests in the online industry (sometimes under different names such as split testing and bucket testing), are the reincarnation of the classic randomized experiments in the new era. The idea of business-hypothesis-driven experimentation, iteratively and continuously deploy and evolve a product is a defining characteristic of product development for online companies. Ries (2011) included A/B tests (split testing in the book) in the definition of a lean startup — a term the author coined as a new methodology for developing businesses and products. Listen to the customers, not the HiPPO (Highest Paid Person’s Opinion) (Kohavi et al. 2009)! The central idea of Ceteris paribus, “all other things being equal,” remains the key intuition that is obvious to anyone. One of the main reasons of the rapid adaptation of A/B testing is its simplicity, as it does not require people to understand deep statistics and probability theory to grasp the idea and be convinced that the change observed must be a causal effect. However, A/B testing is not just a direct adaptation of classic randomized experiments to a new type of business and data. It has its own special aspects, unique characteristics and challenges. 7.1 Special Aspects of A/B Tests Scale. There are a few aspects of scale that make A/B tests different from traditional applications of randomized experiments. The sample size — the number of units that are randomized — is usually at least in the order of thousands. A typical test conducted on a large scale product such as search engine (Tang et al. 2010; Kohavi et al. 2013), social network (Bakshy, Eckles, and Bernstein 2014; Xu et al. 2015), e-commerce or web video streaming (Gomez-Uribe and Hunt 2016) can easily “recruit” millions of individual users. Large scale does not only affect the number of samples, but also the reach and coverage of experiments. For online businesses it is not difficult to conduct experiments across different geo-markets, leading to a better representation of the user base. Experimenters can still choose to analyze each user segments individually to understand potential effect variations. Large scale and sample size also lead to increased sensitivity. In A/B tests, experimenters are often able to detect a small change of 1%.16 In some large scale tests, a change of 0.1% can also be detected. Large sample brings many goodness (and challenges) to the analysis of A/B tests, more to come in Chapter 8. Despite the bless of large sample, A/B tests community are never satisfied with enough sample size and statistical power. Large scale means even a small change can have a big aggregated impact. A 0.1% lift in revenue for a product with a billion revenue annually is a million dollars every year which is enough to fund a small team. Kohavi et al. (2014) reported “an engineer that improves sever performance by 10msec (that’s 1/30 of the speed that our eyes blink) more than pays for their fully-loaded annual costs.” Agility. A mature A/B test platform allows experimenters to run an experiment from days to weeks. Early stage experiments focusing on getting the general direction of a change, whether the effect is likely positive, negative or negligibly small, can often get results back in only a few days. A latter stage ship-decision experiment often takes about a week to a few weeks. An experiment running more than a month is often considered a long-running experiment and needed when novelty or learning effect is expected or the experiment has very low exposure (e.g. a feature with very low triggering rate). The agility also means the speed of start and stop of an experiment need to be fast. Many online products, especially those deployed to a modern cloud, can safely deploy a standard code change within a hour, and deploy an urgent fix or emergency shutdown of an experiment in minutes. Another aspects of agility is continuous data ingestion and short time to analysis results. In A/B tests, data flows in continuously as users use products in real time and the observations for each subject are cumulated over time. This has several implications that could be surprising to a novice. First, sample size increases over time. But it likely will increase only sub-linearly. In fact, the most common unit of interest is user and for most online products there are always one type of recurring heavy users and another type of less engaged and somewhat casual users. All the users at the first day are “new” for the experiment, and at the second day, only users who didn’t appear at the first day are incremental. Similarly, at the k-th day, incremental users are those didn’t appear for the past k-1 days. The number of incremental “new” users will generally decrease as k increases. Second and maybe more important, the notion of the population also changes over time. When doing any statistical analysis, we model the observed data as a sample from a population and the observations follow some distribution. When data is continuously ingested, this hypothesized population also changes continuously. For instance, if we are interested in users’ visits frequency, we look at visits per user. The day one population will contains more heavy users who use the product everyday. As we run the experiment, more and more casual users will be included. So the “mixture” of users keep changing. Also, it is obvious that the distribution of users’ total number of visits is changing. For a product with many recurring users, visits per user — the average of visits — will only increase. Figure 7.1 shows the changes of sample sizes (square root), mean and standard deviation of visits per user (a visit is called a session for search engine) for an experiment on Microsoft Bing (Kohavi et al. 2012). If the sample sizes were to increase linearly, after 9 days, the square root of sample size should be tripled. The reality is it grew by about 60% so the sample size increases su-linearly. The mean and standard deviation of visits per user also increases over time. Figure 7.1: (Appeared in Kohavi et al. (2012)) Changes of square root of sample size (square root), mean and standard deviation over time for Sessions/User. Exception of the above phenomenon exists when the unit of randomization is each visit or page-view — in this case sample size is the number of visits or page-views and by definition there is no recurring units so the sample size will be the sum of sample sizes everyday. Rich Data. Because collecting data is cheap, and storing data is becoming cheaper everyday, online businesses can possess a very comprehensive dataset that can be used to deeply understand user behavior. On one hand this is a huge advantage. For A/B testing, it is very common to define hundreds to even thousands of different metrics each telling a different story about the subjects. Besides large number of metrics, data can be further segmented by many attributes such as device type (PC, mobile phone, etc.), Operation System, Browser, geo locations/markets, and time (day in the week, date, week day or weekend). Some products can even classify their users based on their historical behavior into different categories in dimensions like interest, usage patterns, and engagement levels. The richness of data leads to opportunities to better understand nuances of the product, and the heterogeneity of the causal effect at individual level. On the other hand, it creates new issues such as privacy and ethics issues in data handling. In 2016 EU passed the landmark General Data Protection Regulation (Regulation 2016) (GDPR), effective from May 25th, 2018. Based on the legal premise that a natural person should have control over the data he or she generated, GDPR puts various requirements for a online business to inform consent of data collection, provide safeguards to protect personal data and to give people the right to access and erase their data. Online businesses have to consider and comply with these regulations when deciding how much data they need to collect. Fluid Analysis. Besides a few prescribed analysis targets for OEC (overall evaluation criteria (Kohavi and Longbotham 2015)) or success metrics and key feature metrics, an experimenter will probably observe unexpected results in other metrics. Before seeing the results, it is unlikely that one knows all the questions he or she will be asking after looking at the results of an experiment. The targets of the analysis are almost always adaptive, with one discovery leads to another question. Fluid analysis often means breaking a taboo called “Data Snooping.” We only ask a question because the data suggested us so. One related statistical challenge is multiple testing. When we performing A/B tests on hundreds or thousands of metrics, the chance that we observe some “extreme” metric movements increases. This problem is even a bigger issue considering a typical A/B test provides multiple ways to further segment the data, and the fact that we might be testing many variants at the same time (e.g. A/B/n tests). Another type of fluid analyses is continuous monitoring and early stopping when an experimenter will decide when to stop an experiment based on the data collected up to a time point. Another common practice is to only focus on metrics that moves to the intended direction with enough “statistical significance.” This again is a type of fluid analysis with a strong self-selection. Data snooping exists in many other scientific areas, but in A/B test it is nearly impossible to avoid. In the following chapters where we dive deeper into the “science” part of A/B test, tackling issues arise in fluid analysis will be a recurring theme. Instrumentation and Telemetry. Another aspect of A/B test that differentiates it with traditional application of randomized experiments is the complexity in Instrumentation and Telemetry. In computer science, instrumentation means having a piece of software to be able to monitor or measure the level of a product’s performance, to diagnose errors, and to write trace information. The whole process of logging and collecting these data is also called telemetry. This is a big topic by itself and deserves its own section. 7.2 Instrumentation and Telemetry User Identification and Tracking. The first challenge is how to identify and track users. The most common approach is to use browser cookie. A browser cookie is a small piece of data stored by the user’s browser associated with a particular web domain. When a user send any request to a web domain, cookies associated will be sent to the server in HTTP header (request cookies). The receiving server can look at the cookies and use the information. For user tracking, the cookie needs to contain a globally unique identifier (GUID). When the request cookies does not contain a proper GUID, the server can generate a new one and send it back to the user’s browser (response cookie). Cookies are always associated with a domain. (First party) Cookies are intended to store information a user willing to share with the site or web product one is interacting with. Each web site/product can only read from and write cookies to its own domain. This is why it is hard to do cross-site or cross-product user identification using cookie. This restriction is important for security and privacy! Users using one site or product only knows what they are interacting with and do not expect the information they provide to be accessible by others. One approach for cross-site user tracking is to use a 3rd party domain cookie. To do that, A.com uses a resource (such as a banner ads, or a transparent 1 by 1 pixel) from Z.com. When a user visit A.com, the browser will send a separate request to Z.com and Z.com can manage cookie under its domain. If another site B.com is owned by the same company, and B.com also does the same thing and allow Z.com to track its users, then the company is able to use Z.com as the central user tracking domain and track users across A.com and B.com. However, 3rd party cookie presents a security and privacy concern and most modern browsers contains privacy settings that can block 3rd party cookie. There are several issues when using cookie to identify users. First, cookies are tied to a browser which are usually tied to a device. But we know modern users of web products typically have multiple devices. Worse, people use different browsers even in one devices. When using cookies for user identification, we have to keep in mind that the real number of users are smaller and could be smaller by a significant factor. Second, cookie can get lost or cleaned. Cookie storage is not in any standard so different browsers implement their own way. Sometimes a browser crash or update can cause cookies to be deleted. More often, users out of privacy concerns can choose to delete their cookies periodically, or in some cases use privacy mode or incognito mode in which cookies are automatically deleted when the page is closed. The phenomenon of cookie lost is known as cookie churn in web industry. Using data from Atlas (Facebook’s online advertisement solution), Coey and Bailey (2016) reported in a period of 1 month (July 2015), it is observed that over half of the users have more than 1 cookies and over 10% of them are associated with more than 10. Their paper also shows the distribution of cookies per user in the one month period displays high skewness and long tail (probably due to privacy mode). Cookie churn has several direct impact on A/B tests. First, cookie churn causes experience fragmentation. If an A/B test choose to randomize by user and user is tracked using cookie, for users with multiple cookies they will appear as separate users in the test. Each cookie of the same real user are independently randomized into different experiences, causing user’s experience to switch during the experiment. Experience fragmentation introduces bias in treatment effect estimation. A treatment experience is only exposed to a fraction of a user’s whole experience and potentially reduce or dilute the real effect the treatment could have been should it be exposed to a user for the whole time. This dilution effect means the treatment effect estimated using unadjusted difference of metric values from the treatment and control has a negative bias. Another source of negative bias is through leakage. Some treatment effect has a carry over effect — exposing a user to a treatment can affect the user’s behavior at a later time, even when the user receives the control experience. Because of the leakage effect, a treatment with positive effect can have a carry overed positive effect for the same user when they are under control (with a different cookie) at a later time, making control looks better too. Coey and Bailey (2016) studied the bias induced by leakage and showed some interesting results under a simplified model that individual treatment effect only depends on the rate of exposure and the same effect carry over to the whole user regardless of the treatment assignment of a cookie. Another fundamental impact of cookie churn is that it is often hard and in some cases impossible to track users for months to understand certain long-term effect. Exercise. An experiment ran for N weeks used browser cookies as user identifier. Assume everyone only have one browser, and everyone visit the product exactly 3 times a week. If 90% of users never clear their cookie and 10% clear their cookies after every visit, e.g. in a incognito mode. How will the cookie clearing affect the sample size, and the metric of visits per user? The situation is much better for products requiring users to log in using their accounts. An account id can be a user name or a user’s email address that is used to register with a product. In this case, a straightforward approach is to anonymize this id into a GUID using a deterministic algorithm and store the anonymized id in a browser cookie. Every time this cookie gets lost, the server can always recover it by replaying the same anonymization procedure and return the cookie back. Moreover, if the same id is used to log the same user in a suite of related products, each with their own logs, this id can be also used to merge these logs together into a better view of users’ behavior, under appropriate privacy restriction. This is certainly a much more ideal situation than the simple cookie based user tracking. The drawback is also obvious, the requirement of user creating account and log in might add additional friction to attract new users. To lower this barrier, more web products start to allow users to log in using their existed account for other popular services such as facebook and google mail through OAuth. Account creation also brings other features such as user preference and settings sync. More and more new generation of web sites and mobile applications provide heavily personalized experience, making account creation a natural requirement. However, even for these products, when trying to increase new user sign up and optimize the account creation process, user tracking can still only use cookies. Randomization. Randomization is often implemented using a deterministic hash function. The purpose of the hash function here is not for cryptographic purpose, but only to efficiently bucket an item into a prescribed number of buckets. A typical hash function first maps an object, usually a string representing an id of a unit (randomization unit), into a 16bit or 32bit array and then can be converted into an integer in a range, e.g. 0 to 999. An ideal hash function for A/B tests satisfies the following properties: Uniform. The hash to the integer range is uniformly distributed. Fast and efficient. Hash function can process single item or a batch of item fast enough so the randomization step will not hurt site performance. Avalanche effect. A small change in the input (for example, change one character in the string of id) will cause the output to change significantly. This means the hash value is hard to predict. Also, avalanche effect enables us to construct a new “independent” hash functions by append or prefix a seed to the hash input. In A/B tests running in parallel, it is important that the randomization for the two overlapping experiments are independent. Since the usage of hash function does not need to be as secure as in other cryptographic usages, speed is more important. Fast hashing algorithms such as xxHash, SpookyHash, MurmurHash, CityHash are preferred to most common cryptographic hashing algorithms like MD5 or SHA. Event Tracking. No meaningful measurement can be made without basic client-side instrumentation of event tracking. A client here means a user’s local application that can interact with a web server. Browsers, modern desktop applications with network features, mobile apps are all examples of a client. A client-side instrumentation is a piece of code that will be executed by a client that can log information and (immediately or at a later time in a batch) sent to the server. Most modern web technologies are based on HTTP and JavaScript. For illustration, we use the following example of a web browsing. When a user browses a web site, a HTTP request is sent to the server of that website asking for the web page (HTTP request contains cookies in the header). The server returns a HTML — a format a browser can use to construct and render the web page. This HTML can reference static resources such as images and icons, and also reference JavaScript resources. A JavaScript is a set of procedures that can be executed in the browser when certain event triggers. For example, a piece of code in the JavaScript can say, when a link is clicked, send a new request containing information about the click including the link and the time (the payload) to the server. Since requests emitted by JavaScript and used for tracking are not really requesting information, they are usually implemented by requesting a very small resources such as 1 by 1 transparent pixel images to minimize the impact of user experience and network usage. This type of requests is sometimes called a beacon or a ping. The payload of the request will contain the event name, e.g. a click, a mouse hover event, a keyboard press event, etc. ,timestamp, and other useful information such as a session id that can allow analysis to link events together to reconstruct user’s behavior over time for a period of time. Because event tracking happens at the client-side, there is also a possibility that it will fail and cause telemetry loss. JavaScript can fail to execute. JavaScript is a piece of code and relies on the browser’s JavaScript engine to execute. JavaScript itself can contains bugs that cause crash. Beacon request fail to be received by the server. This can happen when the browser did not make the request, or network issue prevented the browser to make a successful request. Client-side data loss. Sometimes we don’t want to send each individual tracked events back instantly. For mobile devices using cellular network without WiFi, it is important that the app will not waste user’s quota on nonessential telemetry signals. In this case, telemetry can be sent back in batch, and/or wait until the device is connected to a wired or WiFi network. For this period of telemetry storing in local devices, there is always a chance they can get lost due to software bug, crash, or even user instruction of cleaning up. Telemetry loss might be impossible to avoid. A/B testing practitioners nevertheless need to understand its impact starting from a basic understanding of how bad the data lost is. In most cases even there is a minor degree of telemetry lost, as long as different testing variants (treatments and control) are impacted equally, we can treat the telemetry loss as one of the confounders that gets “randomized away.” There exist cases where the treatment can interfere with the event tracking causing differences in telemetry loss rate for different variants. Results of A/B tests in this case will be invalid and can easily lead to wildly misleading results. Click Tracking. Click event is one of the most important events to track. It has its unique challenge and complexity. If a click does not navigate out of the current page-view, then its behavior is similar to other events. However, many clicks are on a hyper-link which tells the browser to navigate to another page. There are two ways of define a hyper-link in HTML. Default is to open new page in the current view. Another is to open new page in a new browser tab or window. In the default open in current view case, there is a race condition for the browser: the browser need to open a new page to replace the current one, the browser also know the current page wants it to send a beacon to the server with click tracking payload. Which one takes priority? Turns out, there is no consensus nor standard rule to determine priority. Different browsers and different JavaScript engines might have different results. To avoid the fate of tracking such an important event by different browser implementation, engineers can insert a delay in click navigation, so the browser will try to send the beacon before start to load the new page. The larger the delay, the more likely the beacon will be successfully sent, and the more reliable click tracking will be. But large delay between users’ clicks and new page loading creates a lagged, hurting user experience. The optimal choice of delay is obviously case by case and deserves some tuning to balance between click tracking loss and user experience. Common choices vary between 50ms to 400ms. Kohavi et al. (2010) compared different choices and for a portal site like msn.com, they recommended 150ms delay for most browsers. Because click tracking is sensitive to the speed of the browser running JavaScript script and sending beacons, sometimes some treatment tested in an A/B test, especially those that touches JavaScript resources, can unexpectedly affect click tracking. Kohavi et al. (2012) documented a real puzzling example. Another much more reliable click tracking method is through HTTP redirect. When a user visiting A.com click a link to B.com, the link is programed to first go to A.com (or another domain dedicated to click tracking owned by the same tracking party) and A.com will redirect the browser to B.com. Because the click beacon in this case will be a normal HTTP request and the browser will not face a race condition of two choices, this design will theoretically have the best fidelity and lowest click loss rate. In fact, that user’s browser will not get the destination to redirect to before the server processed the click beacon. However, the round trip of sending a HTTP request just to get a redirect is usually much longer than 150ms. As a result this approach hurts user experience the most and should only be used when a very high fidelity of click tracking is required. Measuring Speed. Speed, commonly refer to web performance or site performance, is really important! In Section 7.1 we showed that for some case even 10ms is worth spending a whole engineering year to improve. How do we measure it? There are two types of measurements, server-side performance and client-site performance. Server-side performance is much easier. The server can have detailed log about the request arrival, and timestamps of server-side processes, and we can compute any time intervals and know how long it takes for server to finish those tasks. Client-side performance relies on client-side event tracking. The most common metrics for client-side performance is the notion of user perceived time to a functioning page — the time from a user’s browser starts sending HTTP request, to the time the browser receives the HTTP response and is able to render the page for user to start using the page. One of the event is called onLoad, which is triggered once the browser has fully loaded all content for a page (including script files, images, CSS files, etc.). This event can be used to measure the time to a complete experience. Other more fine-grained events can be used to measure time to different stages in the page loading process, such as Document-Object-Model (DOM) content fully loaded. In some cases, to improve perceived speed, the server will send in different chunks and the web page can start be functioning after only a few chunks are received. If the page can show a lot of contents, user can start interact with the page if the content above the fold — contents visible without scrolling down the page — are loaded. It is important to focus on performance that matters most to users’ perception. Before W3C came up with a recommendation of “Navigation Timing Specification” in 2012 for all browsers to implement, site performance tracking has to be instrumented as other event tracking. But there is a catch, the server does not know the time when the browser start sending the HTTP request — it only knows when it receives the request, and it must takes some time for the request to reach the server. Fortunately, this is a non-issue. Because when a page loaded event is triggered, the browser need to send a beacon to the server. This beacon will also take some time to reach to the server. If we assume the time it takes for the initial HTTP request to transfer from client to the server is the same as the time it takes for the beacon to travel to the server, then the server-side time interval of the two requests (the initial HTTP and the beacon request) should be the same as the client-side time interval! Modern browsers and mobile applications based on them all implemented various performance timing interfaces specified by W3C. Under the new specification browsers allow web site and web services to collect better telemetry for performance signals. 7.3 Common Pitfalls A section to briefly talk about common pitfalls. Details will be covered in later sections. 1. Fail to do AA test 2. Misinterpreting statistical result - Fail to do AA test - Underestimate variance - Differences not by the change, not really cp. - Carry over effect - misinterpreting statistical result -misinterpreting p-value -Fail to adjust for multiple testing -Fail to adjust for continuous monitoring - Fail to have enough sample size to detect no ship change for key metrics. - Fail to consider uncertainty in estimation. False trend - Trigger effect dilution error - Fail to run balanced experiment when having cache impact. - Fail to understand External validity - leap of faith? percent delta - bias due to appearance - long term effect and novelty effect https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#10e78e2960ba↩︎ In fact, 1% change is often considered to be large.↩︎ "],["abstats.html", "Chapter 8 Statistical Analysis of A/B Tests 8.1 Metric 8.2 Randomization Unit and Analysis Unit 8.3 Inference for Average Treatment Effect of A/B Tests 8.4 Independence Assumption and Variance Estimation 8.5 Central Limit Theorem and Normal Approximation 8.6 Confidence Interval and Variance Estimation for Percentile metrics 8.7 p-Value, Statistical Power, S and M Error 8.8 Statistical Challenges", " Chapter 8 Statistical Analysis of A/B Tests In Chapter 3 we introduced the two sample problem and used two sample t-test and z-test to analyze a randomized experiment. Statistical analyses of A/B tests are based on the same ideas. But in order to cover various randomization designs and extend metrics beyond simple average, we have to extend classic two sample tests with relaxed assumptions. In this chapter, after a short discussion of metrics as statistics, we review the two sample problem under more general settings. We discuss various practical aspects, including randomization and analysis units, justifying independence assumption, sample size requirement for the central limit theorem and variance estimation for different randomization designs. For interpretation of results, we review confidence interval, p-value, statistical power, and introduce Type S and M error. We end this chapter with a brief discussion of a few common challenges arise in statistical analysis, for which we will revisit later. 8.1 Metric In a data-driven culture, people talk about metric all the time. Many people treat metric value as just another number on a Business Intelligence (BI) report or a dashboard. For example, we can chart those numbers over time to identify trend and try to make forecast. They can also be used for anomaly detection, alerting and insight discovery. More sophisticated usages of metrics require us to understand the statistical properties of them. How noisy is a metric value? How confident can one trust the pattern recognized from looking at the plot? In A/B testing, we focus on comparing two metric values observed from two different groups. The crux is to know whether the observed difference of the two is due to noise or signifies a true effect of a treatment/intervention. This is where statistical concepts like variance, hypothesis testing, p-value, confidence interval are useful. There is more to the statistical perspective of a metric. For metric designers, there are a lot of basic questions on metric definition: why use average, why use percentile, why use double averaging, why do we need weighted average and is this metric definition even make sense. The choices we made in metric definition will also impact the way statistical analysis should be carried. We define metric as the following. A Metric is an estimator for a statistical quantity of a probability distribution. First, a metric is always designed to help understand certain business related characteristic of a population. Why do we care about this metric called visits per user? Because this gives us the idea of our user population’s engagement level in terms of their visiting frequency. What about page error rate for Chrome browser users? This metric measures, for the (sub)population of Chrome browser users, how frequently they encounter any error on a webpage. Population is an important concept because a lot of things implicitly depend on it. Strictly speaking, a metric and any conclusion drawn from it can only be applied to the population upon which the metric is defined. Conclusion made on the Unites States market cannot directly be extended to other international markets. Similarly, conclusion made from weekend data cannot be naively extended to weekdays. If we change the population we are technically looking at a potentially very different metric. There is also a leap of faith when we assume what we learned from the data collected now can be apply to the near future. Issues related to the change of population are called external validity. Second, a metric is for a statistical quantity of interest. A statistical quantity characterize certain property of a probability distribution and the random variable associated with that probability distribution. The two most commonly used (and probably also the two most important) statistical quantities are mean and variance. Median and other percentiles are also frequently used to define a metric. The mean of a distribution characterized the central location of the distribution. It is also called the expectation because it is “on average” what people would expect a random outcome from that distribution would be. The variance of a distribution captures magnitude of the noise. In multivariate case, we also care about the correlation or covariance between a few random variables. Third, a metric is an estimator. An estimator is a rule for calculating an estimate of a given statistical quantity based on observations. That is, an estimator is a function of observations that we use to estimate the true underlying statistical quantity of a probability distribution. Common estimators for mean and variance are average (sample mean) and sample variance. For percentile, sample percentile is an estimator. Maximum Likelihood Estimator ((MLE) is a type of estimator with good large sample properties when the distribution is known to be from a parametric family. An estimator is also a random variable due to random sampling. The distribution for an estimator is called the sampling distribution. For A/B tests, most metric definitions can be classified as one the following: Single average. These metrics are estimators for a population mean. When the observations are binary, the population mean is often called a rate, and the metric is a rate metric. Examples include visits per user, revenue per user, click-through-rate, etc. Double average. Sometimes it is useful to take an average over another average. Click-through-rate is often defined as number of clicks per page-view and thought as a single average. Another flavor of click-through-rate is to compute the rate for each user, and then take average over users. The main distinction between the two flavors is that in the latter double average flavor all users will have the same weight to influence the average, and in the single average flavor users with more page-views will influence the metric more. This can be seen from \\[ \\frac{\\sum_i C_i}{\\sum_i P_i} = \\frac{(C_i/P_i) \\times P_i}{\\sum_i P_i} \\] where \\(C_i\\) and \\(P_i\\) is the number of clicks and page-views for the i-th user, and the RHS is a weighted average of click-through-rate per user \\(C_i/P_i\\) with weight \\(P_i\\). Weighted Average. In some cases metric designer can assign explicit weight to different units. Double average metric can be considered a special case of weighted average. Percentile. Median is a more robust metric for the center of a distribution. Percentile at tail are a way to study the tail behavior, and they are popular metrics in performance/velocity community. Page-loading-time at 75%, 95% and even 99% are common percentile metrics to use. Can we define metrics beyond using average or percentile? There is no definitive answer here. For example Min and Max can be used to define very useful estimators as in the German Tank Problem17. Sum and Count are used for reporting and monitoring purposes, but neither is an estimator for a statistical quantity of a distribution. In A/B tests we haven’t seen usage of metrics beyond average (including weighted average) and percentile. Therefore, we focus on average and percentile only. 8.2 Randomization Unit and Analysis Unit Randomization unit is the unit to be randomized into different variant groups. In Section 7.2 we mentioned that we apply a fast hash function on a string representation of the randomization unit into an integer in a large range such as 0 to 999 for 1000 buckets. Randomization unit determines the granularity of the assignments, and corresponding experiences. The majority of experiments are randomize by user because we want to keep a user’s experience as consistent as possible18. For experiments where the treatment change is invisible to the users and response is instant (e.g. backend algorithm tweak and very subtle user interface change), swapping users experience between treatment and control will not confuse users and keeping a consistent experience for a user is not deemed to be important. For these scenarios impression or page-view level randomization is also used. Randomization unit can also be each visit or session. A mobile app can create a unique id each time the application is opened (or wake up after a long period being a background task) and use that as the randomization unit. Analysis unit is the unit a metric is defined on. Recall a metric is to estimate a statistical quantity for a distribution, and it is typically in a form of an average, weighted average or percentile. The analysis unit is the unit that the average or percentile is defined. In other words, each observation at the analysis unit is an observation from the distribution for which we want to infer its mean or percentile. The two units can (and often) be the same. When randomized by user, by user metrics like visits per user is an average over user level observations — its analysis unit is also user. The two units may differ. We may randomize by user but define a impression level metrics like page-load-time per page-view, or page-load-time at 95%. Both metrics uses impression as the analysis unit. The distinction between the two are very important. As a rule of thumb, Randomization unit can not be more granular than analysis unit. Observations at the Randomization unit level can often be treated as independent and identically distributed. The first rule of thumb is obvious. If randomization unit is more granular, then for each analysis unit, the observation may contain various experiences from different treatments or control. Define metrics at this level cannot separate effects from different experiences. The second rule of thumb is quite deep, and we will discuss more in the next section. But an important remark is when analysis unit is not the same as the randomization unit, we cannot simply treat observations at the analysis unit level to be independent! Failing to realize this will result in invalid statistical analyses. 8.3 Inference for Average Treatment Effect of A/B Tests In this section we review frequentist hypothesis testing and point estimation for ATE with our eyes on a general setting to apply to A/B tests. Without loss of generality, we consider only one treatment group and one control group. Let \\(Y_1^{(t)},\\dots,Y^{(t)}_{N_t}\\) are random variables representing observations at the analysis unit level for treatment and similarly \\(Y_1^{(c)},\\dots,Y^{(c)}_{N_c}\\) for the control group. Let \\(M_t, M_c\\) be the metrics defined from the two groups. Recall metrics are estimators so they are both random variables in a form of average, weighted average or percentile. Generally speaking, the distribution of \\(M_t\\) and \\(M_c\\) will heavily depend on the distribution of individual \\(Y^{(t)}\\) and \\(Y^{(c)}\\). In the classic two sample t-test, they are assumed to be independent and identically distributed normal distribution. In A/B tests, both assumptions of independence and normality are too restrictive. First, these observations are at the analysis unit level and there is no guarantee they are independent when the randomization unit is different from the analysis unit. Second, the distribution of the observations are quite often far away from normal distribution. They are often binary/bernoulli, integer valued and could be highly skewed. Fortunately, A/B tests have big data and we have the large sample/asymptotic theory to our aid. When the sample size is large, \\(M_t\\) and \\(M_c\\) can both be approximated by a normal distribution, even when individual distributions of \\(Y_i\\) are not normally distributed and not necessarily independent of each other! The issue of independence and normal approximation will be the topic of Section 8.4 and Section 8.5. For now, assuming \\[ n_t \\left (M_t - \\mu_t \\right) \\xrightarrow{d} \\text{Normal}(0,\\sigma_t^2)\\,, \\\\ n_c \\left (M_c - \\mu_c \\right) \\xrightarrow{d} \\text{Normal}(0,\\sigma_c^2)\\,, \\] where \\(n_t\\) and \\(n_c\\) is the count of the randomization units in each group, \\(\\mu_t\\) and \\(\\mu_c\\) are the true mean or percentile of the distribution. Note that we didn’t define \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\) as variances of \\(Y^{(t)}\\) and \\(Y^{(c)}\\), because this is not always the case. It is true for a simple average metric when the analysis unit is the same as the randomization unit. For general case, we have to wait until Section 8.4 to discuss how to estimate \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\). For now, all we know is \\(\\sigma_t^2/n_t\\) and \\(\\sigma_c^2/n_c\\) are the asymptotic variances of the two metrics \\(M_t\\) and \\(M_c\\). For Average Treatment Effect, the population average treatment effect is \\(\\delta = \\mu_t - \\mu_c\\) and a unbiased estimator for \\(\\delta\\) is the observed difference between the two metric values \\[ \\widehat \\delta = \\Delta = M_t - M_c\\,, \\] and from the asymptotic normal distribution of \\(M_t\\) and \\(M_c\\) we know \\(\\Delta\\) also has an approximately normal distribution such that \\[ \\frac{\\Delta - \\delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}} \\xrightarrow{d} \\text{Normal}(0,1)\\,. \\] Under the null hypothesis \\(H_0\\) that \\(\\delta = 0\\), \\[\\begin{equation} \\frac{\\Delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}} \\xrightarrow{d} \\text{Normal}(0,1)\\,. \\tag{8.1} \\end{equation}\\] If we assume \\(\\sigma_t\\) and \\(\\sigma_c\\) are known and both \\(n_t\\) and \\(n_c\\) are fixed, Equation (8.1) is the z-statistics with null distribution being the standard normal distribution. This allows us to easily construct a rejection region for a two-sided size \\(\\alpha\\) test: \\[ |\\Delta| &gt; z_{\\alpha/2}\\cdot \\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}\\,. \\] And one-sided size \\(\\alpha\\) rejection region for \\(H_0: \\delta &lt;0\\) is \\(\\Delta &gt; z_{\\alpha}\\cdot \\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}\\) and for \\(H_0: \\delta &gt; 0\\) is \\(\\Delta &lt; -z_{\\alpha}\\cdot \\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}\\). If we want to have a point-estimation with confidence interval, the unbiased estimation of \\(\\delta\\) is \\(\\Delta\\) and \\(1-\\alpha\\) two-sided confidence interval is \\[ \\Delta \\pm z_{\\alpha/2} \\cdot \\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}\\,. \\] The main results so far in this section is similar to the analysis of randomized experiments in Section 3.4, but with a few notable differences: In Section 3.4 and our earlier discussion of causal inference, the notion of the (population) average treatment effect is defined strictly for the average of the individual treatment effect. This is the same as the difference of the population means of the counterfactual pair, which in the randomized setting is the same as the difference of the population mean of observations between treatment and control groups. In this section, we generalized (or slightly abused) the notion of ATE to also include other two types of metrics: weighted average and percentile metrics. Analysis of A/B tests relies heavily on large sample theory and normal approximation based on the central limit theorem. Small sample t-test with strong assumption of normal distribution of \\(Y\\) is practically not applicable. There is no assumption as in Section 3.4 about observations \\(Y_i\\) to be i.i.d. for both treatment and control groups. Variance estimation of \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\) are also more general and often not as simple as using the standard sample variance. We have left many unsolved puzzles in this section to unravel later in this chapter. How should we make independence assumption? How to estimate \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\)? How large sample is needed for central limit theorem to work? What about percentile metrics? We will unravel them one by one. 8.4 Independence Assumption and Variance Estimation Independence assumption is one of the most fundamental assumptions for many statistical analyses. In theoretical research, independence assumption are often stated as part of a data generating process taken as a prerequisite for some results. While it is easy to start with a data generating process with strong independence assumption and derive nice results, applying the results to a real world problem almost always requires us to verify why the data generating process is an appropriate abstraction or simplification of the real world. What justifies independence assumption for a real world problem? In online experimentation, a user is often regarded as the basic unit of autonomy and hence commonly used as the randomization unit. Under what conditions can users be treated as i.i.d.? In user-randomized A/B tests, people routinely treat users as i.i.d. without much questioning, but users’ behavior obviously could be correlated by many factors such as gender, age group, location, organization, etc. Does this invalidate the user-level i.i.d. assumption? A general rule of thumb says that we can always treat the randomization unit as i.i.d. We name this the randomization unit principle (RUP). For example, when an experiment is randomized by user, page-view (S. Deng et al. 2011), cookie-day (Tang et al. 2010) or session/visit, RUP suggests it is reasonable to assume observations at each of these levels respectively are i.i.d. There were no previous published work that explicitly stated RUP. But it is widely used in analyses of A/B tests by the community, and the importance of RUP has been mentioned (Tang et al. 2010; Chamandy, Muralidharan, and Wager 2015; Kohavi, Longbotham, and Walker 2010). However, there is nothing inherent to the randomization process that justifies treating randomization unit level observations as i.i.d. When we randomize by page-view, are page-views from a single user not correlated? We define the unit at which observations can be treated as independent to be independent unit. The purpose of this section is to answer the following questions: What justifies independence assumption? Is RUP a valid rule of thumb to use? That is, is the randomization unit also an independent unit? How to estimate variance of a metric (and the ATE estimator \\(\\Delta\\)) for metrics in forms of average, weighted average and percentile? 8.4.1 Independence Assumption In probability theory, independence is clearly defined. In plain language, if two events are independent, knowing that one of those events occurred does not help to predict whether the other event occurs. In practice, independence is rarely absolute and instead always depends on context. Consider the following puzzle. There is an urn filled with numbered balls. You pull a ball from the urn, read its number, and place it back in the urn. If you do this repeatedly, are the outcomes independent? Surprisingly, such a seemingly simple question does not have a definitive answer. We asked this question to various group of students, data scientists and engineers, there were always disagreements. Here are the collected arguments from both sides. Recall if the outcomes are independent, then previous outcomes have no predictive power for the next outcome. If you have no prior knowledge of the distribution of the numbers on the balls in the urn, then as you see more balls you develop a better understanding of this distribution and therefore a better prediction for the next outcome. In this line of thinking the outcomes are dependent. Alternatively, assume you know the distribution of the numbers on the balls (e.g., uniform from 1 to 500). Then pulling balls from the urn with replacement bears no new information, and the outcomes are independent. To summarize, the observed numbers are conditionally independent given the distribution of the numbers in the urn, but unconditionally dependent. This simple example shows that the notion of independence requires a context. The context in the example above is the distribution of numbers in the urn. Whether this context is a public information that can be treated as known information or not will change the answer. In real world applications, natural unconditional independence is very rare and most independence assumptions will rely on a common context and only conditionally independent. See Wasserman (2003),Murphy (2012) or D. Barber (2012) for more on conditional independence vs. independence. The question is: who decides the context? In the previous example, who gets to say whether the distribution of the numbers on the balls are a publicly known fact or unknown? The answer is there is no single answer. Different contexts give different data generating processes which then affect both the inference procedure and also impact the interpretation of results. If we treat the distribution of numbers as a known fact, then we do inference under the data generating process where the numbers follow exactly the prescribed distribution, and any results we obtained will be limited to this context. If we treat the distribution as unknown, then the data generating process will allow this distribution to vary, and our inference will take the uncertainty of this distribution into account. The results will then be more general, as the data generating processes are less specified compared to fixing the distribution of numbers on the urns. But there is no free lunch. Doing so will make observations from the urns not independent, and as a general rule of thumb, dependencies among observations makes inference harder and less accurate. Let us further explore this idea using another example appeared in Alex Deng, Lu, and Litz (2017). We know users can be correlated by various factors such as gender, age, occupation, etc. Taking gender as an example, it is known that the heights of men and women follow different distributions. When the proportion of men and women in the population is treated as fixed, we can create the overall adult height distribution from the two gender specific distributions using weights from the gender ratio. From this point of view, heights of randomly sampled adults are i.i.d. drawn from a single mixture distribution — the “All Adults” density in Figure 8.1. The observed heights are conditionally independent given the gender ratio. We might then extend this reasoning to other factors and convince ourselves that the i.i.d. assumption for users is reasonable. Figure 8.1: Height distribution for men, women and all adults. But the above reasoning hinges on the existence of a fixed mixture distribution, which requires the mixture ratio of all those factors to be fixed. What justifies treating the gender mixture as fixed? What if we want to make inferences about subsets of the population that may have different gender ratios? When we consider the possibility that the gender ratio might not be fixed, we are implicitly talking about extending the results from the current dataset to a future dataset that might have a different gender ratio. That is, we want to know if treating the gender ratio as fixed will affect the external validity of inferences made from the current data. In A/B tests, external validity often concerns bias resulting from differences between the population from which the inference was drawn and the population upon which the inference will be later applied. When extending externally, inferences can be made invalid due to an under-estimation of uncertainty. Imagine that we sample students from 2 of 20 local 7th grade classes at random to estimate the average height of all local 7th graders. We assume that the height distributions of boys and girls in the 20 schools are the same. If we also assume the gender ratios in the 20 schools are fixed, then we can assume heights of sampled students are i.i.d. from a single mixture distribution and form a confidence interval for the mean of height. But what if even though the gender ratio is close to 50/50 in the whole school district, there exist large differences in gender ratios at different schools? Then it is possible that the two randomly chosen schools have significantly more boys than girls, or vice versa. Under this scenario, the confidence interval we get by assuming a fixed gender ratio and i.i.d. heights will be too narrow because it does not account for the variability of gender ratios among the 20 schools. In other words, the gender ratio for a sample we got may not be “representative” for the general population we want our results to apply to. To make the result externally valid, we must treat gender ratio as a random variable. The following simulation illustrates this point. Table 8.1 shows female and male student counts in 20 hypothetical schools. It is constructed such that: Each school has exactly 1,000 student. School #1 has 690 female and 310 male students. Each incremental school has 20 fewer females and 20 more males than the previous school. School #20 has 310 female and 690 male students. In total, the gender ratio is balanced with exactly 10,000 female students and 10,000 male students. Table 8.1: Female and male students configuration in a hypothetical 20 school district. School Female Count Male Count 1 690 310 2 670 330 3 650 350 4 630 370 5 610 390 \\(\\cdots\\) \\(\\cdots\\) 15 410 590 16 390 610 17 370 630 18 350 650 19 330 670 20 310 690 We generate male heights from a normal distribution with mean 175cm and standard deviation 10cm. Female heights are generated from a normal distribution with mean 160cm and standard deviation 10cm. This simulated data is assumed to be the true heights for each of the 20,000 students. In this dataset, the true average height over the 20,000 students is 167.46. The goal is to sample 200 students out of the 20,000 students to estimate the average height. Table 8.2 shows the results from the 4 sampling methods. In Case 1 we randomly sample 200 students from the combined 20,000 students. For Cases 2 to 4, we use a two-stage sampling. For Case 2 we first sample 2 schools from the 20 schools, and then sample 100 students from each of the 2 chosen schools. In cases 3 and 4 we sample 5 and 10 schools, then 40 students and 20 students for each chosen school, respectively. In all cases we sample 200 students. For each case, after we sampled 200 students, we compute the sample average and also standard deviation from the standard variance formula assuming i.i.d. observations. We used that to compute the 95% confidence interval and recorded whether the true average 167.46 was within the interval. We repeated this process 2,000 times and then compute the coverage, average estimated standard error from the standard formula, and standard deviation of the 2,000 sample averages (which approximates the true standard error of the mean). Table 8.2 shows that case 1 has the correct coverage at 95% and the standard variance formula produces a standard error that is very close to the truth. In all other cases, coverages are all lower than the promised 95%, true standard errors are all larger than those given by standard formula, confirming variance under-estimation. In fact, in all cases the standard formula gives very similar standard errors. This is because it assumes i.i.d. sampled heights. However, when samples are from a small number of randomly selected schools, there is extra variation in the population due to the fluctuation of gender ratio, making the true standard error larger than under the i.i.d. assumption. We saw as the number of schools sampled in the first stage increased from 2 to 10, the true standard deviation decreased and coverage got better. In cases 2-4, if we assume sample observations are i.i.d. and use the standard variance formula, then the inference is only valid for the average height of the chosen schools and cannot be extended to all 20 schools. If we want to make the extended inference, we need to correctly account for additional “between school” variance. Table 8.2: Comparison of 4 sampling mechanisms. The first is a random sample from all students; the other 3 cases select 2, 5 and 10 schools respectively and then sample students from the chosen schools. Results show that the standard variance formula (assuming i.i.d. observations) under-estimates the true variance and gives lower coverage than the promised 95% level. Case #School Coverage SE with standard formula True SE 1 20 0.949 0.879 0.880 2 2 0.753 0.877 1.446 3 5 0.866 0.878 1.141 4 10 0.916 0.879 0.976 From both the numbers in an urn and estimating student’s height examples, we summarize the reasoning of independence (or lack thereof) in the following steps: Sampling: We identify an unit from which we can randomly sample from. Hierarchical Model and Conditional Independence: In reality, observations at the level of the sampling unit are often considered to be dependent on certain higher level factors (e.g. same gender, age, location for users) and many units share a similar value of those factors form clusters. Under this hierarchical model, conditioned on these factors, the sampling unit are independent within each cluster. Factor Mixture and Independence: When the mixture of the factors (e.g. gender ratio, etc.) in the population is considered to be fixed and not varying. The whole distribution of the observations from the sampling unit can be modeled as independently drawn from a mixture distribution. External Validity: Independence assumption relies on treating factor mixture as fixed and not varying. Therefore any results under the independence assumption is tied to the population with a particular mixture of the factors. These results cannot be extended to another population with a very different factor mixture. Let us apply the steps to the unit of user — the commonly assumed unit of independence. Ignoring any practical user tracking issue and assuming we can always identify a user, we can certainly randomly sample users from the whole population of users. For any observation at user level, the higher level factors determining the distribution of such observations has a mixture for the population of users from which we sample from in a given period of time. When we treat this mixture as fixed, we can assume observations at user level to be i.i.d. What is the price to pay to treat the mixture as fixed? The price is we can only derive results for the specific set of user population and the specific period of time when we run the experiment. That is, external validity implies that results from today’s experiment to predict tomorrow’s behavior may not perfect, and results should always be interpreted within the tested population. If we want to allow the population mixture to change and need our results to incorporate this uncertainty, then we cannot assume users to be i.i.d. and our estimation of average treatment effect will have bigger variance and wider confidence intervals than the estimations when users are assumed to be i.i.d. There is no free lunch, the more uncertainties we want to include in our analyses, the less accuracy our estimation will be. By connecting the i.i.d. assumption with external validity, we now understand that the i.i.d. assumption is ultimately not justified by theory, but by choice. In other words, we can make conscious choices to limit the external validity of the results we get, in exchange for independence assumption. This means the question of independence can often be flipped to be a question of external validity. We can apply this argument to units like users, groups or community of users, locations, devices and many other units to justify independence assumption. However, reasoning with external validity may be very subtle in some cases and deemed a bit subjective. For example, if we randomly sample page-views, what is the factor mixture and what does it mean to external validity by fixing this mixture? In practice, many A/B testing practitioners uses the following rule of thumb: Observations at the level of the randomization unit can be treated as independent. We call this randomization unit principle (RUP). The most common randomization unit is user, and we just applied the external validity argument to justify treating it as independent unit. But we have ignored the issues of user tracking. When user tracking uses cookie, we know the real same user can appear as multiple “users.” Other popular randomization units include visit/session, page-view/impression. These units are all more granular than user. In the next section, we will justify RUP from a different angle by showing the following result. For any randomization unit more granular than an independent unit, we can approximately compute the variance of the ATE estimator assuming randomization unit is independent. This result means RUP for variance estimation only relies on the existence of an independent unit less granular than the randomization unit. That is, we do not necessarily need to wrestle with external validity for independence assumption of the randomization unit itself, we only need to assume some independent unit exists above the randomization unit! 8.4.2 Variance Estimation for Average and Weighted Average Variance estimation is directly impacted by the data generating process and independence assumption. If the metric is an average of \\(Y_i,i=1,\\dots, N\\), the variance of the metric is \\(\\mathrm{Var}(Y)/N\\) only when \\(Y_i\\) are independent. Using the variance formula for the independent case to cases where observations are not independent often lead to wild miss-estimation of the variance and invalidate the whole analysis. We already introduced the three different units: \\(\\mathcal{I}\\): the Independent unit. \\(\\mathcal{R}\\): the randomization unit. \\(\\mathcal{A}\\): the analysis unit. We posit the existence of an independent unit \\(\\mathcal{I}\\). In the previous section we explained the justification of independence is deeply connected with external validity and we argued units like user can usually be treated as independent unit. Randomization unit \\(\\mathcal{R}\\) is the unit we use to randomize. When \\(\\mathcal{R}\\) is less granular than \\(\\mathcal{I}\\) (randomization units are clusters of \\(\\mathcal{I}\\) and the clustering is random), the randomization unit is itself an independent unit. Therefore, only three cases are relevant: \\(\\mathcal{I} = \\mathcal{R} = \\mathcal{A}\\) \\(\\mathcal{I} = \\mathcal{R} &gt; \\mathcal{A}\\) \\(\\mathcal{I} &gt; \\mathcal{R} \\ge \\mathcal{A}\\) We provide variance estimations for the ATE estimator \\(\\Delta =M_t - M_c\\) for all the three cases. Case 1: \\(\\mathcal{I} = \\mathcal{R} = \\mathcal{A}\\). Consider metrics defined as an average. Since the analysis unit is also an independent unit, both the treatment and control metric \\(M_t\\) and \\(M_c\\) are average of independent observations \\(Y_i\\). Also, as the randomization unit is also the independent unit, the two metric values \\(M_t\\) and \\(M_c\\) are independent because treatment and control groups are independent. We have, \\[\\begin{align*} \\mathrm{Var}_\\text{S}(\\Delta) &amp; = \\mathrm{Var}(\\overline{Y^{t}})+ \\mathrm{Var}(\\overline{Y^{c}}) \\\\ &amp; = \\frac{\\sigma_t^2}{n_t} + \\frac{\\sigma_c^2}{n_c}, \\end{align*}\\] where the subscript \\(S\\) means the “simple” case, \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\) are variance of the independent observations \\(Y_i\\) for treatment and control groups respectively, and \\(n_t\\) and \\(n_c\\) are count of the observations. \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\) are to be estimated using sample variance formula \\[ \\widehat{\\sigma_g}^2 = \\frac{\\sum_i (Y^{(g)}_i - \\overline{Y^{(g)}})^2}{N_g-1},\\quad g = t,c\\,. \\] Consider metrics defined as weighted average. Let \\(w_i\\) be the weight for the i-th observation. The treatment metric has the form \\[ M_t = \\frac{\\sum_i w_i^{(t)} Y^{(t)}_i}{\\sum_i w_i^{(t)}}\\,. \\] This is a ratio of two sums, which also have an asymptotic normal distribution thanks to the delta method (Casella and Berger 2002; Alex Deng, Knoblich, and Lu 2018). The variance of \\(\\Delta\\) is \\[\\begin{align*} \\mathrm{Var}_\\text{D}(\\Delta) &amp; = \\mathrm{Var}\\left (\\frac{\\sum_i w_i^{(t)} Y^{(t)}_i}{\\sum_i w_i^{(t)}}\\right) + \\mathrm{Var}\\left(\\frac{\\sum_i w_i^{(c)} Y^{(c)}_i}{\\sum_i w_i^{(c)}}\\right)\\\\ &amp; = \\frac{\\xi_t^2}{n_t} + \\frac{\\xi_c^2}{n_c} \\end{align*}\\] where the subscript \\(D\\) means delta method was used, \\(\\xi_t^2\\) and \\(\\xi_c^2\\) are the delta method variance. To estimate the two delta method variance, let \\(S_i^{(g)} = w_i\\cdot Y_i^{(g)}\\) for both groups \\(g=t,c\\). \\[\\begin{equation} \\xi_g^2 = \\frac{1}{\\mathrm{E}(w^{(g)})^2}\\mathrm{Var}(S^{(g)}) + \\frac{\\mathrm{E}(S^{(g)})^2}{\\mathrm{E}(w^{(g)})^4}\\mathrm{Var}(w^{(g)})\\\\ - 2 \\frac{\\mathrm{E}(S^{(g)})}{\\mathrm{E}(w^{(g)})^3} \\mathrm{Cov}(S^{(g)},w^{(g)})\\,,\\quad g=t,c \\tag{8.2} \\end{equation}\\] where all the expectations, variances and covariance can be estimated using sample mean, sample variance and sample covariance. The details of the delta method is included in the appendix 11.2. See Alex Deng, Knoblich, and Lu (2018) for more applications of the delta method in A/B testing. Case 2: \\(\\mathcal{I} = \\mathcal{R} &gt; \\mathcal{A}\\). When the analysis unit \\(\\mathcal{A}\\) is more granular than the independent and randomization unit \\(\\mathcal{R}\\), the observations \\(Y_i\\) and weights \\(w_i\\) are not independent anymore. We need to modify the notation a little. We will keep using \\(i\\) as the index for independent (and in this case randomization) unit, and use \\(j\\) to index the observations at the analysis unit level. Observations are now denoted as \\(Y_{ij}^{g}, i = 1,\\dots,n_g, j = 1,\\dots,N_i^{g}, g = t,c\\). Also let \\(N_g = \\sum_i N_i^{(g)}\\) to be the total count of analysis unit for each group. Using a concrete example, if randomization unit is user and analysis unit is page-view. \\(Y_{ij}^{(g)}\\) is the j-th page-view level observation for the i-th user, \\(n_g\\) is the number of users, and \\(N_g\\) is the number of page-views. Average metrics have the form \\[ \\overline{Y^{(g)}} = \\frac{\\sum_i \\sum_j Y_{ij}^{(g)}}{\\sum_i N_i^{(g)}}\\,. \\] Let \\(S_i^{(g)} = \\sum_j Y_{ij}^{(g)}\\), \\[ \\overline{Y^{(g)}} = \\frac{\\sum_i S_i^{(g)}}{\\sum_i N_i^{(g)}}\\,. \\] This is again a ratio of sum of independent unit level observations — the same form we have seen in Case 1 for weighted average. We show the weighted average for this case also have a similar form. Let \\(w_{ij}^{(g)}\\) be the weight and \\[ \\overline{Y^{(g)}} = \\frac{\\sum_i \\sum_j w_{ij}^{(g)}Y_{ij}^{(g)}}{\\sum_i \\sum_j w_{ij}^{(g)}} = = \\frac{\\sum_i S_i^{(g)}}{\\sum_i W_i^{(g)}}\\,. \\] where \\(S_i^{(g)} = \\sum_j W_{ij}^{(g)}Y_{ij}^{(g)}\\), \\(W_i^{g} = \\sum_j w_{ij}^{(g)}\\). Therefore, for both average and weighted average metrics, they can be expressed as a ratio of sum of i.i.d. observations (at the independent unit level). Their variance can be estimated using the delta method similar to Equation (8.2). Case 3: \\(\\mathcal{I} &gt; \\mathcal{R} \\ge \\mathcal{A}\\). This is the most complicated case, as the same independent unit can be further split between the treatment and control group. We will use i to index independent unit, and j to index the randomization unit. In Case 2, we saw that both average and weighted average metrics have the same form \\[ \\widehat{Y^{(g)}} = \\frac{\\sum_i S_i^{(g)}}{\\sum_i W_i^{(g)}}\\,, \\] where \\(S_i^{(g)}\\) and \\(W_i^{(g)}\\) are both observations at the randomization unit level, and both have a form as a sum of analysis unit level observations. Unlike Case 2 where observations \\((S_i^{(g)}, W_i^{(g)})\\) for treatment group and control group are independent, and we can apply the delta method to the two groups separately and then add the two estimated variance up to get the estimated variance for the \\(\\Delta\\). In Case 3 the same independent unit contains observations in both groups so there is dependency between the treatment and control groups! Let \\((S_i^{(t)}, W_i^{(t)},S_i^{(c)}, W_i^{(c)}),i=1,\\dots,n\\) be independent observation vectors, the ATE estimator is \\[ \\Delta = \\frac{\\sum_i S_i^{(t)}}{\\sum_i W_i^{(t)}} - \\frac{\\sum_i S_i^{(c)}}{\\sum_i W_i^{(c)}}\\,. \\] The multivariate version of the central limit theorem says the sample mean of the 4-d vector \\((S_i^{(t)}, W_i^{(t)},S_i^{(c)}, W_i^{(c)}),i=1,\\dots,n\\) converge in distribution to a multivariate normal distribution. Since \\(\\Delta\\) is a continuous function of the sums (or sample mean) of \\((S_i^{(t)}, W_i^{(t)},S_i^{(c)}, W_i^{(c)})\\), we can again apply the delta method to get a formula for \\(\\mathrm{Var}_{\\text{G}}(\\Delta)\\), where the subscript G stands for “general” since Case 3 is the most general case as this formula contains the other two cases as special cases. The delta method can also be interpreted as the following. Asymptotically, \\(\\sqrt{n}(\\Delta - \\delta)\\) has the same normal distribution as the average of \\[\\begin{equation} \\frac{{S_{i}^{(t)} - W_{i}^{(t)}\\cdot \\mu_t}}{p \\mu_w} - \\frac{{S_{i}^{(c)} - W_{i}^{(c)} \\cdot \\mu_c}}{(1 - p)\\mu_w}\\,,i=1,\\dots,n\\,, \\tag{8.3} \\end{equation}\\] where \\[ \\mu_t = \\frac{\\mathrm{E}(S^{(t)})}{\\mathrm{E}(W^{(t)})}\\,,\\mu_c = \\frac{\\mathrm{E}(S^{(c)})}{\\mathrm{E}(W^{(c)})}\\,,\\\\ \\mu_w = \\mathrm{E}(W^{(t)}+W^{(c)}) \\] can all be estimated accordingly, and \\(p\\) is the randomization probability for the treatment group. This is an average of i.i.d. random variables and its variance can be estimated using sample variance formula. Using the the general variance formula, Alex Deng, Lu, and Litz (2017) studied the difference between the general variance formula result, to the RUP results where \\(\\mathcal{R}\\) is assumed to also be independent. Note that if we assume \\(\\mathcal{R}\\) to be an independent level, then there is no dependency between the two groups, which simplifies the variance estimation quite a lot. Here are a summary of their key results. The RUP results and the general formula are not exactly the same. RUP may under-estimate the variance. The difference is 0 when the treatment effect at each independent unit level, defined as \\(\\tau_i = E(Y_{ij}^{(t)})\\)-\\(E(Y_{ij}^{(c)})\\) is a constant. That is, when all individual treatment effects are the same and there is no treatment effect heterogeneity, RUP agrees with the general formula. Note that this is also true under the sharp null hypothesis that there is no treatment effect for every independent unit. The under-estimation of the standard error by RUP is the same order of the treatment effect. When the treatment effect is small, the under-estimation is also small. This result gives us another way to justify RUP. When the effect is large, because of large sample sizes in A/B tests, inferences are robust against underestimation of the variance. In fact, a percent effect more than 5% is considered a large effect in A/B testing. But a 5% underestimation of the standard error has a much smaller impact to the trustworthiness of the inference comparing to the caveat of external validity. The practical implication of this result is that we only need to posit the existence of an independent unit for which the induced external validity implication are acceptable, and any randomization unit equal or more granular than that independent unit can practically be treated also as independent for the purpose of variance computation. Another application of the general variance formula for Case 3 is for cases where the randomization unit are not recorded properly or intentionally removed from the data. Why do we want to remove the randomization unit info from the data? For privacy! Sometimes it is required that the data needs to be anonymized so that we cannot identify a user from the data and map a user’s strong identifier such as user name or email address by a pseudo random GUID is not enough and we have to mix multiple users into the same GUID so that it is impossible to differentiate users with the same GUID apart. When doing so, this new GUID can be treated as independent unit and all we need to apply the general variance formula (for example, see Equation (8.3)) is to be able to compute \\((S_i^{(t)}, W_i^{(t)},S_i^{(c)}, W_i^{(c)}),i=1,\\dots,n\\) for each independent unit. That is, we can still perform statistical analysis without the user identifier in the data to preserve anonymity. Clustered Randomization. Case 2 \\(\\mathcal{I} = \\mathcal{R} &gt; \\mathcal{A}\\) is also referred to as clustered randomization. We first introduced clustered randomization in Section 3.3. Because analysis units are not sampled individually, but grouped by clusters, the variance of metrics will be larger than Case 1 where randomization unit is the analysis unit. Fail to correctly apply the delta method to estimate the variance results in underestimation of the variance and larger real false positive rate (Type I error) than the nominal significance level. Under strict assumptions, closed-form variance formula for a sample average \\(\\overline{Y}\\) with clustered randomization exists (Donner 1987; Klar and Donner 2001). For example, when the number of observations in each cluster \\(N_i = m\\) are all the same and the conditional variance of observations within each cluster \\(\\sigma_i^2 = \\sigma^2\\) are also the same for all \\(i,\\) \\[\\begin{equation} \\mathrm{Var}(\\bar{Y}) = \\frac{\\sigma^2+\\tau^2}{nm}\\{1+(m-1)\\rho\\}, \\tag{8.4} \\end{equation}\\] where \\(\\tau^2 = \\mathrm{Var}(\\mu_i)\\) is the variance and \\(\\rho = \\tau^2 / (\\sigma^2+\\tau^2)\\) is the coefficient of intra-cluster correlation, which quantifies the contribution of between-cluster variance to the total variance. To facilitate understanding of the variance formula , two extreme cases are worth mentioning: If \\(\\sigma=0,\\) then for each \\(i=1,\\dots,K\\) and all \\(j = 1, \\ldots, N_i,\\) \\(Y_{ij} = \\mu_i.\\) In this case, \\(\\rho=1\\) and \\(\\mathrm{Var}(\\overline{Y})=\\tau^2/K;\\) If \\(\\tau=0\\), then \\(\\mu_i = \\mu\\) for all \\(i=1, \\ldots, K,\\) and therefore the observations \\(Y_{ij}\\)’s are in fact i.i.d. In this case, \\(\\rho=0\\) and Equation (8.4) reduces to \\(\\mathrm{Var}(\\overline{Y})=\\sigma^2/(nm) = \\sigma^2/N\\). Unlike the delta method variance estimator Equation (8.2), Equation has only limited practical value because the assumptions it makes are unrealistic. In reality, the cluster sizes \\(N_i\\) and distributions of \\(Y_{ij}\\) for each cluster \\(i\\) are different, which means that \\(\\mu_i\\) and \\(\\sigma_i^2\\) are different. But it did illustrate that the coefficient of intra-cluster correlation is one of the reason why clustered randomization has larger variance. If we look closer to the delta method variance Equation (8.2), the second term is \\[ \\frac{\\mathrm{E}(S^{(g)})^2}{\\mathrm{E}(w^{(g)})^4}\\mathrm{Var}(w^{(g)}) \\] where \\(\\mathrm{E}(w^{(g)})\\) and \\(\\mathrm{Var}(w^{(g)})\\) are the mean and variance of the total weights in a cluster. For simple average total weights for the i-th cluster is just \\(N_i\\) — the number of analysis units within each cluster. Equation (8.2) tells us not only the coefficient of intra-cluster correlation plays a role, but also the variance of the cluster size! To reduce variance of the metric, clusters with more homogeneous sizes are preferred to clusters with very different sizes. Another common approach to study clustered randomization is to use a mixed effect model, also known as multi-level/hierarchical regression (Gelman and Hill 2006). In a such model, we model the observations at analysis unit level \\(Y_{ij}\\) to be conditionally independent with mean and variance \\(\\mu_i\\) and \\(\\sigma_i^2\\) for each cluster, while the parameters themselves follow a higher order distribution represents heterogeneity of clusters. Under this setting, we can infer the treatment effect as the “fixed” effect for the treatment indicator term19. Stan (Carpenter et al. 2016) implements a Markov Chain Monte Carlo (MCMC) solution to infer the posterior distribution of those parameters, but this needs significant computational effort when data is big. Moreover, the estimated ATE, i.e., the coefficient for the treatment assignment indicator, is for the randomization unit (i.e., cluster) but not the analysis unit level, because it treats all clusters with equal weights and can be viewed as similar to the double average \\(\\sum_i (\\sum_j Y_{ij} / N_j) / n,\\) which is usually different than the population average \\(\\overline Y\\) (A. Deng and Shi 2016). This distinction doesn’t make a systematic difference when effects across clusters are homogeneous. However, in reality the treatment effects are often heterogeneous, and using mixed effect model estimates without further adjustment steps could lead to biases. Proper reweighting adjustment does exists (Alex Deng, Knoblich, and Lu 2018). 8.5 Central Limit Theorem and Normal Approximation So far we have been assuming the sampling distribution of a metric \\(M\\), being either average, weighted average or percentile, can be approximated by a normal distribution once the sample size (of independent observations) is large enough. This is from the central limit theorem (CLT), which says for i.i.d. random variables \\(X_i,i=1,\\dots,n\\) with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), \\[ \\sqrt{n} \\left( \\overline{X} - \\mu \\right) \\xrightarrow{d} \\text{Normal}(0,\\sigma^2) \\,, \\] where the convergence is in the sense of convergence in distribution (Van der Vaart 2000). When \\(X\\) has vector value, such as in the weighted average and clustered randomization case where \\(\\mathcal{R}&gt;\\mathcal{A}\\), the multivariate version of CLT \\[ \\sqrt{n} \\left( \\overline{X} - \\mu \\right) \\xrightarrow{d} \\text{Normal}(0,\\Sigma) \\,, \\] is used, where \\(\\mu\\) is the mean vector and \\(\\Sigma\\) is the covariance matrix. To apply the classic CLT, we need both i.i.d. and finite variance. Finite variance can be usually justified because most observations used to define metrics are naturally bounded. For potentially unbounded metrics, such as time-to-click, in practice we will always cap it (winsorize) by a practical bound. This will both gives us finite variance, and also crucial to tame the variance of the metric so it can achieve a reasonable statistical power. For the i.i.d. assumption, we justified it by connecting it to external validity and the randomization unit principle. However, being an asymptotic result, CLT does not tell us how fast is the convergence to normal distribution and how large of sample size is needed. The answer to this question varies. One rule of thumb and widely adopted opinion is normal approximation works for \\(n&gt;30\\). This seems to be easily achievable in A/B tests where sample sizes are typically at least in thousands. In this section, we will look closer and suggest a required sample size to be at the order of \\(100s^2\\), where \\[ s = \\frac{\\mathrm{E}((X-\\mu)^3)}{\\sigma^3} \\] is the skewness of \\(X\\). The key to study the convergence to normal is to use Edgeworth series (Hall 2013) that further explore the residual terms beyond the normal approximation. Let \\(F(x)\\) be the cumulative distribution function of \\(\\sqrt{n} ( \\overline{X} - \\mu )/\\sigma\\) and \\(\\Phi(x)\\) and \\(\\phi(x)\\) be the cumulative distribution function and density function for the standard normal. Boos and Hughes-Oliver (2000) showed that \\[\\begin{equation} F(x) = \\Phi(x) - \\frac{6}{\\sqrt{n}}s(x^2 - 1)\\phi(x)+ O(1/n) \\tag{8.5} \\end{equation}\\] Three comments: The first error term involves the skewness s and is in the order of \\(n^{-1/2}\\). The second error term is in the order of \\(1/n\\) and the constant term not shown above actually involves the 4th standardized moment kurtosis as well as skewness. If skewness is 0, e.g. when \\(X\\) is symmetric, converge to normal increases from \\(1/\\sqrt{n}\\) to \\(1/n\\) — an order of \\(n^{1/2}\\) faster. Let us focus on the first error term \\(\\frac{6}{\\sqrt{n}}s(x^2 - 1)\\phi(x)\\). For a typical 95% two-sided confidence interval, we look at the 2.5% and 97.5% percentile, and they are very close to -1.96 and 1.96 for the standard normal distribution. Let \\(x = \\pm 1.96\\) and if we want this error term to be less than 0.25%, then we will be getting \\(n &gt; 122.56s^2\\). For a true 95% confidence interval, the probability of missing the true mean at the right and at the left are both 2.5%. With \\(n &gt; 122.56s^2\\) independent samples, ignoring higher order error terms, the actual probability of missing the true mean at the right and left are below 2.75%. That is, we need a sample size of the order of \\(100s^2\\) if we want true missing out rate to be within 10% of the designed level. Boos and Hughes-Oliver (2000) also showed error term for a t-statistic when the variance is unknown. It is similar to the know variance case and they only differ by a constant term and error is bigger due to the unknown variance. Kohavi et al. (2014) derived \\(355s^2\\) based on similar calculation for the unknown variance case. Overall we think the order of \\(100s^2\\) is a better rule of thumb than \\(n&gt;30\\). Most metrics don’t have a large skewness more than 5, and the required sample size is less than 2,500. Kohavi et al. (2014) reported revenue per user to have a skewness of 17.9 from one of their dataset, and more than 30,000 sample size to get to the desired error limit based on our \\(100s^2\\) rule. However, once a proper capping was applied, that is to apply the function \\(f(x) = min(x,c)\\) for a cap c, the skewness dropped to 5 and the required sample size dropped to 2,500! Surprisingly, the most common example of high skewness metric does not even involve large metric value. In fact, a Bernoulli distribution with \\(p\\) close to either 0 or 1 can have huge skewness. If we have a metric that tracks occurrence of certain rare events, such as client error rate. The metric itself is average of binary observations and the skewness of a Bernoulli(\\(p\\)) distribution is \\(\\frac{1-2p}{pq}\\) where \\(q=1-p\\). When \\(p\\) is close to 0, this is about \\(1/\\sqrt{p}\\). A (not so rare) event with \\(p\\) around \\(10^{-4}\\) can lead to a skewness of 100! Figure 8.2 show the sampling distribution of a mean of 50,000 i.i.d. Bernoulli random variables with p=\\(10^{-4}\\). Our recommended sample would be \\(10^6\\) and 50,000 is still very much short of that. As expected, the distribution is far from normal and right-skewed, as easily seen from the QQ plot. We also observed that the distribution is still very discrete, because \\(np\\) in this case is only about 5. Figure 8.2: Right: Sampling distribution of sample mean of 50,000 i.i.d. Bernoulli random variables with p=\\(10^{-4}\\). Left: QQ-Norm plot. Right: Histogram. The distribution is discrete and severely right-skewed. What should we do in cases like this? There are two approaches. The practical approach is to use a balanced design where treatment and control are of the same sample size. The alternative is to adjust the confidence interval or approximate the cumulative distribution function directly using Equation (8.5) with the error term. We focus on balanced design first and will introduce two simple adjustment methods for the Bernoulli case. The reason balanced design mitigates the issue and require much less sample size for normal approximation to work is because we will be looking at the confidence interval of the ATE estimator \\(\\Delta\\). When the sample size of treatment and control groups are exactly the same, \\(\\Delta\\) is sample average of i.i.d. random variable \\(Z = X - Y\\) where X and Y are the random variable for observations in treatment group and control group respectively. The rationale is the skewness of \\(Z\\) is much smaller than \\(X-Y\\). In the sharp null hypothesis case where there is no individual treatment effect, X and Y have the same distribution and X-Y is symmetric with 0 skewness! When skewness is 0, the convergence to normal is \\(\\sqrt{n}\\) faster, roughly meaning the sample size needed is square root of the sample size needed when skewness is the leading term. But even when skewness of Z is not exactly 0, reducing it by a factor of 10 reduces the sample size needed by a factor of 100! Figure 8.3 shows the sampling distribution for \\(\\Delta\\) of two sample means of 50,000 i.i.d. Bernoulli random variables with p=\\(10^{-4}\\). This time we saw the distribution is much closer to normal, with only a little fatter tail. Figure 8.4 shows if the sample size is imbalanced, then the one with larger sample size will be closer to normal (90,000 samples in the example, close to 100,000 needed by \\(100s^2\\) rule) and the one with less sample size still shows strong skewness and discrete spikes, therefore \\(\\Delta\\) behaves like mixture of many normal distribution with many modes. Because balanced design is often also optimal for the best statistical power, the normal approximation sample size requirement gives another reason to use a balanced design for high skewness metrics. Figure 8.3: Right: Sampling distribution of the difference of two sample mean of 50,000 i.i.d. Bernoulli random variables with p=\\(10^{-4}\\). Left: QQ-Norm plot. Right: Histogram. The distribution of the difference is much closer to normal even though each sample mean is severely skewed. Figure 8.4: Right: Sampling distribution of the difference of two sample means of Bernoulli random variables with p=\\(10^{-4}\\), one with 10,000 i.i.d. sample and the other with 90,000. Left: QQ-Norm plot. Right: Histogram. The distribution is multi-model as a mixture of normal distributions. The adjustment of confidence interval using Edgeworth expansion is known as the Cornish–Fisher expansion (S. R. A. Fisher and Cornish 1960). Also see Hall (2013). We found for high skewness due to large value, capping is the most effective and also improve statistical power. For Bernoulli rare event case, we introduce two simple and easy to implement adjustment methods. Both methods still use normal approximation but correct both the sample mean and the estimated variance. In both methods the derived confidence interval will have wider range and closer to the nominal coverage. Let the total sample size be \\(n\\) and the sum of i.i.d. Bernoulli random variables \\(n_1\\). For small p \\(n_1\\) is small. The first adjustment is Laplace Smoothing, which artificially add one positive binary outcome to the Bernoulli samples. With this adjustment, the “sample mean” becomes \\((n_1+1)/(n+1)\\) and the estimated variance will also use the new adjusted “sample mean.” The second adjustment is proposed in Agresti and Coull (1998) and known as Agresti-Coull correction. Like Laplace smoothing, it adds artificial observations to the samples. For two-sided confidence interval of \\(1-\\alpha\\), let \\(z_{\\alpha/2}\\) be the \\(1-\\alpha/2\\) percentile of standard normal, Agresti-Coull adds \\(z_{\\alpha/2}^2\\) artificial samples with half of them positive. The new “sample mean” becomes \\(\\tilde{p} = \\frac{n_1+z_{\\alpha/2}^2/2}{n+z_{\\alpha/2}^2}\\), and the variance of the approximate normal distribution for the sample mean is \\(\\frac{\\tilde{p}(1-\\tilde{p})}{n+z_{\\alpha/2}^2}\\). For 95% confidence interval, \\(z_{\\alpha/2}\\) is about 2 and the adjustment is to simply add 4 instances with 2 successes. 8.6 Confidence Interval and Variance Estimation for Percentile metrics Percentile (also called quantiles) metrics are widely used to focus on tails of distributions. This is very common for performance measurements, where we not only care about an average user’s experience, but even more care about those who suffer from the slowest responses. Within the web performance community, percentiles (of, for example, page loading time) at 75%, 95% or 99% often take the spotlight. In addition, the 50% quantile (median) is sometimes used to replace the mean, because it is more robust to outlier observations (e.g., due to errors in instrumentation and telemetry). This section focuses on estimating the variances of percentile metrics. Suppose we have \\(n\\) i.i.d. observations \\(Y_1,\\dots,Y_n,\\) generated by a cumulative distribution function \\(F(Y) = P(Y\\le y)\\) and a density function \\(f(y)\\)20. The theoretical p-th percentile for the distribution \\(F\\) is defined as \\(F^{-1}(p)\\). Let \\(Y_{(1)}, \\dots, Y_{(n)}\\) be the ascending ordering of the original observations. The sample percentile at \\(p\\) is \\(Y_{(np)}\\) if \\(np\\) is an integer. Otherwise, let \\(\\lfloor np\\rfloor\\) be the floor of \\(np\\), then the sample quantile can be defined as any number between \\(Y_{(\\lfloor np\\rfloor)}\\) and \\(Y_{(\\lfloor np\\rfloor +1)}\\) or a linear interpolation of the two21. For simplicity here we use \\(Y_{(\\lfloor np\\rfloor)},\\) which will not affect any asymptotic results when \\(n\\) is sufficiently large. It is a well-known fact that, if \\(Y_1, \\ldots, Y_n\\) are i.i.d. observations, following the central limit theorem and a rather straightforward application of the Delta method, the sample percentile is approximately normal (Casella and Berger 2002; Van der Vaart 2000): \\[\\begin{equation} \\sqrt{n} \\left\\{ Y_{\\lfloor np\\rfloor} - F^{-1}(p) \\right\\} \\xrightarrow{d} N \\left[ 0, \\frac{\\sigma^2}{f \\left\\{F^{-1}(p)\\right\\}^2} \\right], \\tag{8.6} \\end{equation}\\] where \\(\\sigma^2 = p(1-p).\\) However, unfortunately, in practice we may not have i.i.d. observations when randomization unit is not the same as the analysis unit. For site performance, randomizations are usually on users or devices but analysis unit are at each page-view/impression level. Let \\(I_i = I \\{ Y_i \\le F^{-1}(p) \\},\\) where \\(I\\) is the indicator function. The proof of Equation (8.6) involves a key step to compute \\(\\mathrm{Var}(\\overline I)\\).22 When \\(Y_i\\) are not independent, we use the delta method to compute \\(\\mathrm{Var}(\\overline I)\\) and the only change in the result would be to replace \\(\\sigma^2 = p(1-p)\\) by the adjusted standard deviation. That is, Equation (8.6) still holds in the clustered case with \\(\\sigma^2 = n \\mathrm{Var}(\\overline I)\\). This generalizes the i.i.d. case where \\(n\\mathrm{Var}(\\overline I) = p(1-p)\\). It is still difficult to apply Equation (8.6) in practice because the denominator \\(f \\{ F^{-1}(p) \\}\\) involves the unknown density function \\(f\\) at the unknown true quantile \\(F^{-1}(p).\\) A straightforward approach is to estimate \\(f\\) from the observed \\(Y_i\\) using non-parametric methods such as kernel density estimation (Wasserman 2003). However, any non-parametric density estimation method is trading off between bias and variance. To reduce variance, more aggressive smoothing and hence larger bias need to be introduced to the procedure. This issue is less critical for quantiles at the body of the distribution, e.g. median, where density is high and more data exists around the quantile to make the variance smaller. As we move to the tail, e.g. 90%, 95% or 99%, the noise of the density estimation gets bigger, so we have to introduce more smoothing and more bias. Because the density shows up in the denominator and density in the tail often decays to \\(0\\), a small bias in estimated density can lead to a big bias for the estimated variance (Brown and Wolfe (1983) raised similar criticisms with their simulation study). A second approach is to bootstrap, re-sampling the whole dataset many times and computing quantiles repeatedly. Unlike an average, computing quantiles requires sorting, and sorting in distributed systems (data is distributed in the network) requires data shuffling between nodes, which incurs costly network I/O. Thus, bootstrap works well for small scale data but tends to be too expensive in large scale in its original form (efficient bootstrap on massive data is a research area of its own (Kleiner et al. 2014)). Alex Deng, Knoblich, and Lu (2018) introduced a novel idea to estimate percentile variance without estimating the unknown density function. Recall that \\(I_i = I \\{Y_i \\le F^{-1}(p) \\}\\). \\(\\sum I_i\\) is the count of observations no greater than the quantile. Consider the independent case where \\(I_i\\) are i.i.d., \\(\\sum I_i\\) follows a binomial distribution. Consequently, when \\(n\\) is large \\(\\sqrt{n}(\\overline I-p) \\approx \\text{Normal}(0, \\sigma^2)\\) where \\(\\sigma^2 = p(1-p)\\). If the quantile value \\(F^{-1}(p) \\in [Y_{(r)},Y_{(r+1)}),\\) then \\(\\overline I = r/n\\). The above equation can be inverted into a \\(100(1-\\alpha)\\%\\) confidence interval for \\(r/n:\\) \\(p \\pm z_{\\alpha/2}\\sigma/\\sqrt{n}\\). This means with 95% probability the true percentile is between the lower rank \\(L = n(p-z_{\\alpha/2}\\sigma/\\sqrt{n})\\) and upper rank \\(U = n(p+z_{\\alpha/2}\\sigma/\\sqrt{n})+1\\)! Now consider the general case where \\(I_i\\) are not independent. \\(\\sum I_i\\) would not follow a binomial distribution but the central limit theorem stills applies and the variance of \\(\\overline{I}\\) can be estimated using the delta method. The above procedure still works with a small adjustment to make: we can no longer assume \\(\\sigma^2 = p(1-p)\\) as in the independent case and instead need to do an extra step of delta method to estimate \\(\\sigma^2\\). To summarize, the confidence interval for a percentile can be computed in the following steps: Fetch the quantile \\(Y_{(\\lfloor np\\rfloor)}\\). Compute \\(I_i = I \\{Y_i \\le Y_{(\\lfloor np\\rfloor)}\\)}. Apply the delta method to estimate \\(\\mathrm{Var}(\\overline{I})\\). Compute \\(\\sigma\\) by setting \\(\\sigma^2/n\\) equal to the estimated variance of \\(\\overline{I}\\). Compute \\(L,U = n(p \\pm z_{\\alpha/2}\\sigma)\\). Fetch the two ranks \\(X_{(L)}\\) and \\(X_{(U)}\\). We call this percentile CI with pre-adjustment23. This method reduces the complexity of computing a percentile and its confidence interval into a Delta method step and subsequently fetching three “ntiles.” Alex Deng, Knoblich, and Lu (2018) also proposed a slightly different percentile CI with post-adjustment method that has some computational advantages over the pre-adjustment method: Compute \\(L,U = n(p \\pm z_{\\alpha/2}\\sqrt{p(1-p)/n})\\). Fetch \\(Y_{(\\lfloor np\\rfloor)}\\), \\(Y_{(L)}\\) and \\(Y_{(U)}\\). Compute \\(I_i = I \\{Y_i \\le Y_{(\\lfloor np\\rfloor)}\\)}. Apply the delta method to estimate \\(\\mathrm{Var}(\\overline{I})\\). Compute \\(\\sigma\\) by setting \\(\\sigma^2/n\\) equal to the estimated variance of \\(\\overline{I}\\). Compute the (multiplicative) correction factor \\(\\sigma / \\sqrt{p(1-p)}\\) and apply it to \\(X_{(L)}\\) and \\(X_{(U)}\\). The post-adjustment method computes the percentile CI pretending \\(Y\\) to be independent. Because Equation (8.6) tell us the only change we need to make from independent to clustered randomization case is to adjust the variance of the asymptotic normal distribution by a rescale factor of \\(\\frac{\\sigma^2}{p(1-p)}\\), the confidence interval should also be adjusted accordingly by a rescale of \\(\\frac{\\sigma}{\\sqrt{p(1-p)}}\\)! Using CLT and assuming sample percentile is approximately normally distributed, we can then convert any \\(1-\\alpha\\) two-sided confidence interval to the variance of sample percentile. Note that confidence interval from the percentile CI method might not be exactly symmetric. In practice we can make it symmetric by picking the larger width of the two sides as the half width of a symmetric interval. 8.7 p-Value, Statistical Power, S and M Error The whole large sample theory of A/B testing24 for ATE is based on the asymptotic normality of \\(\\Delta\\), \\[ \\frac{\\Delta - \\delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}} \\xrightarrow{d} \\text{Normal}(0,1)\\,. \\] We spent a few sections in this chapter digesting components of this result: the independent assumptions, the sample size needed for the normal approximation and variance estimation — all are critical to ensure trustworthy inferences. In this section we use it to summarize results and interpret them. 8.7.1 p-Value Let \\(Z = \\frac{\\Delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}}\\) be the observed z-score. Given a pre-specified Type I error bound \\(\\alpha\\), we can derive the two-sided rejection region to be \\(|Z| &gt; z_{\\alpha/2}\\). This is because under null hypothesis \\(\\delta=0\\), \\(Z\\) follows a standard normal distribution. Let \\(\\mathcal{Z}\\) be the standard normal random variable, \\[ P(|Z| \\ge z_{\\alpha/2}|H_0) = P(|\\mathcal{Z}|\\ge z_{\\alpha/2}) = \\alpha\\,. \\] We say this rejection region controls the Type I error, also called false rejection or false positive rate. Rejection by a pre-specified Type I error bound and the corresponding rejection region is a binary decision. In practice, people prefer to use a continuous quantity. One choice is to just use the observed Z-score. A more common approach is to map Z-score to p-value defined as \\[ P(|\\mathcal{Z}|\\ge|Z|)\\,. \\] Here the observed Z-score \\(Z\\) was considered to be fixed and p-value represents the probability that under the null hypothesis we will get a Z-score (absolute value in two-sided test case) more extreme than or equal to the observed one. p-value is always between 0 and 1 and p-value of \\(\\alpha\\) corresponds to a Z-score of \\(z_{\\alpha/2}\\). p-value is more popular than Z-score because it has the scale of a probability and it has a probabilistic meaning. For Type I error bound of \\(\\alpha\\), the rejection region can be defined simply for all p-values less than \\(\\alpha\\). For a given experiment, the smaller a p-value is, the stronger the evidence against the null hypothesis based on the observations we got. However, the probabilistic nature and the seemingly simple definition makes p-value notoriously easy to be misinterpreted! Goodman (2008) documented 12 common misunderstandings. We summarize two patterns here. Misunderstanding 1. A p-value of 5% means the null hypothesis has only a 5% chance of being true. This statement is false because p-value does not tell us the probability of null hypothesis being true. The definition of p-value is a conditional probability of observing certain statistic given the null hypothesis is true. Roughly speaking, it is similar to \\(P(Data|H_0)\\). (\\(P(Data|H_0)\\) is the likelihood of observing the data under the null hypothesis. This is different from the probability of observing a z-score beyond the observed one. But they represents similar idea in the current context.) This is very different from \\(P(H_0|Data)\\), which is the probability the null hypothesis being true given what have been observed. In an extreme case where the null hypothesis is always true, \\(P(H_0|Data) = 1\\). What kind of p-value can we expect? Because the null hypothesis is always correct, the Z-score will follow standard normal distribution and the p-value will follow a uniform distribution! That is, we expect to have 5% chance seeing a p-value less than 5%. But the null hypothesis \\(H_0\\) is 100% true no matter how low p-value is. To get \\(P(H_0|Data)\\), we have to use the Bayes theorem \\[\\begin{equation} P(H_0|Data) = \\frac{P(Data|H_0)P(H_0)}{P(Data|H_0)P(H_0)+P(Data|H_1)P(H_1)}\\,. \\tag{8.7} \\end{equation}\\] To really know \\(P(H_0|Data)\\), we need to know the prior probability \\(P(H_0)\\) that a null hypothesis is true. And also \\(P(Data|H_1)\\) — the likelihood under alternative hypothesis. We leave a deeper discussion of this when we talk about Bayesian A/B testing in Chapter ??. The source of the misunderstanding is p-value represents a probability that is in the wrong direction people normally want to make their decision upon. Many people expect \\(P(H_0|Data)\\), but p-value tells \\(P(Data|H_0)\\). A closely related misunderstanding states that if you reject the null hypothesis based on a 5% p-value, the probability of making a Type I error (false positive error) is only 5%. The mistake is that to make a Type I error or false positive, the null hypothesis needs to be false, so the probability of making a Type I error given 5% p-value is really \\(P(H_0|\\text{p-value}\\le 5\\%)\\), which is different from \\(P(\\text{p-value} \\le 5\\%|H_0)\\)!. The latter \\(P(\\text{p-value} \\le 5\\%|H_0)\\) is 5% from the definition of p-value, but the former \\(P(H_0|\\text{p-value}\\le5\\%)\\) is not and in particular relies on the prior probability \\(P(H_0)\\). When we say Type I error or positive rate is bounded by \\(\\alpha\\), these are all conditioned on \\(H_0\\) being true. Real life is when we make decision we have to consider both null and alternative have a chance to being true. Misunderstanding 2. Studies with the same p-value provide the same evidence against the null hypothesis. This misunderstanding happens when we expect p-value to fully represent the degree of evidence we have against the null. Using Bayesian theorem, we see the odds of the alternative against the null given the observations is \\[\\begin{equation} \\frac{P(H_1|Data)}{P(H_0|Data)} = \\frac{P(Data|H_1)}{P(Data|H_0)}\\times \\frac{P(H_1)}{P(H_0)}\\,. \\tag{8.8} \\end{equation}\\] The term \\(\\frac{P(H_1)}{P(H_0)}\\) is the prior odds, which only depends on the two hypotheses and not on the observations. The term \\[ \\frac{P(Data|H_1)}{P(Data|H_0)} \\] is the likelihood ratio represents the evidence from the observations comparing the null hypothesis to the alternative. p-value only represents \\(P(Data|H_0)\\). \\(P(Data|H_1)\\) is the second component in this term related to statistical power. The evidence from the observations need to always compare the likelihood under both hypotheses, not under null hypothesis alone. The difficulty lies in the ambiguity of the alternative hypothesis. If the null hypothesis is the ATE \\(\\delta\\) equals 0. What is the alternative \\(\\delta\\)? Define the alternative \\(H_1\\) to be \\(\\delta\\neq 0\\) does not help us evaluate \\(P(Data|H_1)\\). 8.7.2 Statistical Power Most of the emphases in the null hypothesis testing framework has been put on the null hypothesis alone. For instance, we derive the distribution of a test statistics, e.g. a z-score, under the null hypothesis and call this the null distribution of the test statistic. A p-value is computed from the null distribution to be the probability of observing a more extreme test statistic under the null hypothesis. In Equation (8.8) when we think about the evidence comparing the alternative hypothesis to the null hypothesis, we found the complete evidence from observed data consists of two terms, one conditioned under the null hypothesis and one under the alternative hypothesis. This means both hypotheses are on an equal footing when we want to access the strength of evidence comparing the two. However, in the (frequentist) null hypothesis testing framework, \\(P(Data|H_1)\\) does not show up unless \\(H_1\\) is specified to be a fixed value. In fact, we can only compute \\(P(Data|H_0)\\) and \\(P(Data|H_1)\\) when the hypothesis is simple and not composite. A simple hypothesis specifies the value of the parameter such as \\(\\delta\\) being tested, while a composite hypothesis only specifies a set or range of values for the parameter. When the parameter is not fixed, the whole idea of conditional probability like \\(P(Data|H_1)\\) is ill-defined unless we posit a prior distribution \\(P(\\delta|H_1)\\) for the parameter under each hypothesis. When we do that, we are no longer in frequentist territory but thinking like a Bayesian. In frequentist method, the closest thing to \\(P(Data|H_1)\\) is statistical power, defined as the probability of rejecting the null hypothesis if the alternative hypothesis is true. First, statistical power depends on the rejection level \\(\\alpha\\) — the intended Type I error bound under null hypothesis. Second, it depends on the true parameter value. In A/B tests, we need to specify the true ATE \\(\\delta\\), and the power is \\[ P(|\\mathcal{Z}|&gt;z_{\\alpha/2})\\, \\] where the random variable \\(\\mathcal{Z}\\) is no longer a standard normal but has a shift in mean and approximately \\(\\text{Normal}\\left(\\frac{\\delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}},1\\right)\\). If we fix \\(\\alpha\\), the statistical power is a function of the true parameter \\(\\delta\\) and is hence called the power curve. When \\(\\delta=0\\), this is the null hypothesis and the probability of rejection is \\(\\alpha\\). As \\(\\left|\\frac{\\delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}}\\right|\\) becomes larger, the statistical power increases to 100%. For \\(\\delta\\neq 0\\), statistical power represents the probability of successfully reject the null hypothesis when it is false. And it increases when \\(|\\delta|\\) increases or \\(\\sigma_t^2/n_t + \\sigma_c^2/n_c\\) decreases. Because statistical power is the probability of reject the null hypothesis given a true parameter \\(\\delta\\) under the alternative hypothesis, it is often used as a measure of sensitivity of a test. 1- power is the false negative rate, or Type II error rate. We saw statistical power depends on \\(\\delta\\) and \\(\\sigma_t^2/n_t + \\sigma_c^2/n_c\\). When \\(\\sigma_t^2\\) and \\(\\sigma_c^2\\) are considered to be known and fixed, and \\(\\delta\\) considered to be out of the experimenter’s control, the only way to change the statistical power for a prescribed \\(\\alpha\\) is to increase the sample sizes \\(n_t\\) and \\(n_c\\). Therefore, we can compute the sample sizes needed to reach a certain level of statistical power for a given \\(\\delta\\). This computation is known as power analysis. When \\(\\sigma_t\\) and \\(\\sigma_c\\) are assumed to be equal, it is easy to see for a fixed total sample size \\(n = n_t+n_c\\), the balanced split where \\(n_t=n_c=n/2\\) gives the smallest \\(\\sigma_t^2/n_t + \\sigma_c^2/n_c\\) and hence highest power. Exercise: Prove the balanced sample size split gives the best power for a given total sample size. For many practitioners of A/B test, power analysis might seem subjective to some extent. There is only a power curve, and the power depends on not only sample size, but also the assumed true parameter value. When we say we want to compute sample size needed to reach to a certain power level, e.g. 80%, which parameter value should we use? Indeed, we don’t know the true effect \\(\\delta\\). In practice, there are two approaches. For a given sample size, we can invert the power calculation to show the minimum parameter value \\(\\delta\\) needed to reach a power level. We can think this minimum parameter value as the resolution of the test, in the sense that any effect less than this is hard to detect. If this minimum value is deemed to be too large, the resolution is not good enough and we need to increase sample size at experiment design time. For A/B testing, there is always a cost in ship a new change. Many new feature requires additional cost. Some new functionality may need extra hardware to support, or simply adding new code to existing codebase would add code maintenance cost (“software debt”). Hence to ship a change its benefit has to meet a criterion. Using this minimum criterion as the parameter value in power analysis provide a starting point. A common malpractice in power analysis is to use the observed effect \\(\\Delta\\) in place of the real effect \\(\\delta\\). The result is often called observed power or post-hoc power (Hoenig and Heisey 2001). It can be shown that the observed power is a function of p-value. Because p-value is from the likelihood under the null hypothesis, observed power as a function of p-value does not provide any insight about the likelihood under the alternative hypothesis. It is misleading to use observed power as a surrogate of the real power. Because the complexity in computing or even talking about the statistical power as it depends on both real parameter value and sample size, it does not always receive enough attention as p-value and Type I error get. This is a big mistake as we have argued above that \\(P(Data|H_1)\\) and \\(P(Data|H_0)\\) are equally important as representing the evidence from the observed data comparing the two hypotheses. We call experiments fail to run with enough sample size to meet power requirement low power experiments. Low power experiments are very problematic and need to be avoided for a few reasons: The treatment may have a positive effect and the experiment failed to detect (reject null hypothesis) that due to the lack of power. The treatment may have a negative effect and the experiment failed to detect. And experimenter decided to launch the treatment by the argument of “no hurt,” that is the treatment not worse than the control. The treatment may have a positive effect detected, but it may also have unexpected negative effects on some key metrics that were not detected due to lack of power. It is important to have enough sample size to make sure appropriate statistical power is reached for metrics the experiment is designed to move, and also for all key metrics even if the experiment is not expected to move these metrics. Another common misunderstanding of power is that it is only relevant before the experiment, after the experiment if we observe a low p-value and can call the result statistically significant, power does not mean anything. This is to say even the power analysis gives a 50% or lower power, if experimenters are willing to take the chance, they may well find a p-value less than 5%. Given the observed p-value, then we can ignore the lower power warning. A win is a win, no matter how unlikely the odds were, right? No! Statistical power is still important for the following reasons: For a mature product such that a change is less likely to have an effect, the strength of evidence for a low powered experiment is weak unless p-value is extremely small. In particular, borderline p-values around 0.05 in a low powered experiment is very weak. ATE estimated from a low powered analysis with less than 50% power can be overly exaggerated on an average of more than 40%! We will explain the second point in the next section when we talk about Type M Error. For the first point, consider a product with a prior odds \\(P(H_1)/P(H_0)\\) of 1:4. That is, about only 20% of changes truly make a difference. In Table 8.3 we substitute power as \\(P(Data|H_1)\\) and p-value as \\(P(Data|H_0)\\) into Equation (8.8) (This is not exactly correct as we explained earlier, but the spirit is not far from the truth.). Also let us assume the alternative \\(H_1\\) is a simple hypothesis with a fixed parameter value of \\(\\delta\\). We found that for p-value of 5%, which is commonly used as the threshold of statistically significant, 80% power is needed to make the probability of alternative being true to be 80%. For a 1% p-value, 40% power means more than 90% probability of alternative being true and 80% power gives about 95%. Interestingly, a common misunderstanding of p-value is think 5% p-value means 95% probability of the alternative being true. The simplified computation here shows this is far from the truth when prior odds is 1:4. A p-value threshold of 1% is much stronger, but still does not mean 99% confidence of the alternative being true. Benjamin et al. (2018) suggested replacing 0.05 by 0.005 as the \\(\\alpha\\) threshold for hypothesis testing. Table 8.3: Rough approximation of \\(P(H_1|Data)\\) using power and p-value to compute the likelihood ratio, with prior odds \\(P(H_1)/P(H_0) = 1/4\\). Power p-value \\(P(H_1|Data)\\) 10% 5% 33.3% 20% 5% 50% 40% 5% 66.7% 80% 5% 80% 10% 1% 71.4% 20% 1% 83.3% 40% 1% 91% 80% 1% 95.2% 8.7.3 Type S and Type M Error Gelman and Carlin (2014) introduced two new errors beyond the classic Type I and Type II errors for standard two-sided test. They are called Type S (Sign) error and Type M (magnitude) error. Type S error rate is defined as the following: under the alternative hypothesis (\\(\\delta\\neq 0\\)) and given we reject the null at significant level \\(\\alpha\\), the probability that we observe a Z-score with the opposite sign to true effect \\(\\delta\\). Similarly, under alternative hypothesis, the expected Type M error (exaggeration ratio) is defined as the expectation of \\(|\\Delta|/\\delta\\) — the ratio of observed effect to the true effect in magnitude, given that we rejected the null at significant level \\(\\alpha\\). Both error are complements of the statistical power, which only considers the probability of rejecting the null hypothesis when the alternative hypothesis is true. Type S and M error considers when the rejection happens, what is the chance that we make mistakes in sign for the estimated effect and what is the average error in magnitude of the estimation? Figure 8.5: (Appeared in Lu, Qiu, and Deng (2018)) Illustration of S and M error from 5000 simulations of a low power (8%) study at the significance level \\(0.05\\) with small positive true effect. The grey round points correspond to statistically insignificance. The black triangular points correspond to occurrences of the type S error. The black (triangular and squared) points together correspond to occurrences of the type M error (statistically significant but over-estimates the magnitude of the effect) Figure 8.5 illustrated the two errors. Note that both errors only focus on cases where a rejection decision is made. It can be shown that both errors are a function of power for the two-sided test of ATE. Table 8.4 shows the mapping for moderate powers more than 50% to high powers of 95% in a two-sided test with \\(\\alpha=5\\%\\). Type S Error is only an issue when power is very small and practically 0% for moderate power. In fact to illustrate S error in Figure 8.5, the simulation was done with a very low power of only 8%. However, M error is still large for power between 50% to 70%. At 50% power, the exaggeration ratio (M error on average) is 40%! This means at this error, when we observe an estimated ATE \\(\\Delta\\) that rejects the null hypothesis, this \\(\\Delta\\) is on average 40% more than the true ATE \\(\\delta\\). It has a positive bias due to the hypothesis testing selection! Even with a 80% power, the exaggeration ratio is still more than 10%. We found more than 95% power gives 3% exaggeration ratio. If we chose to use a lower p-value threshold \\(\\alpha=0.005\\), as suggested by Benjamin et al. (2018). Table 8.5 shows the exaggeration ratio decreased from 40% to 28% at power 50%, but a power of more than 80% is still needed to keep exaggeration ratio below 10%. This result indicates that if the estimated ATE \\(\\Delta\\) is also important for decision making, the statistical power of 80% is required not just to have a fair chance of detect the effect but also to avoid large over-estimation of the true effect due to the selection bias of picking statistically significant results. Table 8.4: Mapping from power to exaggeration ratio (expected M Error) and S Error Rate when \\(\\alpha=0.05\\). Power Exaggeration Ratio S Error Rate 50% 40% &lt;0.01% 60% 29% \\(\\approx\\) 0% 70% 20% \\(\\approx\\) 0% 80% 13% \\(\\approx\\) 0% 85% 9% \\(\\approx\\) 0% 90% 6% \\(\\approx\\) 0% 95% 3% \\(\\approx\\) 0% Table 8.5: Mapping from power to exaggeration ratio (expected M Error) and S Error Rate when \\(\\alpha=0.005\\). Power Exaggeration Ratio S Error Rate 50% 28% \\(\\approx\\) 0% 60% 21% \\(\\approx\\) 0% 70% 15% \\(\\approx\\) 0% 80% 9.5% \\(\\approx\\) 0% 85% 7% \\(\\approx\\) 0% 90% 5% \\(\\approx\\) 0% 95% 2.4% \\(\\approx\\) 0% 8.8 Statistical Challenges In this last section of the chapter, we briefly discuss a few challenges arise in statistical analyses and results interpretation of A/B tests. Challenges listed here are by no means an exhaustive list and many of these challenges also exist in other areas of statistical analysis and not exclusive to A/B tests. Nevertheless, A/B testing really manifested these issues due to its scale, agility, rich data and fluid analysis. Here, we introduce the challenges and mention some existing researches without going into details. More details will be given for some of the challenges in the following chapters. Multiple Testing. The first challenge is the problem of multiple comparison, or multiple testing. The significance level \\(\\alpha\\) in a null hypothesis testing procedure means if the null hypothesis is true, each test has \\(\\alpha\\) of false rejection. When many tests are done simultaneously, we expect on average a proportion of \\(\\alpha\\) of true null hypotheses being falsely rejected. Multiplicity in A/B tests takes multiple dimensions. The first is the number of metrics. Each metric represents a different statistical quantity of interest to compare. It is common to have more than 100 metrics and even thousands of metrics. The second dimension is segmentation. The analysis of one metric can be further divide into different tests for different subgroups of the population. The third dimension is number of variant group comparisons. One A/B test can involve more than one pair of comparison when there are multiple treatments (sometimes called an A/B/n test). This is particularly common for early stage experiments, where experimenters want to assess a large number of ideas and filter the candidates down to only a few promising ones. Another dimension is time. Because of the continuous arrivals of new data, the analysis can also be done sequentially. Multiplicity due to time is also known as the issue of Continuous monitoring and Sequential Testing. There are many known multiple testing adjustment methods, the most well-known and straightforward being the Bonferroni correction, which is to multiply the p-value by the number of multiplicity. These methods are often over-conservative and significantly reduce the sensitivity of a test due to the extremely large number of multiplicity in A/B tests. Methods like false discovery rate (Benjamini and Hochberg 1995) control are less conservative, but the adjustment procedure depends on results of all the tests. In particular, the adjusted results might change if an experimenter decides to add some new segments to analyze. For continuous monitoring of A/B tests, peaking the results and stop experiment opportunistically is one of the common malpractices. Correct inference has to take each interim analysis into account and in statistics literature this line of researches is called sequential analysis (Bartroff, Lai, and Shih 2012). Johari et al. (2017) applied SPRT (sequential probability ratio test) to the scenario of continuous monitoring in A/B testing and also combined this with multiplicities from other dimensions like the number of metrics being tested simultaneously. Alex Deng, Lu, and Chen (2016) studied this problem from a Bayesian angle. Selection Bias and Winner’s curse. Multiple testing can be considered as a special case of a broader issue of selective inference. That is, we cast a wide net of a large number of questions to ask, and only choose to pay attention on a small subset of them that are the “most interesting” selected by certain criteria. Typical selection procedures used in A/B tests include looking at results with low p-values, looking through results using different segments or optionally add new segments to create new analyses, checking a group of metrics only when another metric shows “interesting” results, etc. Inference using the same data that the selection was made is problematic similar to the notion of data snooping is bad in machine learning. Like the training error generally underestimated the true error, post-selection estimates are usually biased. From all metric results from a A/B test, if we select only those with small p-values, the ATE estimator \\(\\Delta\\) are no longer unbiased as it would be without the selection. We introduced Type M error as one way to quantify this bias and we have seen this selection-bias can be quite significant unless the experiment is sufficiently powered to 80% or more. Because of the selection-bias, if we rerun the same experiment again and look at the same \\(\\Delta\\), it is more likely to be smaller in magnitude than the previous run. This phenomenon is also known as “the winner’s curse.” Correct the selection-bias and more generally provide new statistical inference techniques that take selection and data snooping into account is getting a lot of attention in the statistics community because the trend of big data and cheap computation making post-selection inference a salient issue instead of a “silent scandal.” Researches reinvestigated the Bayesian framework [Senn (2008);Lu and Deng (2016);Efron (2011);] and proposed new frequentist inference techniques like bias estimation and correction [reid2017post;@lee2018winner], data splitting and data carving (Wager and Athey 2017; Fithian, Sun, and Taylor 2014; Taylor and Tibshirani 2015), and applications to high-dimensional statistical model selection (Taylor and Tibshirani 2018; Taylor and Tibshirani 2015; R. F. Barber and Candes 2016) all came out in recent years. Heterogeneous Treatment Effect. Although the average treatment effect for a metric is often the first thing experimenters care about and so far we have been focusing on ATE estimator \\(\\Delta\\) in this chapter, it is common sense that individual treatment effect varies: Different subjects reacts to the same change in a different way. Individual treatment effects can vary in magnitude, and also in direction. It is very valuable for experiments to be able to understand how treatment effect varies across several dimensions of the subjects, and be able to find insights explaining why different sub-populations respond to the same change in opposite directions. Time is another important dimension. Many changes have different effect in week days and weekends, and also during work hours and after-work hours. Some treatment might also display novelty effect. A common novelty effect is when a big change is introduced and users need some time to learn the change, in that case initially there might be increased usage such as visits and clicks but the effect decays after users get used to the change. Novelty effect is important because it is not unusual for a change to initially attracts more usage but eventually reduce to mediocre or even negative effect. There are several challenges in detecting and explaining Heterogeneous treatment effect (HTE): High dimension with low signal noise ratio. There are many ways to split the total population into different sub-populations. Because A/B testing typically have low signal noise ratio and we even struggle to improve sensitivity to detect the average treatment effect for the whole population, analyze a large number of sub-populations both decrease the signal noise ratio (since sample sizes are smaller for sub-populations) and increase multiplicity in the multiple testing challenge. Interoperability and actionability. In an experiment with a lot of metrics, experimenters are dealing with large streams of information from each A/B test. Introducing HTE will further introduce more insights for experimenters to consume. It is crucial for these insights to be actionable, that is the experimenters can interpret the results and know how to take action based on the insight. This means the HTE results need to be high fidelity and low chance of being a noise, and the HTE structure is memorable and make sense. Long-Term Effect and Feedback Loop. Novelty effect is a time-dependent heterogeneous treatment effect that can be studied in a short period experiment such as weeks. Long-term effect considers the effect of a change after a longer period of months. It is expected that some types of behavior can only be changed through a long time of exposure to a change. Hohnhold, O’Brien, and Tang (2015) gave an example that improving ads quality by removing lower quality ads in search engine results can reduce users’ “ads blindness” and encourage them to click ads more. The effect of removing low quality ads have a negative effect on the revenue in the short-term, but pays off in the long-term when user clicks high quality ads more. Gomez-Uribe and Hunt (2016) also mentioned that certain changes may have a long feedback loop where a change also impacts other components in the system that can eventually impact the effect of the initial change itself. Take online ads as an example, changing ads quality threshold also will change click-through-rate prediction algorithm, which can also impact the ads defect rate estimation algorithm. Feedback loop can also be external. Because one is often not the only player in the market, an introduced change can cause a shock in the market and takes time to reach to a new equilibrium. Understanding long-term effect is difficult. Running a long experiment of months is against agility, and takes bigger effort to make sure the experiment won’t be polluted by various data quality issues, human errors like interaction and conflict with other experiments, as well as technical issue such as cookie churn being a much bigger problem. Violation of SUTVA Assumption, Leakage and Network Effect. The stable unit treatment value assumption (SUTVA) is an important assumption in the potential outcome framework. It is the reason why we usually can talk about a counterfactual pair instead of a counterfactual vector. Under SUTVA, the potential outcomes of one unit cannot be affected by the assignment of other units, and hence a unit’s potential outcomes only has the degree of freedom equal to that of the treatment assignments. When there is only one treatment, the potential outcomes for any unit is only a pair. Without SUTVA, the potential outcomes for each unit depends on the whole treatment assignment configuration of all units! For a study of n units, one treatment and control creates \\(2^n\\) different assignment configurations, and we need to consider a potential outcome vector of dimension \\(2^n\\)! What a life saver the SUTVA is! However, there are known cases when SUTVA is considered too restrictive and not appropriate. The most notable example is network effect. When there is a network structure connecting a population, the treatment effect for a unit may depend on behaviors of units in the neighborhood, and their behaviors are affected by their treatment assignments! The ideal treatment effect we want to infer is to compare the counterfactual where all units are assigned treatment to the other extreme when all units are in control. Another issue is leakage effect, where SUTVA is violated in the way that units assigned to control will be affected by treatment units. For example, in skill matching of an online shooting game, any change in the matching algorithm will affect all gamers matched together. If the treatment algorithm tweak a stronger gamer into a lower skill level, they will get an easier game but other gamers at that level will be facing harder opponents. Another example is when the treatment and control are running in a combined environment with shared hardware resources, a treatment using more hardware resources can negatively affect control by starving its resources. In this case both treatment and control will be slowed down. External Validity. All A/B testing relies on a leap of faith in External validity. That is, we believe the results we learned in one experiment will carry over into the future for the same population we experimented at a different time. Nevertheless, there are cases we intentionally run experiment on one population and want to project the result to a different population. For one example, beta users are willing to participate in active software or firmware update which make them a good population to test client side software such as operation system. But beta users are very different from the general public users. They probably are more technically savvy, have better hardware and network, or the devices joined the beta program are simply not mission critical. In these cases, external validity remains a big issue and we need to adjust the estimation to account for the differences in populations. http://en.wikipedia.org/wiki/German_tank_problem↩︎ Due to the limitation of user tracking, when cookie is used for user tracking, some level of experience fragmentation is still expected. But at least that experience discontinuity only happens when users change browsers or devices.↩︎ Y \\(\\sim\\) Treatment + (1|User) in lme4 notation. A detailed discussion is beyond the scope of this paper; see Gelman and Hill (2006),Bates et al. (2014).↩︎ We do not consider cases when \\(Y\\) has discrete mass, and \\(F\\) will have jumps. In this case the percentile can take many values and is not well defined. In practice this case can be seen as continuous case with simple discrete correction.↩︎ When p is 0.5, the 50% quantile, or median, is often defined as the average of the middle two numbers if we have even number of observations.↩︎ The rigorous proof involves a rather technical next step that is beyond our scope. A formal proof can be found in Van der Vaart (2000).↩︎ Alex Deng, Knoblich, and Lu (2018) called it outer-CI as the independent case is related to the outer-CI method (Krewski 1976; Meyer 1987).↩︎ Both Frequentist and Bayesian.↩︎ "],["abdiagnosis.html", "Chapter 9 System Diagnosis and Quality Checks for A/B Tests 9.1 System Validation using A/A Test 9.2 Sample Ratio Mismatch 9.3 Trigger and Filter Condition 9.4 Interaction Detection 9.5 Metric Denominator Mismatch", " Chapter 9 System Diagnosis and Quality Checks for A/B Tests 9.1 System Validation using A/A Test 9.2 Sample Ratio Mismatch 9.3 Trigger and Filter Condition 9.4 Interaction Detection 9.5 Metric Denominator Mismatch Diagnosis AA test and retroAA test, p-value should be uniform! Check performance balance! SRM, common reason Linkedin and Yahoo! both published paper, also our own papers trigger condition issue Check metric distribution, outlier MDM Trigger and filter compliment Interaction detection "],["sensitivity.html", "Chapter 10 Improving Metric Sensitivity 10.1 Metric Sensitivity Decomposition 10.2 Variance Reduction", " Chapter 10 Improving Metric Sensitivity There are two important aspects for a metric: its directionality and sensitivity (A. Deng and Shi 2016). The directionality of a metric tells us how to interpret the movement of it. In other words, the movement of the metric positively or negatively should be able to tell us something good or bad happened. Metrics without a clear directionality may still be useful for other purposes such as to provide side information for another main metric, but they are unfit to aid directly in decision making. Sensitivity of a metric reflects how likely we can move the metric in an experiment. The more sensitive the metric, the more actionable the metric is. A metric can have very good directionality but if it rarely moves in a typical A/B tests, it is barely useful. Directionality is largely inherent from the design and definition of the metric itself. Sensitivity, on the other hand, not only depends on the metric definition, but also the experiment design and statistical method used for inference. 10.1 Metric Sensitivity Decomposition By definition, metric sensitivity reflects how easy a metric can be moved in an A/B test. What do we mean by move a metric? We have learned in Chapter 8 that being a statistic, the movement of a metric need to be interpreted in a statistical way. For a predefined confidence level \\(\\alpha\\), we declare a metric moved if the null hypothesis of zero ATE is rejected at \\(\\alpha\\) level. That is, the p-value of a two-sided null hypothesis test is less than \\(\\alpha\\), or equivalently the two-sided \\(1-\\alpha\\) confidence interval excludes zero. Under this definition, the probability of a metric moved in an A/B test is given by: \\[\\begin{equation} P(\\text{Reject $H_0$}) = \\int P(\\text{Reject $H_0$}|\\delta)dP(\\delta)\\,, \\tag{10.1} \\end{equation}\\] where \\(\\delta\\) is the true ATE and the integration is to average over a distribution of treatment effect \\(\\delta\\). From Equation (10.1) it is easy to see the sensitivity can be decomposed into two factors: The statistical power: The probability of rejecting the null hypothesis given a fixed ATE \\(\\delta\\). The distribution of the true ATE \\(\\delta\\). Therefore, to improve the sensitivity, we can either improve the statistical power, or change the effect \\(\\delta\\) itself. This can also be seen from the z-score \\[ \\frac{\\Delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}} \\,, \\] whose mean value is \\[ \\frac{\\delta}{\\sqrt{\\sigma_t^2/n_t + \\sigma_c^2/n_c}} \\,. \\] On average, to increase the value of the absolute value of z-score, we either increase the numerator \\(|\\delta|\\) — that is to change the distribution of \\(\\delta\\), or decrease the denominator to increase statistical power. When we found a metric hard to move in experiments and seeks way to improve its sensitivity, it is crucial to understand which one of the two factors is the main issue. In most cases lacking sensitivity of a metric is mainly due to low statistical power, and we should focus on reducing variance of the metric. We can reduce variance of the metric by either increasing the sample size of each variants, or coming up with a more efficient ATE estimator that has less variance than the standard \\(\\Delta\\). Increasing sample size can be done by running experiment at a larger traffic percentage for each variant group, e.g., using 20% traffic for both treatment and control instead of 5%. This approach is limited to 50% treatment and 50% control as the maximum, and even less traffic per group if we run multiple treatments concurrently. Another common way of increasing sample size is to run experiments longer. Two week experiment typically have more users than one week experiment. However, as mentioned in Section 7.1, when we change experiment period from one week to two weeks, the population mixture for heavy user and casual user also changes, as well as the distribution of metrics. As the result metric variance may not decrease, as shown in Figure 7.1. Nevertheless, for most metrics running experiment longer increases statistical power. Reducing metric variance via more efficient ATE estimator is always a good thing to do whenever possible to make sure we don’t leave any “free” sensitivity on the table. This is be our topic in Section 10.2. When low sensitivity is due to the numerator — the \\(\\delta\\), it means the distribution of \\(\\delta\\) is very close to 0 and can be treated as practically 0 most of the time. The first question we should be asking is what is the triggering rate of the change we are testing. If the change we make by design will only affect a very small proportion of the population, then majority of the users will not have any treatment effect and including them in the analysis will only dilute the \\(\\delta\\). By zooming into those users who are affected by the treatment, we can increase \\(|\\delta|\\) and improve sensitivity. Analyzing only triggered population is called triggered analyses. When doing triggered analysis, it is 10.2 Variance Reduction So far we have been using the simple difference of sample averages \\(\\Delta\\) to estimate the ATE \\(\\delta\\). Randomization guarantees it is unbiased, that is \\(E(\\Delta) = \\delta\\). The statistical power of the two sample test based on \\(\\Delta\\) relies on its variance \\(\\mathrm{Var}(\\Delta)\\). Reducing its variance increases statistical power and the sensitivity of the metric. Usually, there is a trade-off between variance and bias, so called bias-variance trade-off. However, as we will see, there is a way to reduce the variance without introducing any bias. Statisticians calls this efficiency augmentation (Tsiatis 2006). Efficiency augmentation here means more accurate estimation using the same amount of information or sample size. In other words, we seek to come up with a new estimator \\(\\Delta^*\\) to replace \\(\\Delta\\) that is still unbiased and with smaller variance. We show a simple and powerful idea based on variance reduction using control variates from Monte Carlo simulation. The general theory of regression adjustment and semiparametric efficiency augmentation is closely related to the idea of doubly robust estimation in Section 4.9. 10.2.1 Control Variates and CUPED In the context of Monte Carlo simulation, we face a similar problem of estimate the mean \\(\\mathrm{E}(Y)\\) of a random variable \\(Y\\) for which we can simulate (draw sample) from. The naive Monte Carlo estimator is the sample mean \\(\\overline{Y}\\), similar to the naive ATE estimator \\(\\Delta\\). Control variates provide an alternative Monte Carlo estimator with smaller variance. To do that, we need another random variable \\(X\\) with known mean \\(\\mu_x=\\mathrm{E}(X)\\). For any fixed value of \\(\\theta\\), the following is also an unbiased estimator for \\(\\mathrm{E}(Y)\\): \\[ \\widehat{Y}_{cv}:=\\overline{Y} - \\theta \\overline{X} + \\theta \\mu_x\\,. \\] The unbiasedness of \\(\\widehat{Y}_{cv}\\) is due to the fact that last two terms on the right hand side cancels with each other since \\(\\mathrm{E}(\\overline{X})= \\mu_x\\). Also note that this estimator requires \\(\\mu_x\\) to be known to even be defined. The variance of this newly created estimator \\(\\widehat{Y}_{cv}\\) is \\[\\begin{align*} \\mathrm{Var} (\\widehat{Y}_{cv}) &amp;= \\mathrm{Var} (\\overline{Y} - \\theta \\overline{X}) = \\mathrm{Var} (Y-\\theta X)/n \\\\ &amp;=\\frac{1}{n} (\\mathrm{Var} Y) + \\theta^2\\mathrm{Var} (X) - 2\\theta\\mathrm{Cov} (Y,X))\\,. \\end{align*}\\] Note that \\(\\mathrm{Var} (\\widehat{Y}_{cv})\\) is minimized when we choose \\[\\begin{align} \\theta = \\mathrm{Cov}(Y,X)/\\mathrm{Var}(X) \\tag{10.2} \\end{align}\\] and with this optimal choice of \\(\\theta\\), we have \\[ \\mathrm{Var} (\\widehat{Y}_{cv}) = \\mathrm{Var}(\\overline{Y}) (1- \\rho^2) \\,, \\] where \\(\\rho = \\mathrm{Cor} (Y,X)\\) is the correlation between \\(Y\\) and \\(X\\). That is \\[ \\frac{\\mathrm{Var} (\\widehat{Y}_{cv})}{\\mathrm{Var}(\\overline{Y})} = 1 - \\rho^2 \\,. \\] That is, the variance is reduced by a factor of \\(\\rho^2\\)! The larger \\(\\rho\\), the better the variance reduction. The single control variate case can be easily generalized to include multiple variables. It is interesting to point out the connection with linear regression. The optimal \\(\\theta\\) turns out to be the ordinary least square (OLS) solution of regressing (centered) \\(Y\\) on (centered) \\(X\\), which in multiple variable case has variance \\[\\begin{align*} \\mathrm{Var} (\\widehat{Y}_{cv}) &amp;= \\mathrm{Var}(\\overline{Y}) (1- R^2) \\,, \\end{align*}\\] with \\(R^2\\) being the proportion of variance explained coefficient from the linear regression. It is also possible to use nonlinear adjustment. Instead of allowing only linear adjustment, we can minimize variance in a more general functional space. Let \\[\\begin{align} \\widehat{Y}_{cv} = \\overline{Y} - \\overline{f(X)} + \\mathrm{E}(f(X)), \\tag{10.3} \\end{align}\\] and then try to minimize the variance of . It can be shown that the regression function \\(\\mathrm{E} (Y|X)\\) gives the optimal \\(f(X)\\). Exercise: Prove \\(f(X) = \\mathrm{E} (Y|X)\\) is the optimal control variates for \\(Y\\) using \\(X\\). So far we have been looking at one sample mean and assume the mean of control variate \\(\\mu_x\\) to be known. Utilizing control variates to reduce variance is a very common technique in Monte Carlo simulation (Asmussen and Glynn 2008). The difficulty of applying it usually boils down to finding a control variate \\(X\\) that is highly correlated with \\(Y\\) and at the same time has known \\(\\mathrm{E}(X)\\). Alex Deng et al. (2013) made the observation that in a randomized experiment, we don’t need to know \\(\\mu_x\\) to use \\(X\\) as control variate because we care about the ATE, not the two means \\(\\mathrm{Y^{(t)}}\\) and \\(\\mathrm{Y^{(c)}}\\) for treatment and control groups. If we replace \\(\\overline{Y^{(t)}}\\) by \\(\\overline{Y^{(t)}_{cv}}\\) and \\(\\overline{Y^{(c)}}\\) by \\(\\overline{Y^{(c)}_{cv}}\\), and then define \\[\\begin{align*} \\Delta^* &amp;:= \\overline{Y^{(t)}_{cv}} - \\overline{Y^{(c)}_{cv}} \\\\ &amp;= \\Delta(Y) - \\theta \\Delta(X) + \\theta (\\mathrm{E}(X^{t})-\\mathrm{E}(X^{c}))\\,. \\end{align*}\\] Here \\(\\Delta(Y) = \\overline{Y^{t}} - \\overline{Y^{c}}\\) and \\(\\Delta(X) = \\overline{X^{t}} - \\overline{X^{c}}\\) are simple difference of sample means. From Equation (10.4) it is apparent that if \\(\\mathrm{E}(X^{t})=\\mathrm{E}(X^{c}))\\), the last term disappear and \\[\\begin{equation} \\Delta^* = \\Delta(Y) - \\theta \\Delta(X) \\tag{10.4}\\,. \\end{equation}\\] In a few elementary steps, we have achieved wonder. This new \\(\\Delta^*\\) in Equation (10.4) does not involve any unknown mean \\(\\mathrm{E}(X^{t})\\) or \\(\\mathrm{E}(X^{c})\\). Moreover, its variance can be greatly reduced comparing to the original ATE estimator \\(\\Delta\\) thanks to control variate \\(X\\). The only requirement is the control variates \\(X\\) we picked need to satisfy the condition \\[ \\mathrm{E}(X^{t})=\\mathrm{E}(X^{c}))\\,. \\] that is, the ATE on \\(X\\) must be zero. These kind of \\(X\\) are abundant in practice, because a treatment cannot possibly impact anything observed before an experiment unit get exposed to the treatment. Alex Deng et al. (2013) calls these pre-experiment variables and named the simple procedure described so far CUPED (Controlled experiment Using Pre-Experiment Data), and demonstrated the effectiveness of using the same metric data for the pre-experiment period as control variates performs well in practice. Figure 10.1 shows the variance of a metric reduced by more than 50%. With only half of the original sample size, \\(\\Delta^*\\) of CUPED produces more statistical power than estimating ATE by \\(\\Delta\\). Figure 10.1: Variance Reduction in Action for a real experiment. Top: p-value. Bottom: p-value when using only half the users for CUPED. A few important remarks: Optimal \\(\\theta\\) is \\(\\mathrm{Cov}(Y,X)/\\mathrm{Var}(X)\\) for control variates. In CUPED, we have treatment and control groups. Should we use treatment or control group to define the optimal \\(\\theta\\)? The answer is it usually does not matter much. Note that the CUPED estimator \\(\\Delta^*\\) is unbiased for any fixed value of \\(\\theta\\), and different \\(\\theta\\) merely affect variance reduction rate. As long as the treatment effect is not too big, the optimal \\(\\theta\\) for the two groups are very close and it does not make a lot of difference which one to choose. If needed, one can optimize \\(\\theta\\) to minimize the variance of \\(\\Delta^*\\) directly. The minimizer of \\(\\mathrm{Var}(\\Delta^*)\\) is \\[ \\frac{\\mathrm{Cov}(\\overline{Y^{t}},\\overline{X^{t}})+\\mathrm{Cov}(\\overline{Y^{c}},\\overline{X^{c}})}{\\mathrm{Var}(\\overline{X^{t}})+\\mathrm{Var}(\\overline{X^{c}})}\\,. \\] Another choice is to use pooled data of treatment and control to compute \\(\\theta\\). In Section 10.2.2 we will use results from more general semiparametric theory to bring a much clearer picture. Equation (10.4) can be trivially extended to nonlinear adjustment: \\[\\begin{equation} \\Delta^* := \\overline{Y^{(t)}_{cv}} - \\overline{Y^{(c)}_{cv}} = \\Delta(Y) - \\Delta(f(X)) \\tag{10.5}\\,. \\end{equation}\\] Similar to control variates, the closer \\(f(X)\\) to \\(\\mathrm{E}(Y|X)\\), the better the variance reduction. Pre-experiment data does not literally means data collected before the experiment begins. It can be anything before the triggering of the treatment intervention. For example, the day-of-week a user is first observed in the experiment is independent of the experiment itself, as well as the age, gender, browser and the device a user uses. Control variates \\(X\\) can be categorical (discrete) or continuous. When \\(X\\) is categorical, we can use one-hot encoder to create dummy binary variables, and CUPED can be seen as post-stratification adjustment, which is shown to be asymptotically as efficient as doing actual blocking (stratified sampling) (Miratrix, Sekhon, and Yu 2013). For some choice of \\(X\\), it might be not be well-defined for a subset of experiment units. For example, new users just appeared during the experiment do not exist before the experiment period and the pre-experiment period metric value for them are simply not defined. In such cases, Alex Deng et al. (2013) proposed to impute with \\(0\\) and at the same time also include an binary indicator variate to indicate whether a unit has valid pre-experiment and use both in a multiple regression version of CUPED. There is a deep connection between control variates and linear regression. However, control variates method does not actually assume any linear relationship between \\(X\\) and \\(Y\\). The linear regression and the optimal \\(\\theta\\) being solution of OLS is simply a working model. The extension to nonlinear \\(f(X)\\) makes it clear that the working model can be any model and the quality of the model only affects the variance reduction rate. Therefore, CUPED is not the same as directly fitting a linear regression with treatment assignment indicator and predictors \\(X\\), despite the resemblance. In the next section we will use a more general semiparametric model to emphasize the distinction. 10.2.2 General Regression Adjustment and Doubly Robust Estimation There is a clear resemblance between CUPED and linear regression. Let \\(A_i\\) be the binary treatment assignment indicator of the ith experiment unit. Consider the following two common linear regression models: \\[\\begin{equation} Y_i = \\alpha + \\delta A_i + \\beta X_i + \\epsilon_i \\tag{10.6} \\,, \\end{equation}\\] and \\[\\begin{equation} Y_i = \\alpha + \\delta A_i + \\beta X_i + (\\gamma X_i)A_i + \\epsilon_i \\tag{10.7}\\,. \\end{equation}\\] Under the standard linear regression model assumptions, the regressors \\(X\\) and \\(A\\) are considered to be fixed, the residual \\(\\epsilon_i\\) are assumed to be i.i.d. with a normal distribution. The only random component in the underlying data-generating-process are from the residuals alone. \\(\\alpha+\\beta X\\) is the prediction for \\(Y\\) in the control group based on \\(X\\). \\(\\delta\\) is the average treatment effect and \\(\\gamma X_i\\) in the second model is the linear treatment effect adjustment that allows the conditional treatment effect \\(\\mathrm{E}(\\tau|X)\\) to be a linear function of \\(X\\). Fitting the linear model to get estimators of those coefficients and their sampling distributions are also known as Analysis of (Co)Variances (ANOVA/ANCOVA). We call the first model ANCOVA1 and the second ANCOVA2. Both models are widely used in two group comparison for both experiment data and also observational data. Many have pointed out the efficiency gain from the regression model to increase accuracy of estimating the average treatment effect \\(\\delta\\). Nevertheless, it is obvious that the linear model is too restrictive. The data-generating-process for real world problems will involve random \\(X\\) and \\(A\\); the true regression \\(\\mathrm{E}(Y|X,A)\\) will unlikely be linear, so \\(\\epsilon\\) may not even satisfy \\(\\mathrm{E}(\\epsilon|X) = 0\\), let alone i.i.d. normally distributed. Freedman (Freedman 2008) criticized the practices of using parametric linear regression theory on experimentation data, stating: “randomization does not justify the models, bias is likely; nor are the usual variance calculations to be trusted.” Using Neyman’s complete randomization with potential outcomes (see Section 3.1), Freedman avoid postulating a parametric model for \\((Y(T),Y(C),X)\\) by treating them as fixed and the only random component is the treatment assignment \\(A\\). Asymptotic and finite sample theories for the ANCOVA1 model was given in Freedman (2008); and a following work Lin (2013) studied the ANCOVA2 model. Here we follow the independent randomization model (Section 3.2) and assume \\((Y,X,A)\\) are independently sampled from a super population. The model we use is general. The joint distribution of \\((Y,X,A)\\) are allowed to be anything except the restriction that \\(A\\) is result of independent randomization. That is, the joint density has a natural decomposition as \\[\\begin{equation} p(y,x,a) = p_y(y|x,a)p_a(a|x)p(x) \\, \\tag{10.8} \\end{equation}\\] and \\(p_a(a|x)\\) is known to be the Bernoulli density \\(p^a(1-p)^{(1-a)}\\) with fixed treatment probability \\(p\\). This kind of model with a combination of both parametric and nonparametric components are called semiparametric model. The target of the inference is to estimate the ATE \\[ \\delta = \\mathrm{E}(Y|A = 1) - \\mathrm{E}(Y|A=0)\\,. \\] Unlike in a parametric model where the target of inference is either one of the model parameters or a function of them, for semiparametric model, the inference can be any functional of the distribution. For large sample, asymptotic theories exist for semiparametric model just like parametric model. It can be shown that all reasonable consistent and asymptotically normal estimators for \\(\\delta\\) are either exactly or asymptotically equivalent to this form: \\[\\begin{equation} \\overline{Y^{(t)}} - \\overline{Y^{(c)}} + \\frac{1}{n}\\sum \\left((A_i - p)h(X_i)\\right) \\tag{10.9} \\end{equation}\\] for a function \\(h\\) of \\(X\\). A rigorous explanation is beyond our scope and can be found in Tsiatis (2006), Van der Vaart (2000) or Robins and Rotnitzky (1995). Here we just state some general results focusing on high level intuitions. Asymptotic theory for semiparametric models focus on only regular and asymptotically linear estimators (RAL). Regularity condition is to avoid pathological estimators whose behavior can vary dramatically in the neighborhood of the ground truth value, as exemplified by Hodges’ estimator (Van der Vaart 2000). Consistent RAL estimators represents all reasonable estimators of interest with nice properties such as asymptotical normality, including MLE for parametric models, M-estimator and Z-estimator. Semiparametric theory guarantees that all consistent RAL is asymptotically equivalent to an estimator of the form \\[ \\overline {\\psi(Y,X,A)} + \\overline{h(Y,X,A)}\\,, \\] where \\(\\overline{\\psi(Y,X,A)}\\) is any consistent RAL estimator and \\(h(Y,X,A)\\) is from a linear subspace of the Hilbert space of mean-zero finite variance random functions. This linear subspace, denoted by \\(\\mathcal{T}^{\\perp}\\), is the orthogonal component of the tangent space \\(\\mathcal{T}\\). The tangent space for a parametric model can be defined as the linear subspace spanned by score functions. For a semiparametric model, the tangent space contains the tangent space of any parametric submodel – that is, a parametric model whose distribution is also included in the semiparametric model. For our purpose, we already have a consistent RAL. The naive \\(\\Delta\\) estimator \\(\\overline{Y^{(t)}} - \\overline{Y^{(c)}}\\) is asymptotically equivalent to \\[ \\overline {\\frac{AY}{p} - \\frac{(1-A)Y}{1-p}}\\,. \\] Turns out that the linear space \\(\\mathcal{T}^{\\perp}\\) has a very simple form. It contains all mean-zero finite variance functions of \\(h(A,X)\\) such that \\[ \\mathrm{E}(f(A,X)|X) = 0\\,. \\] Since \\(f(A,X)\\) is \\(f(1,X)\\) with probability \\(p\\) and \\(f(0,X)\\) with probability \\(1-p\\), the above condition together with the independence of \\(A\\) and \\(X\\) entails \\[ f(A,X) = (A-p)f(1,A) \\,. \\] Let \\(h(X)= f(1,A)\\), we have shown Equation (10.9) characterizes all consistent RAL for ATE \\(\\delta\\). Exercise: Show \\(\\overline{Y^{(t)}} - \\overline{Y^{(c)}}\\) is asymptotically equivalent to \\(\\overline {\\frac{AY}{p} - \\frac{(1-A)Y}{1-p}}\\). Show \\(f(A,X) = (A-p)f(1,A)\\) if \\(\\mathrm{E}(f(A,X)|X) = 0\\) and \\(A\\) is independent Bernoulli(p). Because \\(\\mathrm{E}\\left((A_i - p)h(X_i)\\right) = 0\\), Equation (10.9) can be seen as a sum of any consistent RAL estimator and another estimator of \\(0\\). This is similar to CUPED where we augment naive ATE estimator \\(\\Delta\\) by \\(\\theta\\Delta(X)\\). Like in CUPED we optimize \\(\\theta\\) to minimize variance, here we can minimize the variance of (10.9) to find the optimal functional form of \\(h(X)\\). Using the asymptotic equivalent form \\(\\overline {\\frac{AY}{p} - \\frac{(1-A)Y}{1-p}}\\) of \\(\\overline{Y^{(t)}} - \\overline{Y^{(c)}}\\), minimize the variance of Equation (10.9) is to minimize variance of \\[ \\frac{AY}{p} - \\frac{(1-A)Y}{1-p} + (A - p)h(X)\\,, \\] which is attained if and only if \\[ \\mathrm{E}\\left(\\left(\\frac{AY}{p} - \\frac{(1-A)Y}{1-p} + (A - p)h(X)\\right) \\times (A-p)h(X)\\right) = 0 \\,. \\] Let \\(h_1(X) = \\mathrm{E}(Y|X,A=1)\\) and \\(h_0(X) = \\mathrm{E}(Y|X,A=0)\\), \\[\\begin{align*} \\mathrm{E}\\left(\\frac{AY}{p} (A-p)h(X)\\right) &amp; = \\mathrm{E}\\left(\\frac{AY}{p} (A-p)h(X)|X \\right) \\\\ &amp; = \\mathrm{E}((1-p)h_1(X) h(X)) \\,. \\end{align*}\\] Similarly, \\[\\begin{align} \\mathrm{E}\\left(\\frac{(1-A)Y}{1-p} (A-p)h(X)\\right) &amp; = \\mathrm{E}(p h_0(X) h(X))\\,,\\\\ \\mathrm{E}((A-p)^2h(X)^2) &amp; = p(1-p)\\mathrm{E}(h(X)^2) \\,. \\end{align}\\] We get \\[ \\mathrm{E}\\left( (1-p)h_1(X) h(X) + p h_0(X) h(X) + p(1-p)h(X)^2 \\right) = 0 \\,. \\] The nontrivial solution (\\(h(X)\\) is not constant 0) is \\[ h(X) = -\\frac{h_1(X)}{p} - \\frac{h_0(X)}{1-p}\\,. \\] Equation (10.9) with this optimal augmentation becomes \\[\\begin{equation} \\overline{Y^{(t)}} - \\overline{Y^{(c)}} - \\frac{1}{n}\\sum \\left((A_i - p)\\left(\\frac{h_1(X)}{p} + \\frac{h_0(X)}{1-p}\\right)\\right) \\tag{10.10} \\end{equation}\\] which is also asymptotically equivalent to \\[\\begin{equation} \\overline{Y^{(t)}} - \\overline{Y^{(c)}} - \\sum \\left((A_i - \\overline{A})\\left(\\frac{h_1(X)}{n_t} + \\frac{h_0(X)}{n_c}\\right)\\right) \\tag{10.11} \\end{equation}\\] Estimator (10.11) solves the problem of the most efficient consistent and RAL estimator for the ATE \\(\\delta\\) under the semiparametric model where we make not a single model assumption other than the independent randomization. To use that, we need to know the true regression \\(h_1(X) = \\mathrm{E}(Y|X,A=1)\\) and \\(h_0(X) = \\mathrm{E}(Y|X,A=0)\\) which requires separated works. However, any choice of \\(h_1\\) and \\(h_0\\) in (10.11) is still of the form (10.9) and is a consistent RAL estimator. In particular, when \\(h_0(X)=h_1(X)=0\\), estimator (10.11) reduced to \\(\\Delta\\). When \\(h_0(X) = h_1(X) = f(X)\\), estimator (10.11) reduces to \\[ \\overline{Y^{(t)}} - \\overline{Y^{(c)}} - (\\overline{f(X^{(t)})}-\\overline{f(X^{(c)})})\\, \\] which is the same as general CUPED in Equation (10.5). Exercise: Show Equation (10.11) reduce to Equation (10.5) when \\(h_0(X)=h_0(X)=f(X)\\). Tsiatis et al. (2008) showed both ANCOVA1 and ANCOVA2 are special case of estimators of the form (10.11) when \\(h_0(X)=h_1(X)=f(X)\\). From this perspective, they are also special cases of CUPED estimator (10.5). The differences between ANCOVA1 and ANCOVA2 are the choice of how to fit linear regression \\(f(X)\\) using treatment and control data. Recall that the linear coefficient estimator is \\(\\mathrm{Cov}(X,Y)/\\mathrm{Var}(X)\\). the denominator \\(\\mathrm{Var}(X)\\) is the same for treatment and control. \\(\\mathrm{Cov}(X,Y)\\) are different due to the treatment effect. ANCOVA1 pools the data together and fit a linear regression. This is to use \\[ \\mathrm{Cov}(X,Y) = p\\mathrm{Cov}_T(X,Y) + (1-p)\\mathrm{Cov}_C(X,Y)\\,, \\] where \\(\\mathrm{Cov}_T(X,Y)\\) is the covariance in the treatment, \\(\\mathrm{Cov}_C(X,Y)\\) for control and \\(p\\) is the proportion of treatment sample size. On the contrary, ANCOVA2 uses \\[ \\mathrm{Cov}(X,Y) = (1-p)\\mathrm{Cov}_T(X,Y) + p\\mathrm{Cov}_C(X,Y)\\,. \\] For CUPED, in the end of Section 10.2.1 we showed if we optimize \\(\\theta\\) to minimize the variance of CUPED estimator \\(\\Delta^*\\) directly, the optimal \\(\\theta\\) is \\[ \\frac{\\mathrm{Cov}(\\overline{Y^{t}},\\overline{X^{t}})+\\mathrm{Cov}(\\overline{Y^{c}},\\overline{X^{c}})}{\\mathrm{Var}(\\overline{X^{t}})+\\mathrm{Var}(\\overline{X^{c}})}\\,. \\] This \\(\\theta\\) asymptotically converge to \\((1-p)\\mathrm{Cov}_T(X,Y) + p\\mathrm{Cov}_C(X,Y)\\). Hence CUPED with this arrangement of \\(\\theta\\) is asymptotically equivalent to ANCOVA2. Because \\(\\mathrm{Cov}(\\overline{Y^{t}},\\overline{X^{t}}) = \\mathrm{Cov}_T(X,Y)/n_t\\) and \\(\\mathrm{Cov}(\\overline{Y^{c}},\\overline{X^{c}}) = \\mathrm{Cov}_C(X,Y)/n_c\\), we can see covariances are weighted inversely proportional to the sample sizes. This explains why it is better to weight \\(\\mathrm{Cov}_T(X,Y)\\) by \\(1-p\\) and \\(\\mathrm{Cov}_C(X,Y)\\) by \\(p\\), instead of using a more straightforward choice of \\(p\\) for \\(\\mathrm{Cov}_T(X,Y)\\) and \\(1-p\\) for \\(\\mathrm{Cov}_C(X,Y)\\) as in ANCOVA1 . From here we also see ANCOVA2 is theoretically better than ANCOVA1. Although in practice this difference is small unless \\(p\\) is away from 0.5 and \\(\\mathrm{Cov}_T(X,Y)\\) is very different from \\(\\mathrm{Cov}_C(X,Y)\\). 10.2.3 Doubly Robust Estimator Before we close the discussion of regression adjustment, take a look at the doubly robust estimator. DR estiator when propensity score is known is the same as (10.11)! The goal of doubly robust estimator was to combine regression prediction to impute missing counterfactuals with propensity reweighting so as long as one of the two models is unbiased the DR estimator remains unbiased. For randomized experiments the propensity model is known and the DR estimation is unbiased for arbitrary regression model. This is the spirit of regression adjustment in (10.11). The closer the regression model is to the true regression \\(h_0=\\mathrm{E}(Y|X,A=0)\\) and \\(h_1=\\mathrm{E}(Y|X,A=1)\\), the better (smaller variance). "],["misc-topics.html", "Chapter 11 Misc Topics 11.1 Delta Method 11.2 Random Denominator for Independent Randomization Experiments 11.3 M-Estimator and Z-Estimator", " Chapter 11 Misc Topics Here we put some technical details that are useful to be included in the book but not so important to add into the main chapter. 11.1 Delta Method Lemma 11.1 Let \\((X,Y),(X_1,Y_1),\\dots,(X_N,Y_N)\\) be i.i.d. random variables such that \\[ \\sqrt{N}\\left(\\overline{X}-\\mathrm{E}(X), \\overline{Y}-\\mathrm{E}(Y)\\right) \\] is asymptotically normal. Let \\(\\mu = \\frac{\\mathrm{E}(Y)}{\\mathrm{E}(X)}\\). Then \\[ \\frac{\\sum Y}{\\sum X} \\to \\mu \\quad a.s. \\] Moreover, \\[\\begin{equation} \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{\\sum X} \\right) - \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{N\\mathrm{E}(X)}\\right) \\to 0 \\quad \\text{in probability}, \\tag{11.1} \\end{equation}\\] which means \\[ \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{\\sum X} \\right) \\quad \\text{and} \\quad \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{N\\mathrm{E}(X)} \\right) \\] have the same asymptotic normal distribution. Proof. \\(\\frac{\\sum Y}{\\sum X} \\to \\mu\\) by law of large number. For (11.1), \\[\\begin{align*} &amp;\\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{\\sum X} \\right) - \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{N\\mathrm{E}(X)}\\right) = \\\\ &amp; \\sqrt{N}\\left(\\frac{\\sum (Y - \\mu X)}{N\\mathrm{E}(X)}\\right)\\times\\left( \\frac{N\\mathrm{E}(X)}{\\sum X} - 1 \\right). \\end{align*}\\] The first term converges to a normal distribution and the second term converge in probability to 0 by law of large number. By Slutsky’s theorem, the product converges in distribution (hence also in probability) to 0. 11.2 Random Denominator for Independent Randomization Experiments Let \\(Z_i,i=1,\\dots,N\\) be i.i.d. assignment indicator where \\(Z_i=1\\) if \\(i\\)th unit is assigned to treatment with probability \\(p\\). Let \\((Y_i(0),Y_i(1)),i=1,\\dots,N\\) be the potential outcome pairs. We consider the asymptotic distribution of \\[ \\frac{\\sum Z_i Y_i(1)}{\\sum Z_i} - \\frac{\\sum (1- Z_i) Y_i(0)}{\\sum (1-Z_i)}. \\] Denote \\(\\sum Z_i\\) by \\(N_T\\) and \\(\\sum (1-Z_i)\\) by \\(N_C\\) for the sample size in treatment and control groups respectively. Because \\(N_T\\) and \\(N_C\\) are no longer a fixed number, the variance of the above cannot be derived simply using the variance of \\(Z_i Y_i(1)\\) divided by \\(N_T\\) (and \\((1-Z_i) Y_i(0)\\) divided by \\(N_C\\)). The asymptotic variance can be derived using the delta method or more directly from Lemma 11.1. Let \\(\\mu_1 = \\mathrm{E}(Y(1))\\) and \\(\\mu_0 = \\mathrm{E}(Y(0))\\). By (11.1) \\[\\begin{align*} \\sqrt{N} \\left( \\frac{\\sum Z_i (Y_i(1) - \\mu_1)}{\\sum Z_i} - \\frac{\\sum (1- Z_i) (Y_i(0) - \\mu_0)}{\\sum (1-Z_i)} \\right) \\end{align*}\\] and \\[\\begin{align} \\sqrt{N} \\left( \\frac{\\sum Z_i (Y_i(1) - \\mu_1)}{Np} - \\frac{\\sum (1- Z_i) (Y_i(0) - \\mu_0)}{N(1-p)} \\right) \\tag{11.2} \\end{align}\\] have the same asymptotic normal distribution. Also note that \\[ \\mathrm{E}(Z_i(Y_i(1)-\\mu_1) \\times (1- Z_i) (Y_i(0) - \\mu_0)) = 0 \\] since \\(Z_i(1-Z_i)=0\\). The variance of (11.2) is \\[\\begin{align} &amp;\\frac{\\mathrm{Var}\\left(Z_i(Y_i(1)-\\mu_1)\\right)}{p^2} + \\frac{\\mathrm{Var}\\left((1-Z_i)(Y_i(0)-\\mu_0)\\right)}{(1-p)^2} \\notag \\\\ = &amp; \\frac{\\mathrm{Var}(Y_i(1))}{p} + \\frac{\\mathrm{Var}(Y_i(0))}{1-p}. \\tag{11.3} \\end{align}\\] The equality is because \\(Z_i\\) and \\((Y_i(1),Y_i(0))\\) is independent so \\[ \\mathrm{Var}\\left(Z_i(Y_i(1)-\\mu_1)\\right) = \\mathrm{E}\\left(Z_i^2(Y_i(1)-\\mu_1)^2 \\right) = p \\mathrm{E}(Y_i(1)-\\mu_1)^2 = p \\mathrm{Var}(Y_i(1)) \\] and similarly \\(\\mathrm{Var}\\left((1-Z_i)(Y_i(0)-\\mu_0)\\right) = (1-p)\\mathrm{Var}(Y_i(0))\\). 11.3 M-Estimator and Z-Estimator Many estimators can be defined as a maximizer or root of an empirical expectation. Quantile and mean share the same form as the solution of minimizing the expectation of a parametrized function \\(\\psi_\\theta(x)\\) of a random variable. By simply replacing the theoretical distribution \\(P\\) by its empirical version \\(\\widetilde{P}\\), the solution of the empirical version of the same minimization problem is called M-estimator. Definition 11.1 Let \\(\\psi_\\theta(x)\\) be a family of functions of \\(x\\) parametrized by \\(\\theta\\). The solution of \\[ \\min_{\\theta} \\frac{1}{N}\\sum_{i=1}^N \\psi_\\theta(X_i) \\] is called a M-estimator. Sample quantile and sample mean are all special cases of M-estimator. The theory of M-estimator is derived as an generalization of MLE (so MLE is a special case of M-estimator). M here refers to Maximum likelihood like estimator. See endnotes. Under mild regularity conditions, M-estimator, like MLE, has an asymptotically normal distribution. Theorem 11.1 (M-Estimator) To do. From van der vaart. "],["probability-minimum.html", "Chapter 12 Probability Minimum 12.1 probability", " Chapter 12 Probability Minimum 12.1 probability 12.1.1 Conditional Independence Below are commonly used rules of conditional independence. Symmetry: \\[ X \\perp \\! \\! \\! \\! \\perp Y \\implies Y \\perp \\! \\! \\! \\! \\perp X. \\] Decomposition: \\[ X \\perp \\! \\! \\! \\! \\perp A,B \\implies X \\perp \\! \\! \\! \\! \\perp A \\text{ and } X \\perp \\! \\! \\! \\! \\perp B \\] Weak union: \\[ X \\perp \\! \\! \\! \\! \\perp A,B \\implies X \\perp \\! \\! \\! \\! \\perp A | B \\text{ and } X \\perp \\! \\! \\! \\! \\perp B | A \\] Contraction: \\[ X \\perp \\! \\! \\! \\! \\perp A|B \\text{ and } X \\perp \\! \\! \\! \\! \\perp B \\iff X \\perp \\! \\! \\! \\! \\perp A, B \\] Intersection: \\[ X \\perp \\! \\! \\! \\! \\perp A|C, B \\text{ and } X \\perp \\! \\! \\! \\! \\perp B|C, A \\implies X \\perp \\! \\! \\! \\! \\perp A, B | C \\] Agresti, Alan, and Brent A Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26. Asmussen, Soren, and Peter Glynn. 2008. Stochastic Simulation. Springer-Verlag. Athey, Susan, Guido W Imbens, and Stefan Wager. 2018. “Approximate Residual Balancing: Debiased Inference of Average Treatment Effects in High Dimensions.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 80 (4): 597–623. Bakshy, Eytan, Dean Eckles, and Michael S Bernstein. 2014. “Designing and Deploying Online Field Experiments.” In Proceedings of the 23rd International Conference on World Wide Web, edited by acm, 283–92. ACM. Barber, David. 2012. Bayesian Reasoning and Machine Learning. Cambridge University Press. Barber, Rina Foygel, and Emmanuel J Candes. 2016. “A Knockoff Filter for High-Dimensional Selective Inference.” arXiv Preprint arXiv:1602.03574. Bartroff, Jay, Tze Leung Lai, and Mei-Chiung Shih. 2012. Sequential Experimentation in Clinical Trials: Design and Analysis. Vol. 298. Springer Science &amp; Business Media. Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2014. “Fitting Linear Mixed-Effects Models Using Lme4.” arXiv Preprint arXiv:1406.5823. Benjamin, Daniel J, James O Berger, Magnus Johannesson, Brian A Nosek, E-J Wagenmakers, Richard Berk, Kenneth A Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6. Benjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the false discovery rate: a practical and powerful approach to multiple testing.” J. R. Stat. Soc. Ser. B, 289–300. Boos, Dennis D, and Jacqueline M Hughes-Oliver. 2000. “How Large Does n Have to Be for z and t Intervals?” The American Statistician 54 (2): 121–28. Brown, Morton B, and Robert A Wolfe. 1983. “Estimation of the Variance of Percentile Estimates.” Computational Statistics &amp; Data Analysis 1: 167–74. Carpenter, Bob, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Michael A Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2016. “Stan: A Probabilistic Programming Language.” Journal of Statistical Software 20: 1–37. Casella, George, and Roger L Berger. 2002. Statistical Inference, Second Edition. Duxbury Press: Pacific Grove, CA. Chamandy, Nicholas, Omkar Muralidharan, and Stefan Wager. 2015. “Teaching Statistics at Google-Scale.” The American Statistician 69 (4): 283–91. Coey, Dominic, and Michael Bailey. 2016. “People and Cookies: Imperfect Treatment Assignment in Online Experiments.” In Proceedings of the 25th International Conference on World Wide Web, 1103–11. International World Wide Web Conferences Steering Committee. Davidian, M, Anastasios A. Tsiatis, and S Leon. 2005. “Semiparametric Estimation of Treatment Effect in a Pretest-Posttest Study with Missing Data.” Stat. Sci. 20 (3): 295–301. Deng, A., and X. Shi. 2016. “Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Deng, Alex, Ulf Knoblich, and Jiannan Lu. 2018. “Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas.” In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining, 233–42. KDD ’18. New York, NY, USA: ACM. https://doi.org/10.1145/3219819.3219919. Deng, Alex, Jiannan Lu, and Shouyuan Chen. 2016. “Continuous Monitoring of a/b Tests Without Pain: Optional Stopping in Bayesian Testing.” In Data Science and Advanced Analytics (DSAA), 2016 IEEE International Conference on, 243–52. IEEE. Deng, Alex, Jiannan Lu, and Jonthan Litz. 2017. “Trustworthy Analysis of Online a/b Tests: Pitfalls, Challenges and Solutions.” In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, 641–49. WSDM ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3018661.3018677. Deng, Alex, Ya Xu, Ron Kohavi, and Toby Walker. 2013. “Improving the sensitivity of online controlled experiments by utilizing pre-experiment data.” In Proc. 6th ACM Int. Conf. Web Search Data Min., edited by acm, 123–32. ACM. Deng, Shaojie, Roger Longbotham, Toby Walker, and Ya Xu. 2011. “Choice of the Randomization Unit in Online Controlled Experiment.” JSM Proc. Donner, Allan. 1987. “Statistical Methodology for Paired Cluster Designs.” American Journal of Epidemiology 126 (5): 972–79. Efron, Bradley. 2011. “Tweedie’s formula and selection bias.” J. Am. Stat. Assoc. 106 (496): 1602–14. Fisher, Ronald. 1958. “Cigarettes, Cancer, and Statistics.” The Centennial Review of Arts &amp; Science 2: 151–66. Fisher, Sir Ronald A, and EA Cornish. 1960. “The Percentile Points of Distributions Having Known Cumulants.” Technometrics 2 (2): 209–25. Fithian, William, Dennis Sun, and Jonathan Taylor. 2014. “Optimal Inference After Model Selection.” arXiv Preprint arXiv:1410.2597. Freedman, David A. 2008. “On regression adjustments to experimental data.” Adv. Appl. Math. 40 (2): 180–93. Gelman, Andrew, and John Carlin. 2014. “Beyond Power Calculations: Assessing Type s (Sign) and Type m (Magnitude) Errors.” Perspectives on Psychological Science 9 (6): 641–51. Gelman, Andrew, and Jennifer Hill. 2006. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Gomez-Uribe, Carlos A, and Neil Hunt. 2016. “The Netflix Recommender System: Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems (TMIS) 6 (4): 13. Goodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value Misconceptions.” In Seminars in Hematology, 45:135–40. 3. Elsevier. Hainmueller, Jens. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. Hall, Peter. 2013. The Bootstrap and Edgeworth Expansion. Springer Science &amp; Business Media. Hoenig, John M, and Dennis M Heisey. 2001. “The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis.” The American Statistician 55 (1): 19–24. Hohnhold, Henning, Deirdre O’Brien, and Diane Tang. 2015. “Focus on the Long-Term: It’s Better for Users and Business.” In Proceedings 21st Conference on Knowledge Discovery and Data Mining. Sydney, Australia. http://dl.acm.org/citation.cfm?doid=2783258.2788583. Imai, Kosuke, and Marc Ratkovic. 2014. “Covariate Balancing Propensity Score.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (1): 243–63. Imbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press. Johari, Ramesh, Pete Koomen, Leonid Pekelis, and David Walsh. 2017. “Peeking at a/b Tests: Why It Matters, and What to Do about It.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1517–25. ACM. Jordan, Michael I. 2004. “Graphical Models.” Statistical Science, 140–55. Kang, Joseph DY, and Joseph L Schafer. 2007. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science, 523–39. Klar, Neil, and Allan Donner. 2001. “Current and Future Challenges in the Design and Analysis of Cluster Randomization Trials.” Statistics in Medicine 20 (24): 3729–40. Kleiner, Ariel, Ameet Talwalkar, Purnamrita Sarkar, and Michael I Jordan. 2014. “A scalable bootstrap for massive data.” J. R. Stat. Soc. Ser. B (Statistical Methodol. Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. “Trustworthy Online Controlled Experiments: Five Puzzling Outcomes Explained.” Proc. 18th Conf. Knowl. Discov. Data Min. Kohavi, Ron, Alex Deng, Brian Frasca, Ya Xu, Toby Walker, and Nils Pohlmann. 2013. “Online Controlled Experiments at Large Scale.” Proc.19th Conf. Knowl. Discov. Data Min. Kohavi, Ron, Alex Deng, Roger Longbotham, and Ya Xu. 2014. “Seven rules of thumb for web site experimenters.” In Proc. 20th Conf. Knowl. Discov. Data Min., edited by acm, 1857–66. KDD ’14. New York, USA. Kohavi, Ron, and Roger Longbotham. 2015. “Online Controlled Experiments and a/b Tests.” Encyclopedia of Machine Learning and Data Mining. Kohavi, Ron, Roger Longbotham, Dan Sommerfield, and Randal M Henne. 2009. “Controlled Experiments on the Web: survey and practical guide.” Data Min. Knowl. Discov. 18: 140–81. Kohavi, Ron, Roger Longbotham, and Toby Walker. 2010. “Online experiments: Practical lessons.” Computer (Long. Beach. Calif). 43 (9): 82–85. Kohavi, Ron, David Messner, Seth Eliot, Juan Lavista Ferres, Randy Henne, Vignesh Kannappan, and Justin Wang. 2010. “Tracking Users’ Clicks and Submits: Tradeoffs Between User Experience and Data Loss.” Redmond: Sn. Krewski, Daniel. 1976. “Distribution-Free Confidence Intervals for Quantile Intervals.” Journal of the American Statistical Association 71 (354): 420–22. Lauritzen, Steffen L. 1996. Graphical Models. Vol. 17. Clarendon Press. Li, Fan, Kari Lock Morgan, and Alan M Zaslavsky. 2018. “Balancing Covariates via Propensity Score Weighting.” Journal of the American Statistical Association 113 (521): 390–400. Lin, Winston. 2013. “Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique.” Ann. Appl. Stat. 7 (1): 295–318. Lu, Jiannan, and Alex Deng. 2016. “Demystifying the Bias from Selective Inference: A Revisit to Dawid’s Treatment Selection Problem.” ArXiv Pre-Print. http://arxiv.org/abs/1601.05835. Lu, Jiannan, Yixuan Qiu, and Alex Deng. 2018. “A Note on Type s/m Errors in Hypothesis Testing.” British Journal of Mathematical and Statistical Psychology. Meyer, John S. 1987. “Outer and Inner Confidence Intervals for Finite Population Quantile Intervals.” Journal of the American Statistical Association 82 (397): 201–4. Miratrix, Luke W, Jasjeet S Sekhon, and Bin Yu. 2013. “Adjusting treatment effect estimates by post-stratification in randomized experiments.” J. R. Stat. Soc. Ser. B (Statistical Methodol. 75 (2): 369–96. Morgan, Stephen L, and Christopher Winship. 2014. Counterfactuals and Causal Inference. Cambridge University Press. Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press. Pearl, Judea. 1995. “Causal Diagrams for Empirical Research.” Biometrika 82 (4): 669–88. ———. 2009. “Causal inference in statistics: An overview.” Stat. Surv. 3: 96–146. Regulation, General Data Protection. 2016. “Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46.” Official Journal of the European Union (OJ) 59 (1-88): 294. Ries, Eric. 2011. The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. Crown Books. Robins, James M, and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression Models with Missing Data.” Journal of the American Statistical Association 90 (429): 122–29. Senn, S. 2008. “A note concerning a selection \"paradox\" of Dawid’s.” Am. Stat. Assoc. 62 (3): 206–10. https://doi.org/10.1198/000313008X331530. Shpitser, Ilya, and Judea Pearl. 2012. “Identification of Conditional Interventional Distributions.” arXiv Preprint arXiv:1206.6876. Tang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. 2010. “Overlapping Experiment Infrastructure: More, Better, Faster Experimentation.” Proc. 16th Conf. Knowl. Discov. Data Min. Taylor, Jonathan, and Robert Tibshirani. 2018. “Post-Selection Inference for-Penalized Likelihood Models.” Canadian Journal of Statistics 46 (1): 41–61. Taylor, Jonathan, and Robert J Tibshirani. 2015. “Statistical Learning and Selective Inference.” Proceedings of the National Academy of Sciences 112 (25): 7629–34. Tsiatis, Anastasios A. 2006. Semiparametric Theory and Missing Data. Springer-Verlag. Tsiatis, Anastasios A., Marie Davidian, Min Zhang, and Xiaomin Lu. 2008. “Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: A principled yet flexible approach.” Stat. Med. 27. Van der Vaart, Aad W. 2000. Asymptotic statistics. Vol. 3. Cambridge university press. Wager, Stefan, and Susan Athey. 2017. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association, no. just-accepted. Wasserman, Larry. 2003. All of Statistics: A Concise Course in Statistical Inference. Springer. Xu, Ya, Nanyu Chen, Addrian Fernandez, Omar Sinno, and Anmol Bhasin. 2015. “From Infrastructure to Culture: A/b Testing Challenges in Large Scale Social Networks.” In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2227–36. KDD ’15. New York, NY, USA: ACM. https://doi.org/10.1145/2783258.2788602. Yang, Li, and Anastasios A. Tsiatis. 2001. “Efficiency Study of Estimators for a Treatment Effect in a Pretest-Posttest Trial.” Am. Stat. 55 (4): 314–21. "]]
